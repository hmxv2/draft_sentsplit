{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [''.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "                #tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./small_data_set/train_set_inputs_10w.pk', 'rb') as f:\n",
    "    train_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/train_set_input_lens_10w.pk', 'rb') as f:\n",
    "    train_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/train_set_labels_10w.pk', 'rb') as f:\n",
    "    train_set_labels = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_inputs_10w.pk', 'rb') as f:\n",
    "    valid_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_input_lens_10w.pk', 'rb') as f:\n",
    "    valid_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_labels_10w.pk', 'rb') as f:\n",
    "    valid_set_labels = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "# with open('./data_set/train_set_inputs.pk', 'rb') as f:\n",
    "#     train_set_inputs = pickle.load(f)\n",
    "# with open('./data_set/train_set_input_lens.pk', 'rb') as f:\n",
    "#     train_set_input_lens = pickle.load(f)\n",
    "# with open('./data_set/train_set_labels.pk', 'rb') as f:\n",
    "#     train_set_labels = pickle.load(f)\n",
    "# with open('./data_set/valid_set_inputs.pk', 'rb') as f:\n",
    "#     valid_set_inputs = pickle.load(f)\n",
    "# with open('./data_set/valid_set_input_lens.pk', 'rb') as f:\n",
    "#     valid_set_input_lens = pickle.load(f)\n",
    "# with open('./data_set/valid_set_labels.pk', 'rb') as f:\n",
    "#     valid_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 100000 100000 100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_inputs), len(train_set_input_lens), len(train_set_labels), \n",
    "      len(valid_set_input_lens), len(valid_set_inputs), len(valid_set_labels))\n",
    "\n",
    "for sent_len in valid_set_input_lens:\n",
    "    if sent_len<=2:\n",
    "        print('why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('pre_train_word_embedding.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "        #self.embed.weight.requires_grad = False\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "        #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "            cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "            cn = cn.index_select(0, Variable(true_order_ids))\n",
    "            \n",
    "        return outputs, (hn,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _inflate(tensor, times, dim):\n",
    "    \"\"\"\n",
    "    Examples::\n",
    "        >> a = torch.LongTensor([[1, 2], [3, 4]])\n",
    "        >> a\n",
    "        1   2\n",
    "        3   4\n",
    "        [torch.LongTensor of size 2x2]\n",
    "        >> b = ._inflate(a, 2, dim=1)\n",
    "        >> b\n",
    "        1   2   1   2\n",
    "        3   4   3   4\n",
    "        [torch.LongTensor of size 2x4]\n",
    "    \"\"\"\n",
    "    repeat_dims = [1] * tensor.dim()\n",
    "    repeat_dims[dim] = times\n",
    "    return tensor.repeat(*repeat_dims)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, use_cuda, encoder, hidden_dim, max_length=25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.input_dim = encoder.input_dim\n",
    "        self.max_length = max_length\n",
    "        self.vocab = encoder.vocab\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        #self.weight[self.vocab.word2token['<eos>']]=1.01\n",
    "        #self.weight[self.vocab.word2token['<split>']]=1.01\n",
    "        \n",
    "        self.hidden_size = self.hidden_dim\n",
    "        self.V = len(self.vocab.word2token)\n",
    "        self.SOS = self.vocab.word2token['<sos>']\n",
    "        self.EOS = self.vocab.word2token['<eos>']\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lstmcell = torch.nn.LSTMCell(input_size=self.input_dim, hidden_size=self.hidden_dim*2, bias=True)\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=encoder.embed# reference share\n",
    "        #fcnn: projection for crossentroy loss\n",
    "        self.fcnn = nn.Linear(in_features = self.hidden_dim*2, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.cost_func = nn.CrossEntropyLoss(torch.Tensor(self.weight))\n",
    "        \n",
    "        print('init lookup embedding matrix size: ', self.embed.weight.data.size())\n",
    "        \n",
    "    def forward(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = 0\n",
    "        predicts = []\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "\n",
    "        for ii in range(self.max_length):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,0])\n",
    "                    all_loss+=loss\n",
    "                \n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #print('max_idxs size: ',max_idxs.size(), max_idxs)\n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "                    all_loss+=loss\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                #print('max_idx size: ', max_idxs.size(), max_idxs)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "        \n",
    "        if is_train:  #training\n",
    "            if self.use_cuda:\n",
    "                return all_loss/(self.max_length), predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return all_loss/(self.max_length), predicts.data.numpy()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return predicts.data.numpy()\n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var\n",
    "    def decode_by_beamsearch(self, encoder_hidden=None, encoder_outputs=None, topk = 10):\n",
    "        self.k = topk\n",
    "        batch_size = encoder_outputs.size(dim=0)\n",
    "        \n",
    "        self.pos_index = self._tocuda(Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1))\n",
    "\n",
    "        hidden = tuple([_inflate(h, self.k, 1).view(batch_size*self.k, -1) for h in encoder_hidden])\n",
    "        #print('hidden0 size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "\n",
    "        # Initialize the scores; for the first step,\n",
    "        # ignore the inflated copies to avoid duplicate entries in the top k\n",
    "        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n",
    "        sequence_scores.fill_(-float('Inf'))\n",
    "        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n",
    "        sequence_scores = self._tocuda(Variable(sequence_scores))\n",
    "\n",
    "        # Initialize the input vector\n",
    "        input_var = self._tocuda(Variable(torch.LongTensor([self.SOS] * batch_size * self.k)))\n",
    "\n",
    "        # Store decisions for backtracking\n",
    "        stored_outputs = list()\n",
    "        stored_scores = list()\n",
    "        stored_predecessors = list()\n",
    "        stored_emitted_symbols = list()\n",
    "        stored_hidden = list()\n",
    "\n",
    "        for ii in range(0, self.max_length):\n",
    "            # Run the RNN one step forward\n",
    "            #print('setp: %s'%ii)\n",
    "            input_vec = self.embed(input_var)\n",
    "            #print('input_var and input_vec size: ', input_var.size(), input_vec.size())\n",
    "            hidden = self.lstmcell(input_vec, hidden)\n",
    "            #print('hidden size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "            \n",
    "            log_softmax_output = self.log_softmax(self.fcnn(hidden[0]))\n",
    "\n",
    "            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n",
    "            sequence_scores = _inflate(sequence_scores, self.V, 1)\n",
    "            sequence_scores += log_softmax_output.squeeze(1)\n",
    "            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n",
    "\n",
    "            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n",
    "            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n",
    "            sequence_scores = scores.view(batch_size * self.k, 1)\n",
    "\n",
    "            # Update fields for next timestep\n",
    "            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple([h.index_select(0, predecessors.squeeze()) for h in hidden])\n",
    "            else:\n",
    "                hidden = hidden.index_select(0, predecessors.squeeze())\n",
    "\n",
    "            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n",
    "            stored_scores.append(sequence_scores.clone())\n",
    "            eos_indices = input_var.data.eq(self.EOS)\n",
    "            if eos_indices.nonzero().dim() > 0:\n",
    "                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n",
    "\n",
    "            # Cache results for backtracking\n",
    "            stored_predecessors.append(predecessors)\n",
    "            stored_emitted_symbols.append(input_var)\n",
    "#             stored_hidden.append(hidden)\n",
    "\n",
    "        # Do backtracking to return the optimal values\n",
    "        output, h_t, h_n, s, l, p = self._backtrack(hidden,\n",
    "                                                    stored_predecessors, stored_emitted_symbols,\n",
    "                                                    stored_scores, batch_size, self.hidden_size)\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        metadata['score'] = s\n",
    "        metadata['topk_length'] = l\n",
    "        metadata['topk_sequence'] = p\n",
    "        metadata['length'] = [seq_len[0] for seq_len in l]\n",
    "        metadata['sequence'] = [seq[0] for seq in p]\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def _backtrack(self, hidden, predecessors, symbols, scores, b, hidden_size):\n",
    "        \"\"\"Backtracks over batch to generate optimal k-sequences.\n",
    "\n",
    "        Args:\n",
    "            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n",
    "            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n",
    "            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n",
    "            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n",
    "            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n",
    "            b: Size of the batch\n",
    "            hidden_size: Size of the hidden state\n",
    "\n",
    "        Returns:\n",
    "            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n",
    "\n",
    "            score [batch, k]: A list containing the final scores for all top-k sequences\n",
    "\n",
    "            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n",
    "\n",
    "            p (batch, k, sequence_len): A Tensor containing predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        lstm = isinstance(hidden, tuple)\n",
    "\n",
    "        # initialize return variables given different types\n",
    "        output = list()\n",
    "        h_t = list()\n",
    "        p = list()\n",
    "        # Placeholder for last hidden state of top-k sequences.\n",
    "        # If a (top-k) sequence ends early in decoding, `h_n` contains\n",
    "        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n",
    "        # the last hidden state of decoding.\n",
    "        if lstm:\n",
    "            state_size = hidden[0].size()\n",
    "            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n",
    "        else:\n",
    "            h_n = torch.zeros(nw_hidden[0].size())\n",
    "        l = [[self.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n",
    "                                                                # Similar to `h_n`\n",
    "\n",
    "        # the last step output of the beams are not sorted\n",
    "        # thus they are sorted here\n",
    "        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n",
    "        # initialize the sequence scores with the sorted last step beam scores\n",
    "        s = sorted_score.clone()\n",
    "\n",
    "        batch_eos_found = [0] * b   # the number of EOS found\n",
    "                                    # in the backward loop below for each batch\n",
    "\n",
    "        t = self.max_length - 1\n",
    "        # initialize the back pointer with the sorted order of the last step beams.\n",
    "        # add self.pos_index for indexing variable with b*k as the first dimension.\n",
    "        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n",
    "        while t >= 0:\n",
    "            # Re-order the variables with the back pointer\n",
    "            current_symbol = symbols[t].index_select(0, t_predecessors)\n",
    "            # Re-order the back pointer of the previous step with the back pointer of\n",
    "            # the current step\n",
    "            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n",
    "\n",
    "            # This tricky block handles dropped sequences that see EOS earlier.\n",
    "            # The basic idea is summarized below:\n",
    "            #\n",
    "            #   Terms:\n",
    "            #       Ended sequences = sequences that see EOS early and dropped\n",
    "            #       Survived sequences = sequences in the last step of the beams\n",
    "            #\n",
    "            #       Although the ended sequences are dropped during decoding,\n",
    "            #   their generated symbols and complete backtracking information are still\n",
    "            #   in the backtracking variables.\n",
    "            #   For each batch, everytime we see an EOS in the backtracking process,\n",
    "            #       1. If there is survived sequences in the return variables, replace\n",
    "            #       the one with the lowest survived sequence score with the new ended\n",
    "            #       sequences\n",
    "            #       2. Otherwise, replace the ended sequence with the lowest sequence\n",
    "            #       score with the new ended sequence\n",
    "            #\n",
    "            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n",
    "            if eos_indices.dim() > 0:\n",
    "                for i in range(eos_indices.size(0)-1, -1, -1):\n",
    "                    # Indices of the EOS symbol for both variables\n",
    "                    # with b*k as the first dimension, and b, k for\n",
    "                    # the first two dimensions\n",
    "                    idx = eos_indices[i]\n",
    "                    b_idx = int(idx[0] / self.k)\n",
    "                    # The indices of the replacing position\n",
    "                    # according to the replacement strategy noted above\n",
    "                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n",
    "                    batch_eos_found[b_idx] += 1\n",
    "                    res_idx = b_idx * self.k + res_k_idx\n",
    "\n",
    "                    # Replace the old information in return variables\n",
    "                    # with the new ended sequence information\n",
    "                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n",
    "\n",
    "                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n",
    "                    s[b_idx, res_k_idx] = scores[t][idx[0]]\n",
    "                    l[b_idx][res_k_idx] = t + 1\n",
    "\n",
    "            # record the back tracked results\n",
    "            p.append(current_symbol)\n",
    "            t -= 1\n",
    "\n",
    "        # Sort and re-order again as the added ended sequences may change\n",
    "        # the order (very unlikely)\n",
    "        s, re_sorted_idx = s.topk(self.k)\n",
    "        for b_idx in range(b):\n",
    "            l[b_idx] = [l[b_idx][k_idx.data[0]] for k_idx in re_sorted_idx[b_idx,:]]\n",
    "\n",
    "        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n",
    "\n",
    "        # Reverse the sequences and re-order at the same time\n",
    "        # It is reversed because the backtracking happens in reverse time order\n",
    "#         output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n",
    "        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n",
    "        #    --- fake output ---\n",
    "        output = None\n",
    "        #    --- fake ---\n",
    "        return output, h_t, h_n, s, l, p\n",
    "\n",
    "    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n",
    "            score[idx] = masking_score\n",
    "\n",
    "    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n",
    "        if len(idx.size()) > 0:\n",
    "            indices = idx[:, 0]\n",
    "            tensor.index_fill_(dim, indices, masking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, use_cuda, input_dim, hidden_dim, vocab, max_length = 25):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.enc = Encoder(use_cuda=use_cuda, hidden_dim=hidden_dim, input_dim=input_dim, vocab=vocab)\n",
    "        self.dec = Decoder(use_cuda=use_cuda, encoder=self.enc, hidden_dim=hidden_dim, max_length=max_length)\n",
    "        if use_cuda:\n",
    "            self.enc = self.enc.cuda()\n",
    "            self.dec = self.dec.cuda()\n",
    "    def forward(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        if is_train:\n",
    "            loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                    h0_and_c0=(enc_hn, enc_cn), \n",
    "                                    sent_lens=input_lens,\n",
    "                                    labels=torch.LongTensor(labels), \n",
    "                                    is_train=1, \n",
    "                                    teaching_rate = 1\n",
    "                                    )\n",
    "            return loss, predicts\n",
    "        else:\n",
    "            predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                sent_lens=input_lens,\n",
    "                                labels=torch.LongTensor(labels), \n",
    "                                is_train=0, \n",
    "                                teaching_rate = 1\n",
    "                                )\n",
    "            return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n",
      "开上一边！我们在仪表飞行。\n",
      "他们不会放过任何一个认识你的人，<low_freq>。你当心点。\n",
      "但同样的，我们的设备是支持<low_freq>培育好用的。\n",
      "我们的风云人物来了。恭喜啊。\n",
      "第三条矿山总统joe：为纾个时候）卖过程和一家医疗部门工作管理登记了下损害。\n",
      "对。所以你是说和我共事是种惩罚\n",
      "不管怎么样，事情一件接一件，而且……\n",
      "如果你不合作的话，\n",
      "你得保证是完美的一次！\n",
      "贫穷的朋友,因为患难之交是真情。\n",
      "我说了别接听！\n",
      "你说完了？什么？\n",
      "他拒绝了本使用的采访。\n",
      "我觉得不可能警戒事发地点。\n",
      "你们要点推荐的今日自选<low_freq>吗？\n",
      "合作品牌网站上的广告。\n",
      "因为演员家伙在他办公室好排行榜。我们把膝夹！\n",
      "实际测试显示模块运作完美。\n",
      "最好是两个男孩两个女孩...\n",
      "快，上车，带上他\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b053ccd8f3e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 26)\n",
    "pre_train = torch.load('./models_better/time-[2019-01-08-06-27-50]-loss-0.925657809-bleu-0.6640-hidden_dim-256-input_dim-300-epoch-2-batch_size-200-batch_id-[17001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "autoencoder.eval()\n",
    "\n",
    "train_set_size = len(train_set_inputs)\n",
    "sample_num=20\n",
    "topk=10\n",
    "batch_id=0\n",
    "bleu_sum=0\n",
    "\n",
    "# dec_beamsearch = TopKDecoder(decoder_rnn=autoencoder.dec, k=topk)\n",
    "\n",
    "enc_outputs, (enc_hn, enc_cn) = autoencoder.enc(torch.LongTensor(train_set_inputs[:sample_num]), \n",
    "                                        torch.LongTensor(train_set_input_lens[:sample_num]))\n",
    "#print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "metadata = autoencoder.dec.decode_by_beamsearch(encoder_hidden = (enc_hn, enc_cn), encoder_outputs = enc_outputs, topk=topk)\n",
    "# metadata = dec_beamsearch(encoder_hidden = (enc_hn, enc_cn), encoder_outputs = enc_outputs)\n",
    "\n",
    "results = metadata['topk_sequence']\n",
    "results =torch.cat(results, dim = 2)\n",
    "results=results.view(sample_num*topk, -1)\n",
    "indices = torch.LongTensor([x*topk for x in range(sample_num)]).cuda()\n",
    "results = results.data.index_select(0, indices)\n",
    "results=results.cpu().tolist()\n",
    "results=batch_tokens_remove_eos(results, vocab)\n",
    "\n",
    "sent_lens = []\n",
    "for tokens in results:\n",
    "    sent_lens.append(len(tokens))\n",
    "    \n",
    "results=batch_tokens2words(results, vocab)\n",
    "results=batch_words2sentence(results)\n",
    "for sent in results:\n",
    "    print(sent)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "stop\n",
    "\n",
    "\n",
    "start_time=time.time()\n",
    "for start_idx in range(0, train_set_size-sample_num, sample_num):\n",
    "    batch_id+=1\n",
    "    \n",
    "    enc_outputs, (enc_hn, enc_cn) = autoencoder.enc(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                        torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]))\n",
    "    #print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "    metadata = dec_beamsearch(encoder_hidden = (enc_hn, enc_cn), encoder_outputs = enc_outputs)\n",
    "\n",
    "    results = metadata['topk_sequence']\n",
    "    results =torch.cat(results, dim = 2)\n",
    "    results=results.view(sample_num*topk, -1)\n",
    "    indices = torch.LongTensor([x*topk for x in range(sample_num)]).cuda()\n",
    "    results = results.data.index_select(0, indices)\n",
    "    results=results.cpu().tolist()\n",
    "    results=batch_tokens_remove_eos(results, vocab)\n",
    "#     results=batch_tokens2words(results, vocab)\n",
    "#     results=batch_words2sentence(results)\n",
    "#     print(results)\n",
    "    \n",
    "    inputs = train_set_inputs[start_idx:start_idx+sample_num]\n",
    "#     inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "#     inputs = batch_tokens2words(inputs, vocab)\n",
    "    inputs_=[]\n",
    "    for tokens in inputs:\n",
    "        x=[]\n",
    "        for token in tokens:\n",
    "            if token!=vocab.word2token['<padding>']:\n",
    "                x.append(token)\n",
    "            else:\n",
    "                break\n",
    "        inputs_.append(x)\n",
    "#     inputs = batch_words2sentence(inputs_)\n",
    "    bleu_scores = batch_tokens_bleu(references=inputs_, candidates=results)\n",
    "    for score in bleu_scores:\n",
    "        bleu_sum+=score\n",
    "    \n",
    "    if batch_id%50==1:\n",
    "        print(batch_id, int(train_set_size/sample_num), 'time: %4.2f mins'%((time.time()-start_time)/60), 'bleu: %2.4f'%(bleu_sum/batch_id/sample_num))\n",
    "    \n",
    "    \n",
    "\n",
    "#    print inputs\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "# print(inputs)\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "for sent in inputs:\n",
    "    print(sent)\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print('results[0] size: ', results[0].size())\n",
    "a=torch.cat(results, dim = 2)\n",
    "b=a.view(sample_num*topk, -1)\n",
    "print(a.size(), b.size())\n",
    "c=b.data.cpu().tolist()\n",
    "d=batch_tokens_remove_eos(c, vocab)\n",
    "e=batch_tokens2words(d, vocab)\n",
    "f=batch_words2sentence(e)\n",
    "for sent in f:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 17, 15, 10, 24, 13, 12, 7, 9, 10, 7, 7, 9, 9, 10, 8, 15, 8, 8, 7] <class 'list'>\n",
      "[9, 16, 14, 9, 23, 12, 11, 6, 8, 9, 6, 6, 8, 8, 9, 7, 14, 7, 7, 6]\n",
      "[-3.24204826 -4.40653467 -4.47240829 -4.8347168  -5.28275204 -5.37414074\n",
      " -5.63503647 -5.94044685 -6.03672504 -6.05442619]\n",
      "[-1.97154188 -4.15584326 -4.34271812 -4.98236847 -5.01742077 -5.13382244\n",
      " -5.46063137 -5.65393639 -5.77483845 -5.95366859]\n",
      "[-2.59985161 -3.37922096 -3.99874973 -4.41872597 -4.78718185 -5.04994202\n",
      " -5.17642212 -5.33260918 -5.58263397 -5.71078587]\n",
      "[-0.11981249 -3.18128395 -5.4015274  -5.69001198 -5.82326365 -5.95669365\n",
      " -6.435359   -6.64287663 -6.64724827 -6.87408447]\n",
      "[-18.40380478 -19.78309059 -20.26621246 -20.76094055 -20.89505768\n",
      " -21.00291443 -21.50175095 -21.6151619  -21.91933441 -22.45663071]\n",
      "[-0.17099404 -2.67351103 -4.61692524 -5.25170612 -5.80711746 -6.28423071\n",
      " -6.34242487 -6.52443314 -6.83719969 -6.89309406]\n",
      "[-0.56672263 -3.50312901 -3.75772285 -4.08013439 -4.53574657 -4.60605526\n",
      " -4.85199547 -5.01956892 -5.58189583 -5.69109535]\n",
      "[ -0.00044727  -8.38636208  -9.33905125 -10.1850338  -11.06351948\n",
      " -11.3778801  -12.19608498 -12.41390038 -12.6096077  -12.61773872]\n",
      "[ -0.06731939  -2.77733278  -6.43195295  -8.41417217  -8.70567322\n",
      "  -9.16070461  -9.51211834  -9.73429775 -10.0713625  -10.15775681]\n",
      "[-0.49253297 -4.31389236 -4.47913885 -4.96476364 -5.12851524 -5.31493759\n",
      " -5.36206055 -5.38765907 -5.45377827 -5.45527363]\n",
      "[ -0.00388718  -6.41149139  -6.71000767  -7.8097744   -8.36080551\n",
      "  -9.56061459  -9.68745995 -10.45626163 -11.18494415 -11.56262016]\n",
      "[ -0.00546646  -5.32893944  -7.56072235 -10.17421341 -11.8360424\n",
      " -11.83705521 -12.01098633 -12.19685936 -12.46723175 -12.57502365]\n",
      "[-1.13505983 -1.3166914  -1.49998188 -3.64936495 -3.73295259 -3.99481487\n",
      " -4.43619919 -4.53348875 -4.64604473 -4.7325449 ]\n",
      "[-0.77765512 -2.43683195 -3.57746267 -4.04319048 -4.25203657 -4.35352564\n",
      " -4.41637611 -4.65082693 -4.68896914 -4.76842403]\n",
      "[-1.49901772 -2.54335022 -3.30813408 -3.33161163 -3.57518959 -3.99632835\n",
      " -4.55832863 -4.63204002 -5.06945229 -5.94569016]\n",
      "[-0.0170393  -4.46509552 -6.34691715 -7.26027679 -8.27240944 -8.69937706\n",
      " -8.71121693 -8.94822216 -9.13085747 -9.17534828]\n",
      "[-7.49579525 -7.80059624 -8.09021664 -8.0959816  -8.14993954 -8.2588129\n",
      " -8.28162003 -8.38532925 -8.41560841 -9.35695648]\n",
      "[-1.07414961 -1.35608292 -4.44886494 -4.45501137 -4.83405209 -4.9552784\n",
      " -5.21141815 -5.56115055 -5.68401241 -5.77346134]\n",
      "[-0.03566742 -4.33280849 -6.65749073 -6.67450809 -6.86815262 -7.1146574\n",
      " -7.44696808 -7.51969433 -7.68854427 -7.7718029 ]\n",
      "[-0.00240231 -7.48697758 -7.8816309  -8.64649105 -8.97883892 -9.32128811\n",
      " -9.53497028 -9.68011475 -9.75339222 -9.87466812]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "meta_lens = metadata['length']\n",
    "print(meta_lens, type(meta_lens))\n",
    "print(sent_lens)\n",
    "\n",
    "seq_score = metadata['score']\n",
    "lseq_socre = seq_score.data.cpu().numpy()\n",
    "for x in lseq_socre:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "pre_train = torch.load('./models_better/time-[2019-01-08-06-27-50]-loss-0.925657809-bleu-0.6640-hidden_dim-256-input_dim-300-epoch-2-batch_size-200-batch_id-[17001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "autoencoder.eval()\n",
    "\n",
    "train_set_size = len(train_set_inputs)\n",
    "sample_num=200\n",
    "batch_id=0\n",
    "bleu_sum=0\n",
    "\n",
    "start_time=time.time()\n",
    "for start_idx in range(0, train_set_size-sample_num, sample_num):\n",
    "    batch_id+=1\n",
    "    \n",
    "    enc_outputs, (enc_hn, enc_cn) = autoencoder.enc(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                        torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]))\n",
    "    predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]), \n",
    "                                             labels=999, \n",
    "                                             is_train=0, teaching_rate=1)\n",
    "\n",
    "    results=batch_tokens_remove_eos(predicts, vocab)\n",
    "#     results=batch_tokens2words(results, vocab)\n",
    "#     results=batch_words2sentence(results)\n",
    "#     print(results)\n",
    "    \n",
    "    inputs = train_set_inputs[start_idx:start_idx+sample_num]\n",
    "#     inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "#     inputs = batch_tokens2words(inputs, vocab)\n",
    "    inputs_=[]\n",
    "    for tokens in inputs:\n",
    "        x=[]\n",
    "        for token in tokens:\n",
    "            if token!=vocab.word2token['<padding>']:\n",
    "                x.append(token)\n",
    "            else:\n",
    "                break\n",
    "        inputs_.append(x)\n",
    "#     inputs = batch_words2sentence(inputs_)\n",
    "    bleu_scores = batch_tokens_bleu(references=inputs_, candidates=results)\n",
    "    for score in bleu_scores:\n",
    "        bleu_sum+=score\n",
    "    \n",
    "    if batch_id%20==1:\n",
    "        print(batch_id, int(train_set_size/sample_num), 'time: %4.2f mins'%((time.time()-start_time)/60), 'bleu: %2.4f'%(bleu_sum/batch_id/sample_num))\n",
    "    \n",
    "    \n",
    "\n",
    "#    print inputs\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "# print(inputs)\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "for sent in inputs:\n",
    "    print(sent)\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print('results[0] size: ', results[0].size())\n",
    "a=torch.cat(results, dim = 2)\n",
    "b=a.view(sample_num*topk, -1)\n",
    "print(a.size(), b.size())\n",
    "c=b.data.cpu().tolist()\n",
    "d=batch_tokens_remove_eos(c, vocab)\n",
    "e=batch_tokens2words(d, vocab)\n",
    "f=batch_words2sentence(e)\n",
    "for sent in f:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "\n",
    "enc = Encoder(use_cuda=use_cuda, \n",
    "            hidden_dim=hidden_dim, \n",
    "            input_dim=input_dim, \n",
    "            vocab=vocab\n",
    "           )\n",
    "if use_cuda:\n",
    "    enc = enc.cuda()\n",
    "    \n",
    "sample_num = 11\n",
    "print('sentences length: ', train_set_input_lens[0:sample_num])\n",
    "\n",
    "enc_outputs, (enc_hn, enc_cn) = enc(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "                                    torch.LongTensor(train_set_input_lens[0:sample_num]))\n",
    "print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "dec = Decoder(use_cuda=use_cuda, encoder=enc, hidden_dim=hidden_dim, max_length=25)\n",
    "if use_cuda:\n",
    "    dec = dec.cuda()\n",
    "    \n",
    "# loss, predicts = dec(enc_outputs = enc_outputs, \n",
    "#                     h0_and_c0=(enc_hn, enc_cn), \n",
    "#                     sent_lens=train_set_input_lens[0:sample_num], \n",
    "#                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "#                     is_train=1, teaching_rate = 1\n",
    "#                     )\n",
    "# print('loss is %4.7f'%loss.data[0])\n",
    "\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "\n",
    "pre_train = torch.load('./models_better/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "\n",
    "autoencoder.eval()\n",
    "predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "                                     torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "                                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "                                     is_train=0, teaching_rate=1)\n",
    "\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "results = batch_tokens_remove_eos(predicts, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "results = batch_tokens2words(results, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "results = batch_words2sentence(results)\n",
    "for inp, res in zip(inputs,results):\n",
    "    print(inp)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "lr=0.005\n",
    "batch_size=200\n",
    "train_set_size=int(len(train_set_inputs)/2)\n",
    "epochs=10\n",
    "train_bleu = 0\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 25)\n",
    "#pre train para\n",
    "#pre_train = torch.load('./models_better/loss-2.099016905-bleu-0.4078-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[7001-[of]-21743]-lr-0.0050')\n",
    "#pre_train = torch.load('./models_better/time-[2019-01-07-16-38-14]-loss-1.881381631-bleu-0.5340-hidden_dim-256-input_dim-300-epoch-0-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "pre_train = torch.load('./models_better/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, autoencoder.parameters()), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def model_train(epoch, batch_size, train_set_size):\n",
    "    batch_id = 0\n",
    "    valid_bleu = 0\n",
    "    for start_idx in range(0, train_set_size-batch_size, batch_size):\n",
    "        batch_id+=1\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        optimizer.zero_grad()#clear\n",
    "        loss, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "                                             torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[start_idx:end_idx]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "        #optimize\n",
    "        loss.backward()#retain_graph=True)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_id%50==1:\n",
    "            autoencoder.eval()\n",
    "            sample_num = 10\n",
    "            rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "            #teaching forcing\n",
    "            loss_, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            tokenized_sents=predicts.tolist()\n",
    "            real_sents=[]\n",
    "            label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "            label_real_sents=[]\n",
    "            for idx, sent in enumerate(tokenized_sents):\n",
    "                real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "            for sent in label_tokenized_sents:\n",
    "                label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "            print('train_set sample: ', rand_idx)\n",
    "            for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "                print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            #no teaching forcing\n",
    "            print('----no teaching forcing----')\n",
    "            predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=0, teaching_rate=1)\n",
    "            tokenized_sents=predicts.tolist()\n",
    "            real_sents=[]\n",
    "            label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "            label_real_sents=[]\n",
    "            for idx, sent in enumerate(tokenized_sents):\n",
    "                real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "            for sent in label_tokenized_sents:\n",
    "                label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "            for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "                print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            info_stamp = 'loss-{:2.9f}-batch_size-{:n}-epoch-{:n}-batch_id-({:n}/{:n})'.format(\n",
    "                              loss.data[0], batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "            print(info_stamp)\n",
    "            #valid_set testing\n",
    "            if batch_id%1000==1:\n",
    "                rand_idx=random.randint(0, len(valid_set_inputs)-batch_size-1-1)\n",
    "                predicts = autoencoder.forward(torch.LongTensor(valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "                                                 torch.LongTensor(valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "                                                 labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "                                                 is_train=0, teaching_rate=1)\n",
    "                tokenized_sents=predicts.tolist()\n",
    "                real_sents=[]\n",
    "                label_tokenized_sents=valid_set_labels[rand_idx:rand_idx+batch_size]\n",
    "                label_real_sents=[]\n",
    "                for idx, sent in enumerate(tokenized_sents):\n",
    "                    real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "                for sent in label_tokenized_sents:\n",
    "                    label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "                bleu_score, valid_num = data_set_bleu(label_real_sents, real_sents)\n",
    "                if valid_num>10:\n",
    "                    valid_bleu = bleu_score/valid_num\n",
    "                       \n",
    "                info_stamp = 'loss-{:2.9f}-bleu-{:1.4f}-hidden_dim-{:n}-input_dim-{:n}-epoch-{:n}-batch_size-{:n}-batch_id-[{:n}-[of]-{:n}]-lr-{:1.4f}'.format(\n",
    "                              loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "                print(valid_num, info_stamp)\n",
    "                now = int(round(time.time()*1000))\n",
    "                time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "                torch.save(autoencoder.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "                \n",
    "            autoencoder.train()\n",
    "            \n",
    "for epoch in range(epochs):\n",
    "    model_train(epoch, batch_size, train_set_size)\n",
    "    \n",
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "pre_train = torch.load('./models_better/loss-3.966628313-bleu-0.3201-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[1001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "print('a')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
