{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def tokenized_sent2real_sent(tokenized_sent, vocab):\n",
    "#     real_sent=[]\n",
    "#     for token in tokenized_sent:\n",
    "#         if token == vocab.word2token['<eos>']:\n",
    "#             break\n",
    "#         else:\n",
    "#             real_sent.append(vocab.token2word[token])\n",
    "#     return ''.join(real_sent)\n",
    "\n",
    "# def reverse_tokenized_sent2real_sent(tokenized_sent, vocab):\n",
    "#     real_sent=[]\n",
    "#     for token in tokenized_sent:\n",
    "#         if token == vocab.word2token['<eos>']:\n",
    "#             break\n",
    "#         else:\n",
    "#             real_sent.append(vocab.token2word[token])\n",
    "#     real_sent.reverse()\n",
    "#     return ''.join(real_sent)\n",
    "\n",
    "# def data_set_bleu(sents1, sents2):\n",
    "#     cnt=0\n",
    "#     bleu_score_sum=0\n",
    "    \n",
    "#     for sent1, sent2 in zip(sents1, sents2):\n",
    "#         if min(len(sent1), len(sent2))<4:\n",
    "#             pass\n",
    "#         else:\n",
    "#             cnt+=1\n",
    "#             bleu_score_sum = sentence_bleu([list(sent1)], list(sent2))+bleu_score_sum\n",
    "            \n",
    "#     return bleu_score_sum, cnt\n",
    "\n",
    "def batch_words2sentence(words_list):\n",
    "    return [''.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./small_data_set/train_set_inputs_10w.pk', 'rb') as f:\n",
    "    train_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/train_set_input_lens_10w.pk', 'rb') as f:\n",
    "    train_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/train_set_labels_10w.pk', 'rb') as f:\n",
    "    train_set_labels = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_inputs_10w.pk', 'rb') as f:\n",
    "    valid_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_input_lens_10w.pk', 'rb') as f:\n",
    "    valid_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_labels_10w.pk', 'rb') as f:\n",
    "    valid_set_labels = pickle.load(f)\n",
    "\n",
    "\n",
    "# with open('./data_set/train_set_inputs.pk', 'rb') as f:\n",
    "#     train_set_inputs = pickle.load(f)\n",
    "# with open('./data_set/train_set_input_lens.pk', 'rb') as f:\n",
    "#     train_set_input_lens = pickle.load(f)\n",
    "# with open('./data_set/train_set_labels.pk', 'rb') as f:\n",
    "#     train_set_labels = pickle.load(f)\n",
    "    \n",
    "# with open('./data_set/valid_set_inputs.pk', 'rb') as f:\n",
    "#     valid_set_inputs = pickle.load(f)\n",
    "# with open('./data_set/valid_set_input_lens.pk', 'rb') as f:\n",
    "#     valid_set_input_lens = pickle.load(f)\n",
    "# with open('./data_set/valid_set_labels.pk', 'rb') as f:\n",
    "#     valid_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num=100000\n",
    "# train_set_inputs_10w = train_set_inputs[:num]\n",
    "# train_set_input_lens_10w = train_set_input_lens[:num]\n",
    "# train_set_labels_10w = train_set_labels[:num]\n",
    "\n",
    "# valid_set_inputs_10w = valid_set_inputs[:num]\n",
    "# valid_set_input_lens_10w = valid_set_input_lens[:num]\n",
    "# valid_set_labels_10w = valid_set_labels[:num]\n",
    "\n",
    "\n",
    "# with open('./small_data_set/train_set_inputs_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_inputs_10w,f)\n",
    "# with open('./small_data_set/train_set_input_lens_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_input_lens_10w ,f)\n",
    "# with open('./small_data_set/train_set_labels_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_labels_10w ,f)\n",
    "    \n",
    "# with open('./small_data_set/valid_set_inputs_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_inputs_10w ,f)\n",
    "# with open('./small_data_set/valid_set_input_lens_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_input_lens_10w ,f)\n",
    "# with open('./small_data_set/valid_set_labels_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_labels_10w ,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_inputs), len(train_set_input_lens), len(train_set_labels))\n",
    "\n",
    "# for sent_len in valid_set_input_lens:\n",
    "#     if sent_len<=2:\n",
    "#         print('why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('pre_train_word_embedding.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "#         self.embed.weight.requires_grad = False\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "        #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "            cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "            cn = cn.index_select(0, Variable(true_order_ids))\n",
    "            \n",
    "        return outputs, (hn,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _inflate(tensor, times, dim):\n",
    "    \"\"\"\n",
    "    Examples::\n",
    "        >> a = torch.LongTensor([[1, 2], [3, 4]])\n",
    "        >> a\n",
    "        1   2\n",
    "        3   4\n",
    "        [torch.LongTensor of size 2x2]\n",
    "        >> b = ._inflate(a, 2, dim=1)\n",
    "        >> b\n",
    "        1   2   1   2\n",
    "        3   4   3   4\n",
    "        [torch.LongTensor of size 2x4]\n",
    "    \"\"\"\n",
    "    repeat_dims = [1] * tensor.dim()\n",
    "    repeat_dims[dim] = times\n",
    "    return tensor.repeat(*repeat_dims)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, use_cuda, encoder, hidden_dim, max_length=25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.input_dim = encoder.input_dim\n",
    "        self.max_length = max_length\n",
    "        self.vocab = encoder.vocab\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        #self.weight[self.vocab.word2token['<eos>']]=1.01\n",
    "        #self.weight[self.vocab.word2token['<split>']]=1.01\n",
    "        \n",
    "        self.hidden_size = self.hidden_dim\n",
    "        self.V = len(self.vocab.word2token)\n",
    "        self.SOS = self.vocab.word2token['<sos>']\n",
    "        self.EOS = self.vocab.word2token['<eos>']\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lstmcell = torch.nn.LSTMCell(input_size=self.input_dim, hidden_size=self.hidden_dim*2, bias=True)\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=encoder.embed# reference share\n",
    "        #fcnn: projection for crossentroy loss\n",
    "        self.fcnn = nn.Linear(in_features = self.hidden_dim*2, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.cost_func = nn.CrossEntropyLoss(weight=torch.Tensor(self.weight), reduce=False)\n",
    "\n",
    "        print('init lookup embedding matrix size: ', self.embed.weight.data.size())\n",
    "        \n",
    "\n",
    "    def forward(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = []\n",
    "        predicts = []\n",
    "        max_probs=[]\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "\n",
    "        for ii in range(self.max_length+1):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,0])\n",
    "                    t=torch.mean(loss)\n",
    "                    all_loss.append(torch.unsqueeze(loss, dim=1))\n",
    "                \n",
    "                #last_timestep_output = self.log_softmax(last_timestep_output)\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 max_prob = self.cost_func(last_timestep_output, labels[:,0])\n",
    "#                 max_probs.append(torch.unsqueeze(max_prob, dim=0))\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #print('max_prob size: ', max_prob.size())\n",
    "                #print('max_idxs size: ',max_idxs.size(), max_idxs)\n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "                    t=torch.mean(loss)\n",
    "                    all_loss.append(torch.unsqueeze(loss, dim=1))\n",
    "#                 _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 #print('max_idx size: ', max_idxs.size(), max_idxs)\n",
    "#                 predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #last_timestep_output = self.log_softmax(last_timestep_output)\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 max_prob = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "#                 max_probs.append(torch.unsqueeze(max_prob, dim=0))\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "#         max_probs = torch.cat(max_probs, dim=0)\n",
    "#         max_probs = torch.transpose(max_probs, 0, 1)\n",
    "#         print('max_probs size: ', max_probs.size())\n",
    "        \n",
    "#         sents_prob = torch.mean(max_probs, dim=1)\n",
    "#         print('sents_prob size: ', sents_prob.size())\n",
    "#         loss = torch.mean(sents_prob)\n",
    "#         loss=0\n",
    "#         for i in all_loss:\n",
    "#             loss+=i\n",
    "#         loss/=len(all_loss)\n",
    "        all_loss = torch.cat(all_loss, dim=1)\n",
    "        all_loss = torch.mean(all_loss, dim=1)\n",
    "        loss = torch.mean(all_loss)\n",
    "        #print('loss size: ', loss.size())\n",
    "        #torch.cuda.empty_cache()\n",
    "        if is_train:  #training\n",
    "            if self.use_cuda:\n",
    "                return loss, predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return loss, predicts.data.numpy()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return predicts.data.numpy()\n",
    "#         if is_train:  #training\n",
    "#             if self.use_cuda:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.numpy()\n",
    "#         else:   #testing\n",
    "#             if self.use_cuda:\n",
    "#                 return predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return predicts.data.numpy()\n",
    "    def loss_func_rl(self, logits):\n",
    "        logprobs = self.log_softmax(logits)\n",
    "        max_logprobs, max_ids = torch.max(logprobs, dim=1)\n",
    "        weights = torch.FloatTensor(self.weight)\n",
    "        index = max_ids.data\n",
    "        if self.use_cuda:\n",
    "            index = index.cpu()\n",
    "        weights = Variable(torch.index_select(weights, 0, index), requires_grad=False)\n",
    "        if self.use_cuda:\n",
    "            weights = weights.cuda()\n",
    "        return -max_logprobs*weights\n",
    "    \n",
    "    def token_sample(self, logits, batch_size, topk=100, e_greedy_rate=0.2, dec_seq_num=10):\n",
    "        topk_logits, topk_idxs = torch.topk(logits, k=topk, dim=1)\n",
    "        if self.use_cuda:\n",
    "            ltopk_probs = self.softmax(topk_logits).data.cpu().tolist()\n",
    "            ltopk_idxs = topk_idxs.data.cpu().tolist()\n",
    "        else:\n",
    "            ltopk_probs = self.softmax(topk_logits).data.tolist()\n",
    "            ltopk_idxs = topk_idxs.data.tolist()\n",
    "        batch_results=[]\n",
    "        for probs, idxs in zip(ltopk_probs, ltopk_idxs):\n",
    "            sum_probs = sum(probs)\n",
    "            probs = [x/sum_probs for x in probs]\n",
    "            results=[]\n",
    "            for ii in range(dec_seq_num):\n",
    "                result = np.random.choice(idxs, p=probs)\n",
    "                if random.random()>e_greedy_rate:\n",
    "                    results.append(int(result))\n",
    "                else:\n",
    "                    results.append(random.randint(0, len(self.vocab.word2token)-1))\n",
    "            batch_results.append(results)\n",
    "        return batch_results\n",
    "    \n",
    "    def train_using_rl_2(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        #nll_loss = nn.NLLLoss(weight=torch.FloatTensor(self.weight).cuda(), reduce=False)\n",
    "        \n",
    "        # parameter labels must be python list type\n",
    "        labels_tokens=labels\n",
    "        labels = Variable(torch.LongTensor(labels))\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "        #print('rl...')\n",
    "        loss=0\n",
    "        all_loss = 0\n",
    "        losss=[]\n",
    "        predicts = []\n",
    "        max_probs=[]\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        topk=20\n",
    "        e_greedy_rate=0.3\n",
    "        dec_seq_num=100\n",
    "        vocab_size = len(self.vocab.word2token)\n",
    "        index_bias = torch.LongTensor([vocab_size*ii for ii in range(batch_size)]).unsqueeze(dim=1)\n",
    "        index_bias = index_bias.expand(batch_size, dec_seq_num).contiguous().view(-1)\n",
    "        if self.use_cuda:\n",
    "            index_bias = index_bias.cuda()\n",
    "        for ii in range(self.max_length+1):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "\n",
    "                batch_ltokens = self.token_sample(logits=last_timestep_output, batch_size=batch_size, topk=topk, e_greedy_rate=e_greedy_rate, dec_seq_num=dec_seq_num)\n",
    "                #print('batch_ltokens size: ', torch.LongTensor(batch_ltokens).size())\n",
    "                batch_ltokens=torch.LongTensor(batch_ltokens).view(-1)\n",
    "                if self.use_cuda:\n",
    "                    batch_ltokens = batch_ltokens.cuda()\n",
    "                predicts.append(batch_ltokens.view(-1,1))\n",
    "                if is_train:\n",
    "                    neg_log_prob = -self.log_softmax(last_timestep_output).view(-1)\n",
    "                    #print('batch_ltokens and index_bias size: ', batch_ltokens.size(), index_bias.size())\n",
    "                    indices = (batch_ltokens+index_bias)\n",
    "                    \n",
    "                    loss=neg_log_prob[indices]\n",
    "#                     loss = self.cost_func(last_timestep_output, pseudo_labels)#labels[:,ii])\n",
    "#                     losss.append(torch.unsqueeze(loss, dim=1))\n",
    "#                 if is_train:\n",
    "#                     loss = self.cost_func(last_timestep_output, max_idxs)#labels[:,0])\n",
    "#                     losss.append(torch.unsqueeze(loss, dim=1))\n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                \n",
    "                batch_ltokens = self.token_sample(logits=last_timestep_output, batch_size=batch_size, topk=topk, e_greedy_rate=e_greedy_rate, dec_seq_num=dec_seq_num)\n",
    "                batch_ltokens=torch.LongTensor(batch_ltokens).view(-1)\n",
    "                if self.use_cuda:\n",
    "                    batch_ltokens = batch_ltokens.cuda()\n",
    "                predicts.append(batch_ltokens.view(-1,1))\n",
    "                if is_train:\n",
    "                    neg_log_prob = -self.log_softmax(last_timestep_output).view(-1)\n",
    "                    indices = (batch_ltokens+index_bias)\n",
    "                    \n",
    "                    loss+=neg_log_prob[indices]\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            predicts = predicts.cpu().numpy()\n",
    "        else:\n",
    "            predicts = predicts.numpy()\n",
    "        \n",
    "        predicts_tokens=batch_tokens_remove_eos(predicts.tolist(), self.vocab)\n",
    "        labels_tokens=[x for x in labels_tokens for ii in range(dec_seq_num)]\n",
    "        labels_tokens=batch_tokens_remove_eos(labels_tokens, self.vocab)\n",
    "        bleu_scores = batch_tokens_bleu(references=labels_tokens, candidates=predicts_tokens, smooth_epsilon=0.01)\n",
    "#         predicts_words=batch_tokens2words(predicts_tokens, self.vocab)\n",
    "#         predicts_sents=batch_words2sentence(predicts_words)\n",
    "        labels_words = batch_tokens2words(labels_tokens, self.vocab)\n",
    "        labels_sents = batch_words2sentence(labels_words)\n",
    "#         print(len(labels_sents))\n",
    "#         for ii in labels_sents[:21]:\n",
    "#             print(ii)\n",
    "        \n",
    "        bleu_scores = torch.FloatTensor(bleu_scores).view(batch_size, dec_seq_num)\n",
    "        bleu_mean = torch.mean(bleu_scores, dim=1).unsqueeze(dim=1)\n",
    "        bleu_scores = bleu_scores-bleu_mean\n",
    "        bleu_scores = bleu_scores.view(-1)\n",
    "        \n",
    "        bleu_scores = Variable(bleu_scores, requires_grad = 0)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            bleu_scores = bleu_scores.cuda()\n",
    "            \n",
    "        loss = torch.dot(loss, bleu_scores)/batch_size/dec_seq_num\n",
    "        \n",
    "        return loss, predicts, torch.mean(bleu_mean.squeeze())\n",
    "    \n",
    "    def train_using_rl_3(self, enc_outputs, h0_and_c0, labels, topk=10):\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        metadata = self.decode_by_beamsearch(encoder_hidden=h0_and_c0, encoder_outputs=enc_outputs, topk = topk)\n",
    "        results = metadata['topk_sequence']\n",
    "        results =torch.cat(results, dim = 2)\n",
    "        results=results.view(batch_size*topk, -1)\n",
    "        if self.use_cuda:\n",
    "            results = results.data.cpu().tolist()\n",
    "        else:\n",
    "            results = results.data.tolist()\n",
    "        results=batch_tokens_remove_eos(results, self.vocab)\n",
    "\n",
    "        labels = [x for x in labels for ii in range(topk)]\n",
    "        labels = batch_tokens_remove_eos(labels, self.vocab)\n",
    "        bleu_scores = batch_tokens_bleu(references=labels, candidates=results, smooth_epsilon=0.01)\n",
    "        \n",
    "        bleu_scores = torch.FloatTensor(bleu_scores).view(batch_size, topk)\n",
    "        bleu_max, _ = torch.max(bleu_scores, dim=1)\n",
    "        \n",
    "        bleu_mean = torch.mean(bleu_scores, dim=1).unsqueeze(dim=1)\n",
    "        bleu_scores = bleu_scores-bleu_mean\n",
    "        bleu_scores = bleu_scores.view(-1)\n",
    "        \n",
    "        bleu_scores = self._tocuda(Variable(bleu_scores, requires_grad = 0))\n",
    "        \n",
    "        log_probs = metadata['score']\n",
    "        log_probs = log_probs.view(batch_size*topk)\n",
    "        \n",
    "        loss = -torch.dot(log_probs, bleu_scores)/batch_size/topk\n",
    "        \n",
    "        return loss, results, torch.mean(bleu_mean.squeeze()), torch.mean(bleu_max)\n",
    "        \n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var\n",
    "    def decode_by_beamsearch(self, encoder_hidden=None, encoder_outputs=None, topk = 10):\n",
    "        self.k = topk\n",
    "        batch_size = encoder_outputs.size(dim=0)\n",
    "        \n",
    "        self.pos_index = self._tocuda(Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1))\n",
    "\n",
    "        hidden = tuple([_inflate(h, self.k, 1).view(batch_size*self.k, -1) for h in encoder_hidden])\n",
    "        #print('hidden0 size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "\n",
    "        # Initialize the scores; for the first step,\n",
    "        # ignore the inflated copies to avoid duplicate entries in the top k\n",
    "        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n",
    "        sequence_scores.fill_(-float('Inf'))\n",
    "        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n",
    "        sequence_scores = self._tocuda(Variable(sequence_scores))\n",
    "\n",
    "        # Initialize the input vector\n",
    "        input_var = self._tocuda(Variable(torch.LongTensor([self.SOS] * batch_size * self.k)))\n",
    "\n",
    "        # Store decisions for backtracking\n",
    "        stored_outputs = list()\n",
    "        stored_scores = list()\n",
    "        stored_predecessors = list()\n",
    "        stored_emitted_symbols = list()\n",
    "        stored_hidden = list()\n",
    "\n",
    "        for ii in range(0, self.max_length):\n",
    "            # Run the RNN one step forward\n",
    "            #print('setp: %s'%ii)\n",
    "            input_vec = self.embed(input_var)\n",
    "            #print('input_var and input_vec size: ', input_var.size(), input_vec.size())\n",
    "            hidden = self.lstmcell(input_vec, hidden)\n",
    "            #print('hidden size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "            \n",
    "            log_softmax_output = self.log_softmax(self.fcnn(hidden[0]))\n",
    "\n",
    "            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n",
    "            sequence_scores = _inflate(sequence_scores, self.V, 1)\n",
    "            sequence_scores += log_softmax_output.squeeze(1)\n",
    "            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n",
    "\n",
    "            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n",
    "            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n",
    "            sequence_scores = scores.view(batch_size * self.k, 1)\n",
    "\n",
    "            # Update fields for next timestep\n",
    "            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple([h.index_select(0, predecessors.squeeze()) for h in hidden])\n",
    "            else:\n",
    "                hidden = hidden.index_select(0, predecessors.squeeze())\n",
    "\n",
    "            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n",
    "            stored_scores.append(sequence_scores.clone())\n",
    "            eos_indices = input_var.data.eq(self.EOS)\n",
    "            if eos_indices.nonzero().dim() > 0:\n",
    "                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n",
    "\n",
    "            # Cache results for backtracking\n",
    "            stored_predecessors.append(predecessors)\n",
    "            stored_emitted_symbols.append(input_var)\n",
    "#             stored_hidden.append(hidden)\n",
    "\n",
    "        # Do backtracking to return the optimal values\n",
    "        output, h_t, h_n, s, l, p = self._backtrack(hidden,\n",
    "                                                    stored_predecessors, stored_emitted_symbols,\n",
    "                                                    stored_scores, batch_size, self.hidden_size)\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        metadata['score'] = s\n",
    "        metadata['topk_length'] = l\n",
    "        metadata['topk_sequence'] = p\n",
    "        metadata['length'] = [seq_len[0] for seq_len in l]\n",
    "        metadata['sequence'] = [seq[0] for seq in p]\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def _backtrack(self, hidden, predecessors, symbols, scores, b, hidden_size):\n",
    "        \"\"\"Backtracks over batch to generate optimal k-sequences.\n",
    "\n",
    "        Args:\n",
    "            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n",
    "            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n",
    "            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n",
    "            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n",
    "            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n",
    "            b: Size of the batch\n",
    "            hidden_size: Size of the hidden state\n",
    "\n",
    "        Returns:\n",
    "            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n",
    "\n",
    "            score [batch, k]: A list containing the final scores for all top-k sequences\n",
    "\n",
    "            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n",
    "\n",
    "            p (batch, k, sequence_len): A Tensor containing predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        lstm = isinstance(hidden, tuple)\n",
    "\n",
    "        # initialize return variables given different types\n",
    "        output = list()\n",
    "        h_t = list()\n",
    "        p = list()\n",
    "        # Placeholder for last hidden state of top-k sequences.\n",
    "        # If a (top-k) sequence ends early in decoding, `h_n` contains\n",
    "        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n",
    "        # the last hidden state of decoding.\n",
    "        if lstm:\n",
    "            state_size = hidden[0].size()\n",
    "            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n",
    "        else:\n",
    "            h_n = torch.zeros(nw_hidden[0].size())\n",
    "        l = [[self.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n",
    "                                                                # Similar to `h_n`\n",
    "\n",
    "        # the last step output of the beams are not sorted\n",
    "        # thus they are sorted here\n",
    "        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n",
    "        # initialize the sequence scores with the sorted last step beam scores\n",
    "        s = sorted_score.clone()\n",
    "\n",
    "        batch_eos_found = [0] * b   # the number of EOS found\n",
    "                                    # in the backward loop below for each batch\n",
    "\n",
    "        t = self.max_length - 1\n",
    "        # initialize the back pointer with the sorted order of the last step beams.\n",
    "        # add self.pos_index for indexing variable with b*k as the first dimension.\n",
    "        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n",
    "        while t >= 0:\n",
    "            # Re-order the variables with the back pointer\n",
    "            current_symbol = symbols[t].index_select(0, t_predecessors)\n",
    "            # Re-order the back pointer of the previous step with the back pointer of\n",
    "            # the current step\n",
    "            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n",
    "\n",
    "            # This tricky block handles dropped sequences that see EOS earlier.\n",
    "            # The basic idea is summarized below:\n",
    "            #\n",
    "            #   Terms:\n",
    "            #       Ended sequences = sequences that see EOS early and dropped\n",
    "            #       Survived sequences = sequences in the last step of the beams\n",
    "            #\n",
    "            #       Although the ended sequences are dropped during decoding,\n",
    "            #   their generated symbols and complete backtracking information are still\n",
    "            #   in the backtracking variables.\n",
    "            #   For each batch, everytime we see an EOS in the backtracking process,\n",
    "            #       1. If there is survived sequences in the return variables, replace\n",
    "            #       the one with the lowest survived sequence score with the new ended\n",
    "            #       sequences\n",
    "            #       2. Otherwise, replace the ended sequence with the lowest sequence\n",
    "            #       score with the new ended sequence\n",
    "            #\n",
    "            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n",
    "            if eos_indices.dim() > 0:\n",
    "                for i in range(eos_indices.size(0)-1, -1, -1):\n",
    "                    # Indices of the EOS symbol for both variables\n",
    "                    # with b*k as the first dimension, and b, k for\n",
    "                    # the first two dimensions\n",
    "                    idx = eos_indices[i]\n",
    "                    b_idx = int(idx[0] / self.k)\n",
    "                    # The indices of the replacing position\n",
    "                    # according to the replacement strategy noted above\n",
    "                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n",
    "                    batch_eos_found[b_idx] += 1\n",
    "                    res_idx = b_idx * self.k + res_k_idx\n",
    "\n",
    "                    # Replace the old information in return variables\n",
    "                    # with the new ended sequence information\n",
    "                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n",
    "\n",
    "                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n",
    "                    s[b_idx, res_k_idx] = scores[t][idx[0]]\n",
    "                    l[b_idx][res_k_idx] = t + 1\n",
    "\n",
    "            # record the back tracked results\n",
    "            p.append(current_symbol)\n",
    "            t -= 1\n",
    "\n",
    "        # Sort and re-order again as the added ended sequences may change\n",
    "        # the order (very unlikely)\n",
    "        s, re_sorted_idx = s.topk(self.k)\n",
    "        for b_idx in range(b):\n",
    "            l[b_idx] = [l[b_idx][k_idx.data[0]] for k_idx in re_sorted_idx[b_idx,:]]\n",
    "\n",
    "        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n",
    "\n",
    "        # Reverse the sequences and re-order at the same time\n",
    "        # It is reversed because the backtracking happens in reverse time order\n",
    "#         output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n",
    "        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n",
    "        #    --- fake output ---\n",
    "        output = None\n",
    "        #    --- fake ---\n",
    "        return output, h_t, h_n, s, l, p\n",
    "\n",
    "    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n",
    "            score[idx] = masking_score\n",
    "\n",
    "    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n",
    "        if len(idx.size()) > 0:\n",
    "            indices = idx[:, 0]\n",
    "            tensor.index_fill_(dim, indices, masking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, use_cuda, input_dim, hidden_dim, vocab, max_length = 25):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.enc = Encoder(use_cuda=use_cuda, hidden_dim=hidden_dim, input_dim=input_dim, vocab=vocab)\n",
    "        self.dec = Decoder(use_cuda=use_cuda, encoder=self.enc, hidden_dim=hidden_dim, max_length=max_length)\n",
    "        if use_cuda:\n",
    "            self.enc = self.enc.cuda()\n",
    "            self.dec = self.dec.cuda()\n",
    "    def forward(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        if is_train:\n",
    "            loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                    h0_and_c0=(enc_hn, enc_cn), \n",
    "                                    sent_lens=input_lens,\n",
    "                                    labels=torch.LongTensor(labels), \n",
    "                                    is_train=1, \n",
    "                                    teaching_rate = 1\n",
    "                                    )\n",
    "            return loss, predicts\n",
    "        else:\n",
    "            predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                sent_lens=input_lens,\n",
    "                                labels=torch.LongTensor(labels), \n",
    "                                is_train=0, \n",
    "                                teaching_rate = 1\n",
    "                                )\n",
    "            return predicts\n",
    "#     def train_using_rl(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "#         enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "#         loss, predicts, bleu_mean = self.dec.train_using_rl_2(enc_outputs = enc_outputs, \n",
    "#                                                 h0_and_c0=(enc_hn, enc_cn), \n",
    "#                                                 sent_lens=input_lens,\n",
    "#                                                 labels=labels,\n",
    "#                                                 is_train=1, \n",
    "#                                                 teaching_rate = 1\n",
    "#                                                 )\n",
    "#         return loss, predicts, bleu_mean\n",
    "    def train_using_rl(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        topk=10\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        loss, results, bleu_mean, bleu_max = self.dec.train_using_rl_3(enc_outputs=enc_outputs, h0_and_c0=(enc_hn, enc_cn), labels=labels, topk=topk)\n",
    "        return loss, results, bleu_mean, bleu_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use_cuda = 1\n",
    "# hidden_dim = 512\n",
    "# input_dim = 300\n",
    "\n",
    "# enc = Encoder(use_cuda=use_cuda, \n",
    "#             hidden_dim=hidden_dim, \n",
    "#             input_dim=input_dim, \n",
    "#             vocab=vocab\n",
    "#            )\n",
    "# if use_cuda:\n",
    "#     enc = enc.cuda()\n",
    "    \n",
    "# sample_num = 11\n",
    "# print('sentences length: ', train_set_input_lens[0:sample_num])\n",
    "\n",
    "# # enc_outputs, (enc_hn, enc_cn) = enc(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "# #                                     torch.LongTensor(train_set_input_lens[0:sample_num]))\n",
    "# # print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "# # dec = Decoder(use_cuda=use_cuda, encoder=enc, hidden_dim=hidden_dim, max_length=25)\n",
    "# # if use_cuda:\n",
    "# #     dec = dec.cuda()\n",
    "    \n",
    "# # loss, predicts = dec(enc_outputs = enc_outputs, \n",
    "# #                     h0_and_c0=(enc_hn, enc_cn), \n",
    "# #                     sent_lens=train_set_input_lens[0:sample_num], \n",
    "# #                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "# #                     is_train=1, teaching_rate = 1\n",
    "# #                     )\n",
    "# # print('loss is %4.7f'%loss.data[0])\n",
    "\n",
    "# # autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "# # a=time.time()\n",
    "# # loss, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "# #                                      torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "# #                                      labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "# #                                      is_train=1, teaching_rate=1)\n",
    "# # print('autocoder: loss is %4.7f'%loss.data[0])\n",
    "# # print(time.time()-a)\n",
    "\n",
    "# autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "# a=time.time()\n",
    "# loss, predicts, bleu_reward = autoencoder.train_using_rl(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "#                                      torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "#                                      labels=train_set_labels[0:sample_num], \n",
    "#                                      is_train=1, teaching_rate=1)\n",
    "# print('autocoder: loss is %4.7f'%loss.data[0], bleu_reward)\n",
    "# print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n",
      "三号球，角球。 ----<o_o>---- 三号球，角球。\n",
      "你叫你的妻儿走，这不是抛弃？ ----<o_o>---- 你叫你的妻儿走，这不是抛弃？\n",
      "我要你把赶来的特别行动小组调到大街大街上。 ----<o_o>---- 我要你把赶来的特别行动小组调到百老汇大街上。\n",
      "把车子修好就回赌城。 ----<o_o>---- 把车子修好就回赌城。\n",
      "从<low_freq>号出口、<low_freq>高速车道）沿着高速，或者在短时间号高速公路下高速。 ----<o_o>---- 从<low_freq>号出口（即大学车道）下高速，或者在380号高速公路下高速。\n",
      "昨晚你非要出去玩在医院里？即使不是你当班。 ----<o_o>---- 昨晚你故意呆在医院里？即使不是你当班。\n",
      "去吧，爬。走吧。 ----<o_o>---- 去吧，爬。走吧。\n",
      "我们能住在……地方公园里，依然任何地方。 ----<o_o>---- 我们能住在……国家公园里，山上任何地方。\n",
      "加油，丹尼！超呼！ ----<o_o>---- 加油，丹尼！喔呼！\n",
      "脾气和动其实就在<low_freq>。 ----<o_o>---- 直和弯其实就在<low_freq>。\n",
      "----no teaching forcing----\n",
      "loss=-0.251054168-train_bleu_mean=0.383645330-train_bleu_max=0.781558485-batch_size=20-epoch=0-batch_id=(1/500)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e5a3e8c914bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running time: %.2f mins'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e5a3e8c914bf>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(epoch, batch_size, train_set_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#         torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#retain_graph=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "lr=0.005\n",
    "batch_size=20\n",
    "train_set_size=int(len(train_set_inputs)/10)\n",
    "epochs=10000\n",
    "train_bleu = 0\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 25)\n",
    "#pre train para\n",
    "#pre_train = torch.load('./models_better/loss-2.099016905-bleu-0.4078-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[7001-[of]-21743]-lr-0.0050')\n",
    "pre_train = torch.load('./models_saved/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "\n",
    "# autoencoder.dec.init_nll_loss()\n",
    "\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, autoencoder.parameters()), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def model_train(epoch, batch_size, train_set_size):\n",
    "    batch_id = 0\n",
    "    valid_bleu = 0\n",
    "    train_reward = 0\n",
    "    for start_idx in range(0, train_set_size-batch_size+1, batch_size):\n",
    "#         print('batch id: ', batch_id)\n",
    "            \n",
    "        batch_id+=1\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        optimizer.zero_grad()#clear\n",
    "#         loss, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "#                                      torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "#                                      labels=torch.LongTensor(train_set_labels[start_idx:end_idx]), \n",
    "#                                      is_train=1, teaching_rate=1)\n",
    "        \n",
    "        loss, predicts, train_bleu_mean, train_bleu_max = autoencoder.train_using_rl(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "                                                                 torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "                                                                 labels=train_set_labels[start_idx:end_idx], \n",
    "                                                                 is_train=1, teaching_rate=1)\n",
    "#         torch.cuda.empty_cache()\n",
    "        #optimize\n",
    "        loss.backward()#retain_graph=True)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_id%20==1:\n",
    "            autoencoder.eval()\n",
    "            sample_num = 10\n",
    "            rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "            #teaching forcing\n",
    "            loss_, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            \n",
    "            predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "            labels = batch_tokens_remove_eos(train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "            predicts = batch_tokens2words(predicts, vocab)\n",
    "            labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "            predicts_sents = batch_words2sentence(predicts)\n",
    "            labels_sents = batch_words2sentence(labels)\n",
    "            \n",
    "            for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "                print(predict_sent, '----<o_o>----', label_sent)\n",
    "                \n",
    "            #no teaching forcing\n",
    "            print('----no teaching forcing----')\n",
    "#             predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "#                                              torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "#                                              labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "#                                              is_train=0, teaching_rate=1)\n",
    "#             tokenized_sents=predicts.tolist()\n",
    "#             real_sents=[]\n",
    "#             label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "#             label_real_sents=[]\n",
    "#             for idx, sent in enumerate(tokenized_sents):\n",
    "#                 real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "#             for sent in label_tokenized_sents:\n",
    "#                 label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "#             for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "#                 print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            info_stamp = 'loss={:2.9f}-train_bleu_mean={:2.9f}-train_bleu_max={:2.9f}-batch_size={:n}-epoch={:n}-batch_id=({:n}/{:n})'.format(\n",
    "                              loss.data[0], train_bleu_mean, train_bleu_max, batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "            print(info_stamp)\n",
    "#             #valid_set testing\n",
    "#             if batch_id%1000==1:\n",
    "#                 rand_idx=random.randint(0, len(valid_set_inputs)-batch_size-1-1)\n",
    "#                 predicts = autoencoder.forward(torch.LongTensor(valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "#                                                  torch.LongTensor(valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "#                                                  labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "#                                                  is_train=0, teaching_rate=1)\n",
    "#                 tokenized_sents=predicts.tolist()\n",
    "#                 real_sents=[]\n",
    "#                 label_tokenized_sents=valid_set_labels[rand_idx:rand_idx+batch_size]\n",
    "#                 label_real_sents=[]\n",
    "#                 for idx, sent in enumerate(tokenized_sents):\n",
    "#                     real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "#                 for sent in label_tokenized_sents:\n",
    "#                     label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "#                 bleu_score, valid_num = data_set_bleu(label_real_sents, real_sents)\n",
    "#                 if valid_num>10:\n",
    "#                     valid_bleu = bleu_score/valid_num\n",
    "                       \n",
    "#                 info_stamp = 'loss-{:2.9f}-bleu-{:1.4f}-hidden_dim-{:n}-input_dim-{:n}-epoch-{:n}-batch_size-{:n}-batch_id-[{:n}-[of]-{:n}]-lr-{:1.4f}'.format(\n",
    "#                               loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "#                 print(valid_num, info_stamp)\n",
    "#                 now = int(round(time.time()*1000))\n",
    "#                 time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "#                 torch.save(autoencoder.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "                \n",
    "            autoencoder.train()\n",
    "            \n",
    "for epoch in range(epochs):\n",
    "    model_train(epoch, batch_size, train_set_size)\n",
    "    \n",
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time: 1.48 mins\n"
     ]
    }
   ],
   "source": [
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
