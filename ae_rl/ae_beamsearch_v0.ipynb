{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [''.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "                #tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./small_data_set/train_set_inputs_10w.pk', 'rb') as f:\n",
    "    train_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/train_set_input_lens_10w.pk', 'rb') as f:\n",
    "    train_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/train_set_labels_10w.pk', 'rb') as f:\n",
    "    train_set_labels = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_inputs_10w.pk', 'rb') as f:\n",
    "    valid_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_input_lens_10w.pk', 'rb') as f:\n",
    "    valid_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_labels_10w.pk', 'rb') as f:\n",
    "    valid_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 100000 100000 100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_inputs), len(train_set_input_lens), len(train_set_labels), \n",
    "      len(valid_set_input_lens), len(valid_set_inputs), len(valid_set_labels))\n",
    "\n",
    "for sent_len in valid_set_input_lens:\n",
    "    if sent_len<=2:\n",
    "        print('why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('pre_train_word_embedding.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "        #self.embed.weight.requires_grad = False\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "        #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "            cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "            cn = cn.index_select(0, Variable(true_order_ids))\n",
    "            \n",
    "        return outputs, (hn,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, use_cuda, encoder, hidden_dim, max_length=25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.input_dim = encoder.input_dim\n",
    "        self.max_length = max_length\n",
    "        self.vocab = encoder.vocab\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        #self.weight[self.vocab.word2token['<eos>']]=1.01\n",
    "        #self.weight[self.vocab.word2token['<split>']]=1.01\n",
    "        \n",
    "        self.lstmcell = torch.nn.LSTMCell(input_size=self.input_dim, hidden_size=self.hidden_dim*2, bias=True)\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=encoder.embed# reference share\n",
    "        #fcnn: projection for crossentroy loss\n",
    "        self.fcnn = nn.Linear(in_features = self.hidden_dim*2, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.cost_func = nn.CrossEntropyLoss(torch.Tensor(self.weight))\n",
    "        \n",
    "        print('init lookup embedding matrix size: ', self.embed.weight.data.size())\n",
    "        \n",
    "    def forward(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = 0\n",
    "        predicts = []\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "\n",
    "        for ii in range(self.max_length+1):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,0])\n",
    "                    all_loss+=loss\n",
    "                \n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #print('max_idxs size: ',max_idxs.size(), max_idxs)\n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "                    all_loss+=loss\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                #print('max_idx size: ', max_idxs.size(), max_idxs)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "        \n",
    "        if is_train:  #training\n",
    "            if self.use_cuda:\n",
    "                return all_loss/(self.max_length+1), predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return all_loss/(self.max_length+1), predicts.data.numpy()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return predicts.data.numpy()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _inflate(tensor, times, dim):\n",
    "    \"\"\"\n",
    "    Examples::\n",
    "        >> a = torch.LongTensor([[1, 2], [3, 4]])\n",
    "        >> a\n",
    "        1   2\n",
    "        3   4\n",
    "        [torch.LongTensor of size 2x2]\n",
    "        >> b = ._inflate(a, 2, dim=1)\n",
    "        >> b\n",
    "        1   2   1   2\n",
    "        3   4   3   4\n",
    "        [torch.LongTensor of size 2x4]\n",
    "    \"\"\"\n",
    "    repeat_dims = [1] * tensor.dim()\n",
    "    repeat_dims[dim] = times\n",
    "    return tensor.repeat(*repeat_dims)\n",
    "\n",
    "class TopKDecoder(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Top-K decoding with beam search.\n",
    "\n",
    "    Args:\n",
    "        decoder_rnn (DecoderRNN): An object of DecoderRNN used for decoding.\n",
    "        k (int): Size of the beam.\n",
    "\n",
    "    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n",
    "        - **inputs** (seq_len, batch, input_size): list of sequences, whose length is the batch size and within which\n",
    "          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default is `None`)\n",
    "        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features\n",
    "          in the hidden state `h` of encoder. Used as the initial hidden state of the decoder.\n",
    "        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n",
    "          Used for attention mechanism (default is `None`).\n",
    "        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n",
    "          (default is `torch.nn.functional.log_softmax`).\n",
    "        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n",
    "          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n",
    "          teacher forcing would be used (default is 0).\n",
    "\n",
    "    Outputs: decoder_outputs, decoder_hidden, ret_dict\n",
    "        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n",
    "          outputs of the decoder.\n",
    "        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n",
    "          state of the decoder.\n",
    "        - **ret_dict**: dictionary containing additional information as follows {*length* : list of integers\n",
    "          representing lengths of output sequences, *topk_length*: list of integers representing lengths of beam search\n",
    "          sequences, *sequence* : list of sequences, where each sequence is a list of predicted token IDs,\n",
    "          *topk_sequence* : list of beam search sequences, each beam is a list of token IDs, *inputs* : target\n",
    "          outputs if provided for decoding}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder_rnn, k):\n",
    "        super(TopKDecoder, self).__init__()\n",
    "        self.rnn = decoder_rnn\n",
    "        self.k = k\n",
    "        self.hidden_size = self.rnn.hidden_dim\n",
    "        self.V = len(self.rnn.vocab.word2token)\n",
    "        self.SOS = self.rnn.vocab.word2token['<sos>']\n",
    "        self.EOS = self.rnn.vocab.word2token['<eos>']\n",
    "        self.use_cuda = self.rnn.use_cuda\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var\n",
    "\n",
    "    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None, function=F.log_softmax,\n",
    "                    teacher_forcing_ratio=0, retain_output_probs=True):\n",
    "        \"\"\"\n",
    "        Forward rnn for MAX_LENGTH steps.  Look at :func:`seq2seq.models.DecoderRNN.DecoderRNN.forward_rnn` for details.\n",
    "        \"\"\"\n",
    "\n",
    "#         inputs, batch_size, max_length = self.rnn._validate_args(inputs, encoder_hidden, encoder_outputs,\n",
    "#                                                                  function, teacher_forcing_ratio)\n",
    "\n",
    "        batch_size = encoder_outputs.size(dim=0)\n",
    "        max_length = self.rnn.max_length+1\n",
    "        \n",
    "        self.pos_index = self._tocuda(Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1))\n",
    "\n",
    "        # Inflate the initial hidden states to be of size: b*k x h\n",
    "#         encoder_hidden = self.rnn._init_state(encoder_hidden)\n",
    "#         if encoder_hidden is None:\n",
    "#             hidden = None\n",
    "#         else:\n",
    "#             if isinstance(encoder_hidden, tuple):\n",
    "#                 hidden = tuple([_inflate(h, self.k, 1) for h in encoder_hidden])\n",
    "#             else:\n",
    "#                 hidden = _inflate(encoder_hidden, self.k, 1)\n",
    "\n",
    "        hidden = tuple([_inflate(h, self.k, 1).view(batch_size*self.k, -1) for h in encoder_hidden])\n",
    "        #print('hidden0 size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "        # ... same idea for encoder_outputs and decoder_outputs\n",
    "#         if self.rnn.use_attention:\n",
    "#             inflated_encoder_outputs = _inflate(encoder_outputs, self.k, 0)\n",
    "#         else:\n",
    "#             inflated_encoder_outputs = None\n",
    "\n",
    "        # Initialize the scores; for the first step,\n",
    "        # ignore the inflated copies to avoid duplicate entries in the top k\n",
    "        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n",
    "        sequence_scores.fill_(-float('Inf'))\n",
    "        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n",
    "        sequence_scores = self._tocuda(Variable(sequence_scores))\n",
    "\n",
    "        # Initialize the input vector\n",
    "        input_var = self._tocuda(Variable(torch.LongTensor([self.SOS] * batch_size * self.k)))\n",
    "\n",
    "        # Store decisions for backtracking\n",
    "        stored_outputs = list()\n",
    "        stored_scores = list()\n",
    "        stored_predecessors = list()\n",
    "        stored_emitted_symbols = list()\n",
    "        stored_hidden = list()\n",
    "\n",
    "        for ii in range(0, max_length):\n",
    "            # Run the RNN one step forward\n",
    "            #print('setp: %s'%ii)\n",
    "            input_vec = self.rnn.embed(input_var)\n",
    "            #print('input_var and input_vec size: ', input_var.size(), input_vec.size())\n",
    "            hidden = self.rnn.lstmcell(input_vec, hidden)\n",
    "            #print('hidden size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "            \n",
    "            log_softmax_output = self.log_softmax(self.rnn.fcnn(hidden[0]))\n",
    "            # If doing local backprop (e.g. supervised training), retain the output layer\n",
    "#             if retain_output_probs:\n",
    "#                 stored_outputs.append(log_softmax_output)\n",
    "\n",
    "            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n",
    "            sequence_scores = _inflate(sequence_scores, self.V, 1)\n",
    "            sequence_scores += log_softmax_output.squeeze(1)\n",
    "            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n",
    "\n",
    "            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n",
    "            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n",
    "            sequence_scores = scores.view(batch_size * self.k, 1)\n",
    "\n",
    "#             if ii ==0:\n",
    "#                 result0 = input_var.data.cpu().tolist()\n",
    "#                 print(result0)\n",
    "#                 result0=batch_tokens2words(result0, vocab)\n",
    "#                 print(result0)\n",
    "            # Update fields for next timestep\n",
    "            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple([h.index_select(0, predecessors.squeeze()) for h in hidden])\n",
    "            else:\n",
    "                hidden = hidden.index_select(0, predecessors.squeeze())\n",
    "\n",
    "            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n",
    "            stored_scores.append(sequence_scores.clone())\n",
    "            eos_indices = input_var.data.eq(self.EOS)\n",
    "            if eos_indices.nonzero().dim() > 0:\n",
    "                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n",
    "\n",
    "            # Cache results for backtracking\n",
    "            stored_predecessors.append(predecessors)\n",
    "            stored_emitted_symbols.append(input_var)\n",
    "            stored_hidden.append(hidden)\n",
    "\n",
    "        # Do backtracking to return the optimal values\n",
    "        output, h_t, h_n, s, l, p = self._backtrack(stored_hidden,\n",
    "                                                    stored_predecessors, stored_emitted_symbols,\n",
    "                                                    stored_scores, batch_size, self.hidden_size)\n",
    "\n",
    "        # Build return objects\n",
    "#         decoder_outputs = [step[:, 0, :] for step in output]\n",
    "        if isinstance(h_n, tuple):\n",
    "            decoder_hidden = tuple([h[:, :, 0, :] for h in h_n])\n",
    "        else:\n",
    "            decoder_hidden = h_n[:, :, 0, :]\n",
    "        metadata = {}\n",
    "        metadata['inputs'] = inputs\n",
    "        metadata['output'] = output\n",
    "        metadata['h_t'] = h_t\n",
    "        metadata['score'] = s\n",
    "        metadata['topk_length'] = l\n",
    "        metadata['topk_sequence'] = p\n",
    "        metadata['length'] = [seq_len[0] for seq_len in l]\n",
    "        metadata['sequence'] = [seq[0] for seq in p]\n",
    "        return decoder_hidden, metadata\n",
    "\n",
    "    def _backtrack(self, nw_hidden, predecessors, symbols, scores, b, hidden_size):\n",
    "        \"\"\"Backtracks over batch to generate optimal k-sequences.\n",
    "\n",
    "        Args:\n",
    "            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n",
    "            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n",
    "            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n",
    "            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n",
    "            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n",
    "            b: Size of the batch\n",
    "            hidden_size: Size of the hidden state\n",
    "\n",
    "        Returns:\n",
    "            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n",
    "\n",
    "            score [batch, k]: A list containing the final scores for all top-k sequences\n",
    "\n",
    "            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n",
    "\n",
    "            p (batch, k, sequence_len): A Tensor containing predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        lstm = isinstance(nw_hidden[0], tuple)\n",
    "\n",
    "        # initialize return variables given different types\n",
    "        output = list()\n",
    "        h_t = list()\n",
    "        p = list()\n",
    "        # Placeholder for last hidden state of top-k sequences.\n",
    "        # If a (top-k) sequence ends early in decoding, `h_n` contains\n",
    "        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n",
    "        # the last hidden state of decoding.\n",
    "        if lstm:\n",
    "            state_size = nw_hidden[0][0].size()\n",
    "            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n",
    "        else:\n",
    "            h_n = torch.zeros(nw_hidden[0].size())\n",
    "        l = [[self.rnn.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n",
    "                                                                # Similar to `h_n`\n",
    "\n",
    "        # the last step output of the beams are not sorted\n",
    "        # thus they are sorted here\n",
    "        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n",
    "        # initialize the sequence scores with the sorted last step beam scores\n",
    "        s = sorted_score.clone()\n",
    "\n",
    "        batch_eos_found = [0] * b   # the number of EOS found\n",
    "                                    # in the backward loop below for each batch\n",
    "\n",
    "        t = self.rnn.max_length - 1\n",
    "        # initialize the back pointer with the sorted order of the last step beams.\n",
    "        # add self.pos_index for indexing variable with b*k as the first dimension.\n",
    "        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n",
    "        while t >= 0:\n",
    "            # Re-order the variables with the back pointer\n",
    "#             current_output = nw_output[t].index_select(0, t_predecessors)\n",
    "            if lstm:\n",
    "                current_hidden = tuple([h.index_select(0, t_predecessors) for h in nw_hidden[t]])\n",
    "            else:\n",
    "                current_hidden = nw_hidden[t].index_select(0, t_predecessors)\n",
    "            current_symbol = symbols[t].index_select(0, t_predecessors)\n",
    "            # Re-order the back pointer of the previous step with the back pointer of\n",
    "            # the current step\n",
    "            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n",
    "\n",
    "            # This tricky block handles dropped sequences that see EOS earlier.\n",
    "            # The basic idea is summarized below:\n",
    "            #\n",
    "            #   Terms:\n",
    "            #       Ended sequences = sequences that see EOS early and dropped\n",
    "            #       Survived sequences = sequences in the last step of the beams\n",
    "            #\n",
    "            #       Although the ended sequences are dropped during decoding,\n",
    "            #   their generated symbols and complete backtracking information are still\n",
    "            #   in the backtracking variables.\n",
    "            #   For each batch, everytime we see an EOS in the backtracking process,\n",
    "            #       1. If there is survived sequences in the return variables, replace\n",
    "            #       the one with the lowest survived sequence score with the new ended\n",
    "            #       sequences\n",
    "            #       2. Otherwise, replace the ended sequence with the lowest sequence\n",
    "            #       score with the new ended sequence\n",
    "            #\n",
    "            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n",
    "            if eos_indices.dim() > 0:\n",
    "                for i in range(eos_indices.size(0)-1, -1, -1):\n",
    "                    # Indices of the EOS symbol for both variables\n",
    "                    # with b*k as the first dimension, and b, k for\n",
    "                    # the first two dimensions\n",
    "                    idx = eos_indices[i]\n",
    "                    b_idx = int(idx[0] / self.k)\n",
    "                    # The indices of the replacing position\n",
    "                    # according to the replacement strategy noted above\n",
    "                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n",
    "                    batch_eos_found[b_idx] += 1\n",
    "                    res_idx = b_idx * self.k + res_k_idx\n",
    "\n",
    "                    # Replace the old information in return variables\n",
    "                    # with the new ended sequence information\n",
    "                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n",
    "#                     current_output[res_idx, :] = nw_output[t][idx[0], :]\n",
    "                    if lstm:\n",
    "                        current_hidden[0][res_idx, :] = nw_hidden[t][0][idx[0], :]\n",
    "                        current_hidden[1][res_idx, :] = nw_hidden[t][1][idx[0], :]\n",
    "                        h_n[0][res_idx, :] = nw_hidden[t][0][idx[0], :].data\n",
    "                        h_n[1][res_idx, :] = nw_hidden[t][1][idx[0], :].data\n",
    "                    else:\n",
    "                        current_hidden[res_idx, :] = nw_hidden[t][idx[0], :]\n",
    "                        h_n[res_idx, :] = nw_hidden[t][idx[0], :].data\n",
    "                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n",
    "                    s[b_idx, res_k_idx] = scores[t][idx[0]].data[0]\n",
    "                    l[b_idx][res_k_idx] = t + 1\n",
    "\n",
    "            # record the back tracked results\n",
    "#             output.append(current_output)\n",
    "            h_t.append(current_hidden)\n",
    "            p.append(current_symbol)\n",
    "\n",
    "            t -= 1\n",
    "\n",
    "        # Sort and re-order again as the added ended sequences may change\n",
    "        # the order (very unlikely)\n",
    "        s, re_sorted_idx = s.topk(self.k)\n",
    "        for b_idx in range(b):\n",
    "            l[b_idx] = [l[b_idx][k_idx.data[0]] for k_idx in re_sorted_idx[b_idx,:]]\n",
    "\n",
    "        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n",
    "\n",
    "        # Reverse the sequences and re-order at the same time\n",
    "        # It is reversed because the backtracking happens in reverse time order\n",
    "#         output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n",
    "        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n",
    "        if lstm:\n",
    "            h_t = [tuple([h.index_select(0, re_sorted_idx).view(-1, b, self.k, hidden_size) for h in step]) for step in reversed(h_t)]\n",
    "            h_n = tuple([self._tocuda(x) for x in h_n])\n",
    "            h_n = tuple([h.index_select(0, re_sorted_idx.data).view(-1, b, self.k, hidden_size) for h in h_n])\n",
    "        else:\n",
    "            h_t = [step.index_select(0, re_sorted_idx).view(-1, b, self.k, hidden_size) for step in reversed(h_t)]\n",
    "            h_n = h_n.index_select(0, re_sorted_idx.data).view(-1, b, self.k, hidden_size)\n",
    "        s = s.data\n",
    "\n",
    "        #    --- fake output ---\n",
    "        output = None\n",
    "        #    --- fake ---\n",
    "        return output, h_t, h_n, s, l, p\n",
    "\n",
    "    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n",
    "            score[idx] = masking_score\n",
    "\n",
    "    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n",
    "        if len(idx.size()) > 0:\n",
    "            indices = idx[:, 0]\n",
    "            tensor.index_fill_(dim, indices, masking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, use_cuda, input_dim, hidden_dim, vocab, max_length = 25):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.enc = Encoder(use_cuda=use_cuda, hidden_dim=hidden_dim, input_dim=input_dim, vocab=vocab)\n",
    "        self.dec = Decoder(use_cuda=use_cuda, encoder=self.enc, hidden_dim=hidden_dim, max_length=max_length)\n",
    "        if use_cuda:\n",
    "            self.enc = self.enc.cuda()\n",
    "            self.dec = self.dec.cuda()\n",
    "    def forward(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        if is_train:\n",
    "            loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                    h0_and_c0=(enc_hn, enc_cn), \n",
    "                                    sent_lens=input_lens,\n",
    "                                    labels=torch.LongTensor(labels), \n",
    "                                    is_train=1, \n",
    "                                    teaching_rate = 1\n",
    "                                    )\n",
    "            return loss, predicts\n",
    "        else:\n",
    "            predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                sent_lens=input_lens,\n",
    "                                labels=torch.LongTensor(labels), \n",
    "                                is_train=0, \n",
    "                                teaching_rate = 1\n",
    "                                )\n",
    "            return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n",
      "1 5000 time: 0.00 mins bleu: 0.8149\n",
      "21 5000 time: 0.00 mins bleu: 0.7195\n",
      "41 5000 time: 0.00 mins bleu: 0.7145\n",
      "61 5000 time: 1.00 mins bleu: 0.7142\n",
      "81 5000 time: 1.00 mins bleu: 0.7036\n",
      "101 5000 time: 2.00 mins bleu: 0.7044\n",
      "121 5000 time: 2.00 mins bleu: 0.7065\n",
      "141 5000 time: 3.00 mins bleu: 0.7083\n",
      "161 5000 time: 3.00 mins bleu: 0.7039\n",
      "181 5000 time: 3.00 mins bleu: 0.7003\n",
      "201 5000 time: 4.00 mins bleu: 0.6971\n",
      "221 5000 time: 4.00 mins bleu: 0.6943\n",
      "241 5000 time: 5.00 mins bleu: 0.6933\n",
      "261 5000 time: 5.00 mins bleu: 0.6945\n",
      "281 5000 time: 6.00 mins bleu: 0.6948\n",
      "301 5000 time: 6.00 mins bleu: 0.6943\n",
      "321 5000 time: 6.00 mins bleu: 0.6949\n",
      "341 5000 time: 7.00 mins bleu: 0.6938\n",
      "361 5000 time: 7.00 mins bleu: 0.6953\n",
      "381 5000 time: 8.00 mins bleu: 0.6976\n",
      "401 5000 time: 8.00 mins bleu: 0.6975\n",
      "421 5000 time: 9.00 mins bleu: 0.6985\n",
      "441 5000 time: 9.00 mins bleu: 0.6980\n",
      "461 5000 time: 9.00 mins bleu: 0.6962\n",
      "481 5000 time: 10.00 mins bleu: 0.6962\n",
      "501 5000 time: 10.00 mins bleu: 0.6978\n",
      "521 5000 time: 11.00 mins bleu: 0.6969\n",
      "541 5000 time: 11.00 mins bleu: 0.6971\n",
      "561 5000 time: 11.00 mins bleu: 0.6980\n",
      "581 5000 time: 12.00 mins bleu: 0.6985\n",
      "601 5000 time: 12.00 mins bleu: 0.6989\n",
      "621 5000 time: 13.00 mins bleu: 0.6979\n",
      "641 5000 time: 13.00 mins bleu: 0.6974\n",
      "661 5000 time: 14.00 mins bleu: 0.6978\n",
      "681 5000 time: 14.00 mins bleu: 0.6973\n",
      "701 5000 time: 14.00 mins bleu: 0.6970\n",
      "721 5000 time: 15.00 mins bleu: 0.6969\n",
      "741 5000 time: 15.00 mins bleu: 0.6973\n",
      "761 5000 time: 16.00 mins bleu: 0.6977\n",
      "781 5000 time: 16.00 mins bleu: 0.6984\n",
      "801 5000 time: 17.00 mins bleu: 0.6979\n",
      "821 5000 time: 17.00 mins bleu: 0.6984\n",
      "841 5000 time: 17.00 mins bleu: 0.6987\n",
      "861 5000 time: 18.00 mins bleu: 0.6985\n",
      "881 5000 time: 18.00 mins bleu: 0.6987\n",
      "901 5000 time: 19.00 mins bleu: 0.6987\n",
      "921 5000 time: 19.00 mins bleu: 0.6990\n",
      "941 5000 time: 20.00 mins bleu: 0.6993\n",
      "961 5000 time: 20.00 mins bleu: 0.6985\n",
      "981 5000 time: 20.00 mins bleu: 0.6977\n",
      "1001 5000 time: 21.00 mins bleu: 0.6990\n",
      "1021 5000 time: 21.00 mins bleu: 0.6994\n",
      "1041 5000 time: 22.00 mins bleu: 0.6992\n",
      "1061 5000 time: 22.00 mins bleu: 0.6986\n",
      "1081 5000 time: 23.00 mins bleu: 0.6988\n",
      "1101 5000 time: 23.00 mins bleu: 0.6986\n",
      "1121 5000 time: 24.00 mins bleu: 0.6981\n",
      "1141 5000 time: 24.00 mins bleu: 0.6982\n",
      "1161 5000 time: 24.00 mins bleu: 0.6976\n",
      "1181 5000 time: 25.00 mins bleu: 0.6977\n",
      "1201 5000 time: 25.00 mins bleu: 0.6973\n",
      "1221 5000 time: 26.00 mins bleu: 0.6978\n",
      "1241 5000 time: 26.00 mins bleu: 0.6977\n",
      "1261 5000 time: 26.00 mins bleu: 0.6971\n",
      "1281 5000 time: 27.00 mins bleu: 0.6972\n",
      "1301 5000 time: 27.00 mins bleu: 0.6975\n",
      "1321 5000 time: 28.00 mins bleu: 0.6975\n",
      "1341 5000 time: 28.00 mins bleu: 0.6977\n",
      "1361 5000 time: 29.00 mins bleu: 0.6976\n",
      "1381 5000 time: 29.00 mins bleu: 0.6976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8bc9b4b72620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_beamsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menc_hn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topk_sequence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d1ddc3655ed1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio, retain_output_probs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output, h_t, h_n, s, l, p = self._backtrack(stored_hidden,\n\u001b[1;32m    165\u001b[0m                                                     \u001b[0mstored_predecessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstored_emitted_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                                     stored_scores, batch_size, self.hidden_size)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Build return objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d1ddc3655ed1>\u001b[0m in \u001b[0;36m_backtrack\u001b[0;34m(self, nw_hidden, predecessors, symbols, scores, b, hidden_size)\u001b[0m\n\u001b[1;32m    292\u001b[0m                         \u001b[0mcurrent_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                         \u001b[0mcurrent_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                         \u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                         \u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "pre_train = torch.load('./models_better/time-[2019-01-08-06-27-50]-loss-0.925657809-bleu-0.6640-hidden_dim-256-input_dim-300-epoch-2-batch_size-200-batch_id-[17001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "autoencoder.eval()\n",
    "\n",
    "train_set_size = len(train_set_inputs)\n",
    "sample_num=20\n",
    "topk=10\n",
    "batch_id=0\n",
    "bleu_sum=0\n",
    "\n",
    "dec_beamsearch = TopKDecoder(decoder_rnn=autoencoder.dec, k=topk)\n",
    "\n",
    "start_time=time.time()\n",
    "for start_idx in range(0, train_set_size-sample_num, sample_num):\n",
    "    batch_id+=1\n",
    "    \n",
    "    enc_outputs, (enc_hn, enc_cn) = autoencoder.enc(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                        torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]))\n",
    "    #print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "    _, metadata = dec_beamsearch(encoder_hidden = (enc_hn, enc_cn), encoder_outputs = enc_outputs)\n",
    "\n",
    "    results = metadata['topk_sequence']\n",
    "    results =torch.cat(results, dim = 2)\n",
    "    results=results.view(sample_num*topk, -1)\n",
    "    indices = torch.LongTensor([x*topk for x in range(sample_num)]).cuda()\n",
    "    results = results.data.index_select(0, indices)\n",
    "    results=results.cpu().tolist()\n",
    "    results=batch_tokens_remove_eos(results, vocab)\n",
    "#     results=batch_tokens2words(results, vocab)\n",
    "#     results=batch_words2sentence(results)\n",
    "#     print(results)\n",
    "    \n",
    "    inputs = train_set_inputs[start_idx:start_idx+sample_num]\n",
    "#     inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "#     inputs = batch_tokens2words(inputs, vocab)\n",
    "    inputs_=[]\n",
    "    for tokens in inputs:\n",
    "        x=[]\n",
    "        for token in tokens:\n",
    "            if token!=vocab.word2token['<padding>']:\n",
    "                x.append(token)\n",
    "            else:\n",
    "                break\n",
    "        inputs_.append(x)\n",
    "#     inputs = batch_words2sentence(inputs_)\n",
    "    bleu_scores = batch_tokens_bleu(references=inputs_, candidates=results)\n",
    "    for score in bleu_scores:\n",
    "        bleu_sum+=score\n",
    "    \n",
    "    if batch_id%20==1:\n",
    "        print(batch_id, int(train_set_size/sample_num), 'time: %4.2f mins'%((time.time()-start_time)/60), 'bleu: %2.4f'%(bleu_sum/batch_id/sample_num))\n",
    "    \n",
    "    \n",
    "\n",
    "#    print inputs\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "# print(inputs)\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "for sent in inputs:\n",
    "    print(sent)\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print('results[0] size: ', results[0].size())\n",
    "a=torch.cat(results, dim = 2)\n",
    "b=a.view(sample_num*topk, -1)\n",
    "print(a.size(), b.size())\n",
    "c=b.data.cpu().tolist()\n",
    "d=batch_tokens_remove_eos(c, vocab)\n",
    "e=batch_tokens2words(d, vocab)\n",
    "f=batch_words2sentence(e)\n",
    "for sent in f:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n",
      "1 500 time: 0.01 mins bleu: 0.6400\n",
      "21 500 time: 0.12 mins bleu: 0.6536\n",
      "41 500 time: 0.26 mins bleu: 0.6533\n",
      "61 500 time: 0.39 mins bleu: 0.6536\n",
      "81 500 time: 0.52 mins bleu: 0.6529\n",
      "101 500 time: 0.66 mins bleu: 0.6548\n",
      "121 500 time: 0.78 mins bleu: 0.6526\n",
      "141 500 time: 0.92 mins bleu: 0.6528\n",
      "161 500 time: 1.05 mins bleu: 0.6513\n",
      "181 500 time: 1.18 mins bleu: 0.6506\n",
      "201 500 time: 1.31 mins bleu: 0.6504\n",
      "221 500 time: 1.45 mins bleu: 0.6505\n",
      "241 500 time: 1.58 mins bleu: 0.6509\n",
      "261 500 time: 1.72 mins bleu: 0.6523\n",
      "281 500 time: 1.86 mins bleu: 0.6516\n",
      "301 500 time: 1.98 mins bleu: 0.6515\n",
      "321 500 time: 2.11 mins bleu: 0.6513\n",
      "341 500 time: 2.24 mins bleu: 0.6514\n",
      "361 500 time: 2.37 mins bleu: 0.6513\n",
      "381 500 time: 2.50 mins bleu: 0.6518\n",
      "401 500 time: 2.62 mins bleu: 0.6520\n",
      "421 500 time: 2.77 mins bleu: 0.6524\n",
      "441 500 time: 2.89 mins bleu: 0.6525\n",
      "461 500 time: 3.03 mins bleu: 0.6526\n",
      "481 500 time: 3.16 mins bleu: 0.6522\n",
      "\n",
      "<low_freq>\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "<low_freq><low_freq>\n",
      "\n",
      "\n",
      "<low_freq>15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1930\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "<low_freq>\n",
      "<low_freq><low_freq>\n",
      "45\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "<low_freq>\n",
      "20089\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq><low_freq><low_freq><low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq><low_freq>\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "<low_freq><low_freq>\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "<low_freq><low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<low_freq>\n",
      "81<low_freq>\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a99b8688d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results[0] size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "pre_train = torch.load('./models_better/time-[2019-01-08-06-27-50]-loss-0.925657809-bleu-0.6640-hidden_dim-256-input_dim-300-epoch-2-batch_size-200-batch_id-[17001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "autoencoder.eval()\n",
    "\n",
    "train_set_size = len(train_set_inputs)\n",
    "sample_num=200\n",
    "batch_id=0\n",
    "bleu_sum=0\n",
    "\n",
    "start_time=time.time()\n",
    "for start_idx in range(0, train_set_size-sample_num, sample_num):\n",
    "    batch_id+=1\n",
    "    \n",
    "    enc_outputs, (enc_hn, enc_cn) = autoencoder.enc(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                        torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]))\n",
    "    predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]), \n",
    "                                             labels=999, \n",
    "                                             is_train=0, teaching_rate=1)\n",
    "\n",
    "    results=batch_tokens_remove_eos(predicts, vocab)\n",
    "#     results=batch_tokens2words(results, vocab)\n",
    "#     results=batch_words2sentence(results)\n",
    "#     print(results)\n",
    "    \n",
    "    inputs = train_set_inputs[start_idx:start_idx+sample_num]\n",
    "#     inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "#     inputs = batch_tokens2words(inputs, vocab)\n",
    "    inputs_=[]\n",
    "    for tokens in inputs:\n",
    "        x=[]\n",
    "        for token in tokens:\n",
    "            if token!=vocab.word2token['<padding>']:\n",
    "                x.append(token)\n",
    "            else:\n",
    "                break\n",
    "        inputs_.append(x)\n",
    "#     inputs = batch_words2sentence(inputs_)\n",
    "    bleu_scores = batch_tokens_bleu(references=inputs_, candidates=results)\n",
    "    for score in bleu_scores:\n",
    "        bleu_sum+=score\n",
    "    \n",
    "    if batch_id%20==1:\n",
    "        print(batch_id, int(train_set_size/sample_num), 'time: %4.2f mins'%((time.time()-start_time)/60), 'bleu: %2.4f'%(bleu_sum/batch_id/sample_num))\n",
    "    \n",
    "    \n",
    "\n",
    "#    print inputs\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "# print(inputs)\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "for sent in inputs:\n",
    "    print(sent)\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print('results[0] size: ', results[0].size())\n",
    "a=torch.cat(results, dim = 2)\n",
    "b=a.view(sample_num*topk, -1)\n",
    "print(a.size(), b.size())\n",
    "c=b.data.cpu().tolist()\n",
    "d=batch_tokens_remove_eos(c, vocab)\n",
    "e=batch_tokens2words(d, vocab)\n",
    "f=batch_words2sentence(e)\n",
    "for sent in f:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1f410946e68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m            )\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bf82403aa28d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, use_cuda, hidden_dim, input_dim, vocab)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#loading pre trained word embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pre_train_word_embedding.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "\n",
    "enc = Encoder(use_cuda=use_cuda, \n",
    "            hidden_dim=hidden_dim, \n",
    "            input_dim=input_dim, \n",
    "            vocab=vocab\n",
    "           )\n",
    "if use_cuda:\n",
    "    enc = enc.cuda()\n",
    "    \n",
    "sample_num = 11\n",
    "print('sentences length: ', train_set_input_lens[0:sample_num])\n",
    "\n",
    "enc_outputs, (enc_hn, enc_cn) = enc(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "                                    torch.LongTensor(train_set_input_lens[0:sample_num]))\n",
    "print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "dec = Decoder(use_cuda=use_cuda, encoder=enc, hidden_dim=hidden_dim, max_length=25)\n",
    "if use_cuda:\n",
    "    dec = dec.cuda()\n",
    "    \n",
    "# loss, predicts = dec(enc_outputs = enc_outputs, \n",
    "#                     h0_and_c0=(enc_hn, enc_cn), \n",
    "#                     sent_lens=train_set_input_lens[0:sample_num], \n",
    "#                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "#                     is_train=1, teaching_rate = 1\n",
    "#                     )\n",
    "# print('loss is %4.7f'%loss.data[0])\n",
    "\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "\n",
    "pre_train = torch.load('./models_better/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "\n",
    "autoencoder.eval()\n",
    "predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "                                     torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "                                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "                                     is_train=0, teaching_rate=1)\n",
    "\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "results = batch_tokens_remove_eos(predicts, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "results = batch_tokens2words(results, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "results = batch_words2sentence(results)\n",
    "for inp, res in zip(inputs,results):\n",
    "    print(inp)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_sent2real_sent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a46203c33948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running time: %.2f mins'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a46203c33948>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(epoch, batch_size, train_set_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mlabel_real_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mreal_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sent2real_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_tokenized_sents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mlabel_real_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sent2real_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_sent2real_sent' is not defined"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "lr=0.005\n",
    "batch_size=200\n",
    "train_set_size=int(len(train_set_inputs)/2)\n",
    "epochs=10\n",
    "train_bleu = 0\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 25)\n",
    "#pre train para\n",
    "#pre_train = torch.load('./models_better/loss-2.099016905-bleu-0.4078-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[7001-[of]-21743]-lr-0.0050')\n",
    "#pre_train = torch.load('./models_better/time-[2019-01-07-16-38-14]-loss-1.881381631-bleu-0.5340-hidden_dim-256-input_dim-300-epoch-0-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "pre_train = torch.load('./models_better/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, autoencoder.parameters()), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def model_train(epoch, batch_size, train_set_size):\n",
    "    batch_id = 0\n",
    "    valid_bleu = 0\n",
    "    for start_idx in range(0, train_set_size-batch_size, batch_size):\n",
    "        batch_id+=1\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        optimizer.zero_grad()#clear\n",
    "        loss, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "                                             torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[start_idx:end_idx]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "        #optimize\n",
    "        loss.backward()#retain_graph=True)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_id%50==1:\n",
    "            autoencoder.eval()\n",
    "            sample_num = 10\n",
    "            rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "            #teaching forcing\n",
    "            loss_, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            tokenized_sents=predicts.tolist()\n",
    "            real_sents=[]\n",
    "            label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "            label_real_sents=[]\n",
    "            for idx, sent in enumerate(tokenized_sents):\n",
    "                real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "            for sent in label_tokenized_sents:\n",
    "                label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "            print('train_set sample: ', rand_idx)\n",
    "            for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "                print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            #no teaching forcing\n",
    "            print('----no teaching forcing----')\n",
    "            predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=0, teaching_rate=1)\n",
    "            tokenized_sents=predicts.tolist()\n",
    "            real_sents=[]\n",
    "            label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "            label_real_sents=[]\n",
    "            for idx, sent in enumerate(tokenized_sents):\n",
    "                real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "            for sent in label_tokenized_sents:\n",
    "                label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "            for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "                print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            info_stamp = 'loss-{:2.9f}-batch_size-{:n}-epoch-{:n}-batch_id-({:n}/{:n})'.format(\n",
    "                              loss.data[0], batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "            print(info_stamp)\n",
    "            #valid_set testing\n",
    "            if batch_id%1000==1:\n",
    "                rand_idx=random.randint(0, len(valid_set_inputs)-batch_size-1-1)\n",
    "                predicts = autoencoder.forward(torch.LongTensor(valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "                                                 torch.LongTensor(valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "                                                 labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "                                                 is_train=0, teaching_rate=1)\n",
    "                tokenized_sents=predicts.tolist()\n",
    "                real_sents=[]\n",
    "                label_tokenized_sents=valid_set_labels[rand_idx:rand_idx+batch_size]\n",
    "                label_real_sents=[]\n",
    "                for idx, sent in enumerate(tokenized_sents):\n",
    "                    real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "                for sent in label_tokenized_sents:\n",
    "                    label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "                bleu_score, valid_num = data_set_bleu(label_real_sents, real_sents)\n",
    "                if valid_num>10:\n",
    "                    valid_bleu = bleu_score/valid_num\n",
    "                       \n",
    "                info_stamp = 'loss-{:2.9f}-bleu-{:1.4f}-hidden_dim-{:n}-input_dim-{:n}-epoch-{:n}-batch_size-{:n}-batch_id-[{:n}-[of]-{:n}]-lr-{:1.4f}'.format(\n",
    "                              loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "                print(valid_num, info_stamp)\n",
    "                now = int(round(time.time()*1000))\n",
    "                time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "                torch.save(autoencoder.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "                \n",
    "            autoencoder.train()\n",
    "            \n",
    "for epoch in range(epochs):\n",
    "    model_train(epoch, batch_size, train_set_size)\n",
    "    \n",
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "pre_train = torch.load('./models_better/loss-3.966628313-bleu-0.3201-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[1001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "print('a')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
