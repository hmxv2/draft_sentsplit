{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [''.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "                #tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./small_data_set/train_set_inputs_10w.pk', 'rb') as f:\n",
    "    train_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/train_set_input_lens_10w.pk', 'rb') as f:\n",
    "    train_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/train_set_labels_10w.pk', 'rb') as f:\n",
    "    train_set_labels = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_inputs_10w.pk', 'rb') as f:\n",
    "    valid_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_input_lens_10w.pk', 'rb') as f:\n",
    "    valid_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_labels_10w.pk', 'rb') as f:\n",
    "    valid_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 100000 100000 100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_inputs), len(train_set_input_lens), len(train_set_labels), \n",
    "      len(valid_set_input_lens), len(valid_set_inputs), len(valid_set_labels))\n",
    "\n",
    "for sent_len in valid_set_input_lens:\n",
    "    if sent_len<=2:\n",
    "        print('why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('pre_train_word_embedding.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "        #self.embed.weight.requires_grad = False\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "        #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "            cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "            cn = cn.index_select(0, Variable(true_order_ids))\n",
    "            \n",
    "        return outputs, (hn,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, use_cuda, encoder, hidden_dim, max_length=25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.input_dim = encoder.input_dim\n",
    "        self.max_length = max_length\n",
    "        self.vocab = encoder.vocab\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        #self.weight[self.vocab.word2token['<eos>']]=1.01\n",
    "        #self.weight[self.vocab.word2token['<split>']]=1.01\n",
    "        \n",
    "        self.lstmcell = torch.nn.LSTMCell(input_size=self.input_dim, hidden_size=self.hidden_dim*2, bias=True)\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=encoder.embed# reference share\n",
    "        #fcnn: projection for crossentroy loss\n",
    "        self.fcnn = nn.Linear(in_features = self.hidden_dim*2, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.cost_func = nn.CrossEntropyLoss(torch.Tensor(self.weight))\n",
    "        \n",
    "        print('init lookup embedding matrix size: ', self.embed.weight.data.size())\n",
    "        \n",
    "    def forward(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = 0\n",
    "        predicts = []\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "\n",
    "        for ii in range(self.max_length+1):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,0])\n",
    "                    all_loss+=loss\n",
    "                \n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #print('max_idxs size: ',max_idxs.size(), max_idxs)\n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "                    all_loss+=loss\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                #print('max_idx size: ', max_idxs.size(), max_idxs)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "        \n",
    "        if is_train:  #training\n",
    "            if self.use_cuda:\n",
    "                return all_loss/(self.max_length+1), predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return all_loss/(self.max_length+1), predicts.data.numpy()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return predicts.data.numpy()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _inflate(tensor, times, dim):\n",
    "    \"\"\"\n",
    "    Examples::\n",
    "        >> a = torch.LongTensor([[1, 2], [3, 4]])\n",
    "        >> a\n",
    "        1   2\n",
    "        3   4\n",
    "        [torch.LongTensor of size 2x2]\n",
    "        >> b = ._inflate(a, 2, dim=1)\n",
    "        >> b\n",
    "        1   2   1   2\n",
    "        3   4   3   4\n",
    "        [torch.LongTensor of size 2x4]\n",
    "    \"\"\"\n",
    "    repeat_dims = [1] * tensor.dim()\n",
    "    repeat_dims[dim] = times\n",
    "    return tensor.repeat(*repeat_dims)\n",
    "\n",
    "class TopKDecoder(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Top-K decoding with beam search.\n",
    "\n",
    "    Args:\n",
    "        decoder_rnn (DecoderRNN): An object of DecoderRNN used for decoding.\n",
    "        k (int): Size of the beam.\n",
    "\n",
    "    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n",
    "        - **inputs** (seq_len, batch, input_size): list of sequences, whose length is the batch size and within which\n",
    "          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default is `None`)\n",
    "        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features\n",
    "          in the hidden state `h` of encoder. Used as the initial hidden state of the decoder.\n",
    "        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n",
    "          Used for attention mechanism (default is `None`).\n",
    "        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n",
    "          (default is `torch.nn.functional.log_softmax`).\n",
    "        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n",
    "          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n",
    "          teacher forcing would be used (default is 0).\n",
    "\n",
    "    Outputs: decoder_outputs, decoder_hidden, ret_dict\n",
    "        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n",
    "          outputs of the decoder.\n",
    "        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n",
    "          state of the decoder.\n",
    "        - **ret_dict**: dictionary containing additional information as follows {*length* : list of integers\n",
    "          representing lengths of output sequences, *topk_length*: list of integers representing lengths of beam search\n",
    "          sequences, *sequence* : list of sequences, where each sequence is a list of predicted token IDs,\n",
    "          *topk_sequence* : list of beam search sequences, each beam is a list of token IDs, *inputs* : target\n",
    "          outputs if provided for decoding}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder_rnn, k):\n",
    "        super(TopKDecoder, self).__init__()\n",
    "        self.rnn = decoder_rnn\n",
    "        self.k = k\n",
    "        self.hidden_size = self.rnn.hidden_dim\n",
    "        self.V = len(self.rnn.vocab.word2token)\n",
    "        self.SOS = self.rnn.vocab.word2token['<sos>']\n",
    "        self.EOS = self.rnn.vocab.word2token['<eos>']\n",
    "        self.use_cuda = self.rnn.use_cuda\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var\n",
    "\n",
    "    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None, function=F.log_softmax,\n",
    "                    teacher_forcing_ratio=0, retain_output_probs=True):\n",
    "        \"\"\"\n",
    "        Forward rnn for MAX_LENGTH steps.  Look at :func:`seq2seq.models.DecoderRNN.DecoderRNN.forward_rnn` for details.\n",
    "        \"\"\"\n",
    "\n",
    "#         inputs, batch_size, max_length = self.rnn._validate_args(inputs, encoder_hidden, encoder_outputs,\n",
    "#                                                                  function, teacher_forcing_ratio)\n",
    "\n",
    "        batch_size = encoder_outputs.size(dim=0)\n",
    "        max_length = self.rnn.max_length+1\n",
    "        \n",
    "        self.pos_index = self._tocuda(Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1))\n",
    "\n",
    "        # Inflate the initial hidden states to be of size: b*k x h\n",
    "#         encoder_hidden = self.rnn._init_state(encoder_hidden)\n",
    "#         if encoder_hidden is None:\n",
    "#             hidden = None\n",
    "#         else:\n",
    "#             if isinstance(encoder_hidden, tuple):\n",
    "#                 hidden = tuple([_inflate(h, self.k, 1) for h in encoder_hidden])\n",
    "#             else:\n",
    "#                 hidden = _inflate(encoder_hidden, self.k, 1)\n",
    "\n",
    "        hidden = tuple([_inflate(h, self.k, 1).view(batch_size*self.k, -1) for h in encoder_hidden])\n",
    "        #print('hidden0 size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "        # ... same idea for encoder_outputs and decoder_outputs\n",
    "#         if self.rnn.use_attention:\n",
    "#             inflated_encoder_outputs = _inflate(encoder_outputs, self.k, 0)\n",
    "#         else:\n",
    "#             inflated_encoder_outputs = None\n",
    "\n",
    "        # Initialize the scores; for the first step,\n",
    "        # ignore the inflated copies to avoid duplicate entries in the top k\n",
    "        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n",
    "        sequence_scores.fill_(-float('Inf'))\n",
    "        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n",
    "        sequence_scores = self._tocuda(Variable(sequence_scores))\n",
    "\n",
    "        # Initialize the input vector\n",
    "        input_var = self._tocuda(Variable(torch.LongTensor([self.SOS] * batch_size * self.k)))\n",
    "\n",
    "        # Store decisions for backtracking\n",
    "        stored_outputs = list()\n",
    "        stored_scores = list()\n",
    "        stored_predecessors = list()\n",
    "        stored_emitted_symbols = list()\n",
    "        stored_hidden = list()\n",
    "\n",
    "        for ii in range(0, max_length):\n",
    "            # Run the RNN one step forward\n",
    "            #print('setp: %s'%ii)\n",
    "            input_vec = self.rnn.embed(input_var)\n",
    "            #print('input_var and input_vec size: ', input_var.size(), input_vec.size())\n",
    "            hidden = self.rnn.lstmcell(input_vec, hidden)\n",
    "            #print('hidden size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "            \n",
    "            log_softmax_output = self.log_softmax(self.rnn.fcnn(hidden[0]))\n",
    "            # If doing local backprop (e.g. supervised training), retain the output layer\n",
    "#             if retain_output_probs:\n",
    "#                 stored_outputs.append(log_softmax_output)\n",
    "\n",
    "            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n",
    "            sequence_scores = _inflate(sequence_scores, self.V, 1)\n",
    "            sequence_scores += log_softmax_output.squeeze(1)\n",
    "            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n",
    "\n",
    "            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n",
    "            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n",
    "            sequence_scores = scores.view(batch_size * self.k, 1)\n",
    "\n",
    "#             if ii ==0:\n",
    "#                 result0 = input_var.data.cpu().tolist()\n",
    "#                 print(result0)\n",
    "#                 result0=batch_tokens2words(result0, vocab)\n",
    "#                 print(result0)\n",
    "            # Update fields for next timestep\n",
    "            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple([h.index_select(0, predecessors.squeeze()) for h in hidden])\n",
    "            else:\n",
    "                hidden = hidden.index_select(0, predecessors.squeeze())\n",
    "\n",
    "            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n",
    "            stored_scores.append(sequence_scores.clone())\n",
    "            eos_indices = input_var.data.eq(self.EOS)\n",
    "            if eos_indices.nonzero().dim() > 0:\n",
    "                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n",
    "\n",
    "            # Cache results for backtracking\n",
    "            stored_predecessors.append(predecessors)\n",
    "            stored_emitted_symbols.append(input_var)\n",
    "            stored_hidden.append(hidden)\n",
    "\n",
    "        # Do backtracking to return the optimal values\n",
    "        output, h_t, h_n, s, l, p = self._backtrack(stored_hidden,\n",
    "                                                    stored_predecessors, stored_emitted_symbols,\n",
    "                                                    stored_scores, batch_size, self.hidden_size)\n",
    "\n",
    "        # Build return objects\n",
    "#         decoder_outputs = [step[:, 0, :] for step in output]\n",
    "        if isinstance(h_n, tuple):\n",
    "            decoder_hidden = tuple([h[:, :, 0, :] for h in h_n])\n",
    "        else:\n",
    "            decoder_hidden = h_n[:, :, 0, :]\n",
    "        metadata = {}\n",
    "        metadata['inputs'] = inputs\n",
    "        metadata['output'] = output\n",
    "        metadata['h_t'] = h_t\n",
    "        metadata['score'] = s\n",
    "        metadata['topk_length'] = l\n",
    "        metadata['topk_sequence'] = p\n",
    "        metadata['length'] = [seq_len[0] for seq_len in l]\n",
    "        metadata['sequence'] = [seq[0] for seq in p]\n",
    "        return decoder_hidden, metadata\n",
    "\n",
    "    def _backtrack(self, nw_hidden, predecessors, symbols, scores, b, hidden_size):\n",
    "        \"\"\"Backtracks over batch to generate optimal k-sequences.\n",
    "\n",
    "        Args:\n",
    "            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n",
    "            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n",
    "            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n",
    "            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n",
    "            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n",
    "            b: Size of the batch\n",
    "            hidden_size: Size of the hidden state\n",
    "\n",
    "        Returns:\n",
    "            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n",
    "\n",
    "            score [batch, k]: A list containing the final scores for all top-k sequences\n",
    "\n",
    "            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n",
    "\n",
    "            p (batch, k, sequence_len): A Tensor containing predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        lstm = isinstance(nw_hidden[0], tuple)\n",
    "\n",
    "        # initialize return variables given different types\n",
    "        output = list()\n",
    "        h_t = list()\n",
    "        p = list()\n",
    "        # Placeholder for last hidden state of top-k sequences.\n",
    "        # If a (top-k) sequence ends early in decoding, `h_n` contains\n",
    "        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n",
    "        # the last hidden state of decoding.\n",
    "        if lstm:\n",
    "            state_size = nw_hidden[0][0].size()\n",
    "            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n",
    "        else:\n",
    "            h_n = torch.zeros(nw_hidden[0].size())\n",
    "        l = [[self.rnn.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n",
    "                                                                # Similar to `h_n`\n",
    "\n",
    "        # the last step output of the beams are not sorted\n",
    "        # thus they are sorted here\n",
    "        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n",
    "        # initialize the sequence scores with the sorted last step beam scores\n",
    "        s = sorted_score.clone()\n",
    "\n",
    "        batch_eos_found = [0] * b   # the number of EOS found\n",
    "                                    # in the backward loop below for each batch\n",
    "\n",
    "        t = self.rnn.max_length - 1\n",
    "        # initialize the back pointer with the sorted order of the last step beams.\n",
    "        # add self.pos_index for indexing variable with b*k as the first dimension.\n",
    "        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n",
    "        while t >= 0:\n",
    "            # Re-order the variables with the back pointer\n",
    "#             current_output = nw_output[t].index_select(0, t_predecessors)\n",
    "            if lstm:\n",
    "                current_hidden = tuple([h.index_select(0, t_predecessors) for h in nw_hidden[t]])\n",
    "            else:\n",
    "                current_hidden = nw_hidden[t].index_select(0, t_predecessors)\n",
    "            current_symbol = symbols[t].index_select(0, t_predecessors)\n",
    "            # Re-order the back pointer of the previous step with the back pointer of\n",
    "            # the current step\n",
    "            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n",
    "\n",
    "            # This tricky block handles dropped sequences that see EOS earlier.\n",
    "            # The basic idea is summarized below:\n",
    "            #\n",
    "            #   Terms:\n",
    "            #       Ended sequences = sequences that see EOS early and dropped\n",
    "            #       Survived sequences = sequences in the last step of the beams\n",
    "            #\n",
    "            #       Although the ended sequences are dropped during decoding,\n",
    "            #   their generated symbols and complete backtracking information are still\n",
    "            #   in the backtracking variables.\n",
    "            #   For each batch, everytime we see an EOS in the backtracking process,\n",
    "            #       1. If there is survived sequences in the return variables, replace\n",
    "            #       the one with the lowest survived sequence score with the new ended\n",
    "            #       sequences\n",
    "            #       2. Otherwise, replace the ended sequence with the lowest sequence\n",
    "            #       score with the new ended sequence\n",
    "            #\n",
    "            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n",
    "            if eos_indices.dim() > 0:\n",
    "                for i in range(eos_indices.size(0)-1, -1, -1):\n",
    "                    # Indices of the EOS symbol for both variables\n",
    "                    # with b*k as the first dimension, and b, k for\n",
    "                    # the first two dimensions\n",
    "                    idx = eos_indices[i]\n",
    "                    b_idx = int(idx[0] / self.k)\n",
    "                    # The indices of the replacing position\n",
    "                    # according to the replacement strategy noted above\n",
    "                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n",
    "                    batch_eos_found[b_idx] += 1\n",
    "                    res_idx = b_idx * self.k + res_k_idx\n",
    "\n",
    "                    # Replace the old information in return variables\n",
    "                    # with the new ended sequence information\n",
    "                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n",
    "#                     current_output[res_idx, :] = nw_output[t][idx[0], :]\n",
    "                    if lstm:\n",
    "                        current_hidden[0][res_idx, :] = nw_hidden[t][0][idx[0], :]\n",
    "                        current_hidden[1][res_idx, :] = nw_hidden[t][1][idx[0], :]\n",
    "                        h_n[0][res_idx, :] = nw_hidden[t][0][idx[0], :].data\n",
    "                        h_n[1][res_idx, :] = nw_hidden[t][1][idx[0], :].data\n",
    "                    else:\n",
    "                        current_hidden[res_idx, :] = nw_hidden[t][idx[0], :]\n",
    "                        h_n[res_idx, :] = nw_hidden[t][idx[0], :].data\n",
    "                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n",
    "                    s[b_idx, res_k_idx] = scores[t][idx[0]].data[0]\n",
    "                    l[b_idx][res_k_idx] = t + 1\n",
    "\n",
    "            # record the back tracked results\n",
    "#             output.append(current_output)\n",
    "            h_t.append(current_hidden)\n",
    "            p.append(current_symbol)\n",
    "\n",
    "            t -= 1\n",
    "\n",
    "        # Sort and re-order again as the added ended sequences may change\n",
    "        # the order (very unlikely)\n",
    "        s, re_sorted_idx = s.topk(self.k)\n",
    "        for b_idx in range(b):\n",
    "            l[b_idx] = [l[b_idx][k_idx.data[0]] for k_idx in re_sorted_idx[b_idx,:]]\n",
    "\n",
    "        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n",
    "\n",
    "        # Reverse the sequences and re-order at the same time\n",
    "        # It is reversed because the backtracking happens in reverse time order\n",
    "#         output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n",
    "        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n",
    "        if lstm:\n",
    "            h_t = [tuple([h.index_select(0, re_sorted_idx).view(-1, b, self.k, hidden_size) for h in step]) for step in reversed(h_t)]\n",
    "            h_n = tuple([self._tocuda(x) for x in h_n])\n",
    "            h_n = tuple([h.index_select(0, re_sorted_idx.data).view(-1, b, self.k, hidden_size) for h in h_n])\n",
    "        else:\n",
    "            h_t = [step.index_select(0, re_sorted_idx).view(-1, b, self.k, hidden_size) for step in reversed(h_t)]\n",
    "            h_n = h_n.index_select(0, re_sorted_idx.data).view(-1, b, self.k, hidden_size)\n",
    "        s = s.data\n",
    "\n",
    "        #    --- fake output ---\n",
    "        output = None\n",
    "        #    --- fake ---\n",
    "        return output, h_t, h_n, s, l, p\n",
    "\n",
    "    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n",
    "            score[idx] = masking_score\n",
    "\n",
    "    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n",
    "        if len(idx.size()) > 0:\n",
    "            indices = idx[:, 0]\n",
    "            tensor.index_fill_(dim, indices, masking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, use_cuda, input_dim, hidden_dim, vocab, max_length = 25):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.enc = Encoder(use_cuda=use_cuda, hidden_dim=hidden_dim, input_dim=input_dim, vocab=vocab)\n",
    "        self.dec = Decoder(use_cuda=use_cuda, encoder=self.enc, hidden_dim=hidden_dim, max_length=max_length)\n",
    "        if use_cuda:\n",
    "            self.enc = self.enc.cuda()\n",
    "            self.dec = self.dec.cuda()\n",
    "    def forward(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        if is_train:\n",
    "            loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                    h0_and_c0=(enc_hn, enc_cn), \n",
    "                                    sent_lens=input_lens,\n",
    "                                    labels=torch.LongTensor(labels), \n",
    "                                    is_train=1, \n",
    "                                    teaching_rate = 1\n",
    "                                    )\n",
    "            return loss, predicts\n",
    "        else:\n",
    "            predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                sent_lens=input_lens,\n",
    "                                labels=torch.LongTensor(labels), \n",
    "                                is_train=0, \n",
    "                                teaching_rate = 1\n",
    "                                )\n",
    "            return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n",
      "1 5000 time: 0.00 mins bleu: 0.8149\n",
      "21 5000 time: 0.00 mins bleu: 0.7195\n",
      "41 5000 time: 0.00 mins bleu: 0.7145\n",
      "61 5000 time: 1.00 mins bleu: 0.7142\n",
      "81 5000 time: 1.00 mins bleu: 0.7036\n",
      "101 5000 time: 2.00 mins bleu: 0.7044\n",
      "121 5000 time: 2.00 mins bleu: 0.7065\n",
      "141 5000 time: 3.00 mins bleu: 0.7083\n",
      "161 5000 time: 3.00 mins bleu: 0.7039\n",
      "181 5000 time: 3.00 mins bleu: 0.7003\n",
      "201 5000 time: 4.00 mins bleu: 0.6971\n",
      "221 5000 time: 4.00 mins bleu: 0.6943\n",
      "241 5000 time: 5.00 mins bleu: 0.6933\n",
      "261 5000 time: 5.00 mins bleu: 0.6945\n",
      "281 5000 time: 6.00 mins bleu: 0.6948\n",
      "301 5000 time: 6.00 mins bleu: 0.6943\n",
      "321 5000 time: 6.00 mins bleu: 0.6949\n",
      "341 5000 time: 7.00 mins bleu: 0.6938\n",
      "361 5000 time: 7.00 mins bleu: 0.6953\n",
      "381 5000 time: 8.00 mins bleu: 0.6976\n",
      "401 5000 time: 8.00 mins bleu: 0.6975\n",
      "421 5000 time: 9.00 mins bleu: 0.6985\n",
      "441 5000 time: 9.00 mins bleu: 0.6980\n",
      "461 5000 time: 9.00 mins bleu: 0.6962\n",
      "481 5000 time: 10.00 mins bleu: 0.6962\n",
      "501 5000 time: 10.00 mins bleu: 0.6978\n",
      "521 5000 time: 11.00 mins bleu: 0.6969\n",
      "541 5000 time: 11.00 mins bleu: 0.6971\n",
      "561 5000 time: 11.00 mins bleu: 0.6980\n",
      "581 5000 time: 12.00 mins bleu: 0.6985\n",
      "601 5000 time: 12.00 mins bleu: 0.6989\n",
      "621 5000 time: 13.00 mins bleu: 0.6979\n",
      "641 5000 time: 13.00 mins bleu: 0.6974\n",
      "661 5000 time: 14.00 mins bleu: 0.6978\n",
      "681 5000 time: 14.00 mins bleu: 0.6973\n",
      "701 5000 time: 14.00 mins bleu: 0.6970\n",
      "721 5000 time: 15.00 mins bleu: 0.6969\n",
      "741 5000 time: 15.00 mins bleu: 0.6973\n",
      "761 5000 time: 16.00 mins bleu: 0.6977\n",
      "781 5000 time: 16.00 mins bleu: 0.6984\n",
      "801 5000 time: 17.00 mins bleu: 0.6979\n",
      "821 5000 time: 17.00 mins bleu: 0.6984\n",
      "841 5000 time: 17.00 mins bleu: 0.6987\n",
      "861 5000 time: 18.00 mins bleu: 0.6985\n",
      "881 5000 time: 18.00 mins bleu: 0.6987\n",
      "901 5000 time: 19.00 mins bleu: 0.6987\n",
      "921 5000 time: 19.00 mins bleu: 0.6990\n",
      "941 5000 time: 20.00 mins bleu: 0.6993\n",
      "961 5000 time: 20.00 mins bleu: 0.6985\n",
      "981 5000 time: 20.00 mins bleu: 0.6977\n",
      "1001 5000 time: 21.00 mins bleu: 0.6990\n",
      "1021 5000 time: 21.00 mins bleu: 0.6994\n",
      "1041 5000 time: 22.00 mins bleu: 0.6992\n",
      "1061 5000 time: 22.00 mins bleu: 0.6986\n",
      "1081 5000 time: 23.00 mins bleu: 0.6988\n",
      "1101 5000 time: 23.00 mins bleu: 0.6986\n",
      "1121 5000 time: 24.00 mins bleu: 0.6981\n",
      "1141 5000 time: 24.00 mins bleu: 0.6982\n",
      "1161 5000 time: 24.00 mins bleu: 0.6976\n",
      "1181 5000 time: 25.00 mins bleu: 0.6977\n",
      "1201 5000 time: 25.00 mins bleu: 0.6973\n",
      "1221 5000 time: 26.00 mins bleu: 0.6978\n",
      "1241 5000 time: 26.00 mins bleu: 0.6977\n",
      "1261 5000 time: 26.00 mins bleu: 0.6971\n",
      "1281 5000 time: 27.00 mins bleu: 0.6972\n",
      "1301 5000 time: 27.00 mins bleu: 0.6975\n",
      "1321 5000 time: 28.00 mins bleu: 0.6975\n",
      "1341 5000 time: 28.00 mins bleu: 0.6977\n",
      "1361 5000 time: 29.00 mins bleu: 0.6976\n",
      "1381 5000 time: 29.00 mins bleu: 0.6976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8bc9b4b72620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_beamsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menc_hn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topk_sequence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d1ddc3655ed1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio, retain_output_probs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output, h_t, h_n, s, l, p = self._backtrack(stored_hidden,\n\u001b[1;32m    165\u001b[0m                                                     \u001b[0mstored_predecessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstored_emitted_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                                     stored_scores, batch_size, self.hidden_size)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Build return objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d1ddc3655ed1>\u001b[0m in \u001b[0;36m_backtrack\u001b[0;34m(self, nw_hidden, predecessors, symbols, scores, b, hidden_size)\u001b[0m\n\u001b[1;32m    292\u001b[0m                         \u001b[0mcurrent_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                         \u001b[0mcurrent_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                         \u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                         \u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "pre_train = torch.load('./models_better/time-[2019-01-08-06-27-50]-loss-0.925657809-bleu-0.6640-hidden_dim-256-input_dim-300-epoch-2-batch_size-200-batch_id-[17001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "autoencoder.eval()\n",
    "\n",
    "train_set_size = len(train_set_inputs)\n",
    "sample_num=20\n",
    "topk=10\n",
    "batch_id=0\n",
    "bleu_sum=0\n",
    "\n",
    "dec_beamsearch = TopKDecoder(decoder_rnn=autoencoder.dec, k=topk)\n",
    "\n",
    "start_time=time.time()\n",
    "for start_idx in range(0, train_set_size-sample_num, sample_num):\n",
    "    batch_id+=1\n",
    "    \n",
    "    enc_outputs, (enc_hn, enc_cn) = autoencoder.enc(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                        torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]))\n",
    "    #print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "    _, metadata = dec_beamsearch(encoder_hidden = (enc_hn, enc_cn), encoder_outputs = enc_outputs)\n",
    "\n",
    "    results = metadata['topk_sequence']\n",
    "    results =torch.cat(results, dim = 2)\n",
    "    results=results.view(sample_num*topk, -1)\n",
    "    indices = torch.LongTensor([x*topk for x in range(sample_num)]).cuda()\n",
    "    results = results.data.index_select(0, indices)\n",
    "    results=results.cpu().tolist()\n",
    "    results=batch_tokens_remove_eos(results, vocab)\n",
    "#     results=batch_tokens2words(results, vocab)\n",
    "#     results=batch_words2sentence(results)\n",
    "#     print(results)\n",
    "    \n",
    "    inputs = train_set_inputs[start_idx:start_idx+sample_num]\n",
    "#     inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "#     inputs = batch_tokens2words(inputs, vocab)\n",
    "    inputs_=[]\n",
    "    for tokens in inputs:\n",
    "        x=[]\n",
    "        for token in tokens:\n",
    "            if token!=vocab.word2token['<padding>']:\n",
    "                x.append(token)\n",
    "            else:\n",
    "                break\n",
    "        inputs_.append(x)\n",
    "#     inputs = batch_words2sentence(inputs_)\n",
    "    bleu_scores = batch_tokens_bleu(references=inputs_, candidates=results)\n",
    "    for score in bleu_scores:\n",
    "        bleu_sum+=score\n",
    "    \n",
    "    if batch_id%20==1:\n",
    "        print(batch_id, int(train_set_size/sample_num), 'time: %4.2f mins'%((time.time()-start_time)/60), 'bleu: %2.4f'%(bleu_sum/batch_id/sample_num))\n",
    "    \n",
    "    \n",
    "\n",
    "#    print inputs\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "# print(inputs)\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "for sent in inputs:\n",
    "    print(sent)\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print('results[0] size: ', results[0].size())\n",
    "a=torch.cat(results, dim = 2)\n",
    "b=a.view(sample_num*topk, -1)\n",
    "print(a.size(), b.size())\n",
    "c=b.data.cpu().tolist()\n",
    "d=batch_tokens_remove_eos(c, vocab)\n",
    "e=batch_tokens2words(d, vocab)\n",
    "f=batch_words2sentence(e)\n",
    "for sent in f:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n",
      "1 500 time: 0.01 mins bleu: 0.6400\n",
      "21 500 time: 0.12 mins bleu: 0.6536\n",
      "41 500 time: 0.26 mins bleu: 0.6533\n",
      "61 500 time: 0.39 mins bleu: 0.6536\n",
      "81 500 time: 0.52 mins bleu: 0.6529\n",
      "101 500 time: 0.66 mins bleu: 0.6548\n",
      "121 500 time: 0.78 mins bleu: 0.6526\n",
      "141 500 time: 0.92 mins bleu: 0.6528\n",
      "161 500 time: 1.05 mins bleu: 0.6513\n",
      "181 500 time: 1.18 mins bleu: 0.6506\n",
      "201 500 time: 1.31 mins bleu: 0.6504\n",
      "221 500 time: 1.45 mins bleu: 0.6505\n",
      "241 500 time: 1.58 mins bleu: 0.6509\n",
      "261 500 time: 1.72 mins bleu: 0.6523\n",
      "281 500 time: 1.86 mins bleu: 0.6516\n",
      "301 500 time: 1.98 mins bleu: 0.6515\n",
      "321 500 time: 2.11 mins bleu: 0.6513\n",
      "341 500 time: 2.24 mins bleu: 0.6514\n",
      "361 500 time: 2.37 mins bleu: 0.6513\n",
      "381 500 time: 2.50 mins bleu: 0.6518\n",
      "401 500 time: 2.62 mins bleu: 0.6520\n",
      "421 500 time: 2.77 mins bleu: 0.6524\n",
      "441 500 time: 2.89 mins bleu: 0.6525\n",
      "461 500 time: 3.03 mins bleu: 0.6526\n",
      "481 500 time: 3.16 mins bleu: 0.6522\n",
      "开上海域！我们在仪表飞行。\n",
      "他们不会放过任何一个认识你的人，<low_freq>。你当心点。\n",
      "但同样的，我们的设备是支持<low_freq>硬件解码的。\n",
      "我们的风云人物来了。恭喜啊。\n",
      "第三条国家外汇管理局及其分支局（以下简称外汇局）负责大额和可疑外汇资金交易报告工作的监督和管理。\n",
      "对。所以你是说和我共事是种惩罚\n",
      "不管怎么样，事情一件接一件，而且……\n",
      "如果你不合作的话，\n",
      "你得保证是完美的一次！\n",
      "贫穷的朋友,因为患难之交是真情。\n",
      "我说了别接听！\n",
      "你说完了？什么？\n",
      "他拒绝了本纪录片的采访。\n",
      "我觉得不可能警戒事发地点。\n",
      "你们要点今日的推荐自选<low_freq>吗？\n",
      "合作品牌网站上的广告。\n",
      "因为演员不够好他在找到替补前就把人开了！\n",
      "实际测试显示模块运作完美。\n",
      "最好是两个男孩两个女孩...\n",
      "快，上车，带上他\n",
      "所以曾经的敌人，现在是朋友。我帮你，你帮我？\n",
      "在我地说话时就是发生这些事。\n",
      "我第一次在河上看见这么大的船。\n",
      "我们的新住所太<low_freq>了，\n",
      "中华人民共和国政府和<low_freq>及北爱尔兰联合王国政府关于香港问题的联合声明。\n",
      "我要等的不是王子。\n",
      "我只需要睡一会儿。\n",
      "找到你们了。抱歉推迟了谈话。\n",
      "在荷兰过着安静，普通生活的人。\n",
      "真高兴认识了你,歇洛克福尔摩斯\n",
      "好像<low_freq><low_freq>一样。\n",
      "我在壁橱里找到的。\n",
      "很多头发问题都是由饮食引起的。\n",
      "德瑞<low_freq>勒斯花园贝斯特韦斯特酒店距离火车站大约15分钟的车程。\n",
      "不管有怎样的艰难险阻，\n",
      "好，听着，从那个生物危害处理厂我们查不到尸体从哪来的。\n",
      "他是在保护他的上级。\n",
      "接收叛国者，约翰达德利。你跟我走，大人。\n",
      "事实上，简单来说有一个更早的领导者：强生公司在1930年为医疗用途开发了一次性尿布（称为\n",
      "你能帮我摆脱妻子？\n",
      "所以求你们了，请务必让我履行自己的职责。\n",
      "此电邮以密件传给我名单中所有的人以便大家得以分享之。\n",
      "好像我粉碎了他的梦想之类的一样但是规定就是规定。\n",
      "她不让孩子<low_freq>这点很有意思。\n",
      "卡洛斯没有放弃。\n",
      "帮助她什么！\n",
      "啊！等等嗨。你认识那个老夫人吗？\n",
      "我们到<low_freq>com网站查了查，第一个定义是“目无法纪、欺诈成性且靠不住的人”。\n",
      "怪你觉得她不漂亮。是她睡着的时候。\n",
      "那么，我们现在攀爬的是啥年代的？\n",
      "就在停车场死着。\n",
      "他要去法院。\n",
      "你的罪行就会<low_freq>天下了。\n",
      "受害人的脚踝不是<low_freq>捆绑痕迹吗？\n",
      "中国的“吃货”们守在银幕前，一边被诱人美食<low_freq><low_freq>口水，一边为质朴的人情故事\n",
      "45分钟内与卡斯特罗会面。\n",
      "如果让我自己选的话，\n",
      "我时不时地有罪恶感。\n",
      "她人更好一些。\n",
      "其实啊，得利不一样了...\n",
      "我可是“初中可是<low_freq>过来的”？\n",
      "布鲁塞尔钻石交易会安特卫普钻石交易会。\n",
      "你在注视那个女的。\n",
      "你家族的姓氏就随你而终。\n",
      "我为什么不能吃熏肉肠？\n",
      "先先生？\n",
      "他说他要帮我们的忙，但最后却丢开我们不管了。\n",
      "去和别人用这个东西吧。我要你滚出这里。现在。\n",
      "我不敢相信你怎么舍得离开。\n",
      "另外不管怎么说我也有很多年没穿过这些了。\n",
      "尤其是查尔斯王子。\n",
      "一个下午，我们光着身子安静地在温泉里坐了近3个小时。\n",
      "我觉得最好还是坚持…\n",
      "什么又是生命？\n",
      "一女孩儿对她伙伴说。\n",
      "因为我们一直想要这样。因为孩子们还很小。\n",
      "我们广为人知的免费丰盛的热自助早餐包括定制煎蛋卷、煎鸡蛋、培根、香肠、新鲜出炉的日常的肉桂\n",
      "我只是做好预防准备。\n",
      "愿今年财神爷帮您<low_freq>。\n",
      "从表面来看，赢的一方会吃掉活着的敌人。\n",
      "以及可见的病变。\n",
      "夫人，就在你那里。什么在我那？！\n",
      "但是前东德的<low_freq>看起来似乎还能工作。\n",
      "他的孙女来了<low_freq>。\n",
      "政治不确定性是经济复苏的一大阻碍，但我可能会倾向于强调，由于2008年9月金融崩溃激发\n",
      "如果他进球了，我请你过来，兄弟，坐边上来。\n",
      "此后，香港又推出了人民币支票。\n",
      "我吓坏了。\n",
      "但均值回归永远无法预测超乎寻常的事情。\n",
      "我听到了很多挣扎声。\n",
      "但一样死了。\n",
      "如果你还继续抱着把这事儿隐瞒下去的希望\n",
      "<low_freq><low_freq>设计中心，因扩大生产，需印染<low_freq><low_freq>。\n",
      "但是我痊愈了。\n",
      "吓唬他们不让他们自满，不让他们犯傻。\n",
      "她们同时也相信敌人的敌人就是朋友\n",
      "他在童话里，就如同恶魔。\n",
      "我劝你三思而行。\n",
      "当你要打断对方时，稍稍抬抬手，对方就知道你要讲话了。\n",
      "你试图维持一个谎言，\n",
      "介於鸽子跟猫头鹰出没的时刻最美。\n",
      "好啊，谢谢，哥们。\n",
      "除了官方的信息反馈渠道以外，一家名为“奥运吉普赛人”的个体行业应运而生：活跃在各个领域的体育顾问行走\n",
      "对你非常重要。\n",
      "求婚者多的少女往往选中最差的<low_freq>\n",
      "卡尔已经离开家八年之久了，大部分时间都在一个苏维埃劳动集中营。\n",
      "但我不认为这值得引起\n",
      "打仗这方面我很在行。\n",
      "嗯，我确实闻到一股巧克力曲奇的味道。我也是。\n",
      "我把钱用在我的孩子身上？\n",
      "他死在医院里。\n",
      "骗取税款超过所缴纳的税款部分，依照前款的规定处罚。\n",
      "我不用它拍屏幕我拍摄。我自己的反应回家看。\n",
      "去年的假日季中，沃尔玛在美国数个城市试点推出了本地门店网上订单的当日递送业务。\n",
      "尽是些毒贩子。\n",
      "由于费用高昂，对大多数土库曼人来说，网络成了奢侈品。土库曼斯坦政府也正是利用这个手段限制人们使用\n",
      "我告诉年轻人，这个非常热门。\n",
      "嗨，伙计们，我一会再打给你们？\n",
      "我觉得很奇怪那那家伙也很怪\n",
      "你开玩笑吧，他...\n",
      "你带给我的礼物。是很美丽但我更想你！\n",
      "洛伦佐，那孩子载了跟头不是你的错。\n",
      "克拉克，我给你爸妈打个电话。\n",
      "我可是月历先生！\n",
      "旅途如何？\n",
      "我只希望跟她说话的时候她不拿白眼翻我。\n",
      "我睡觉了，宝贝。\n",
      "因为我们不会离开的。\n",
      "帮他闭上了眼睑。\n",
      "你在开什么玩笑呢先生？大家都在忙呢。\n",
      "告诉他我们不应该这么仓促。\n",
      "实话实说。我哥吃这套。\n",
      "另外那些铁轨，\n",
      "既然这些土地被一起这么久了，我能选择任何的一块么？\n",
      "他认为，国家，最近超过日本成为世界第二大经济体，增长顺利通过继续实行彻底的特色，儒家色彩\n",
      "怕我会杀了他？\n",
      "尖端科技个屁。\n",
      "你现在想买房了？\n",
      "我的巨著极品包里克每个人都有自己的小说而这是我的作品。\n",
      "你不会是坏男孩吧？\n",
      "所以我才在这。你只是因为坏了我的刀所以才在这。\n",
      "不过，我们必须有一个人留下来\n",
      "你在附近吗？\n",
      "汽油，食物，零件以及军火很少从<low_freq>送到<low_freq>，不仅如此，阿富汗方还依赖于美军的大炮，空军力量\n",
      "每天都有，伙计。\n",
      "我吓倒你了吗？没有。\n",
      "使得我失去了冷静。\n",
      "有时这种电梯里会有...\n",
      "是大豆做成的，干净燃烧。把它<low_freq>你的性伙伴的全身。\n",
      "反正不是白金汉宫\n",
      "先离开这里，好吗？\n",
      "也不过是个测试而已\n",
      "法外处决，监禁是一种癌症。\n",
      "我们说的是警察操作不当，\n",
      "他的净资产还是2元，他的心也很忐忑不安。\n",
      "你对付那两个，我对付这几个。\n",
      "昨天早上离开的不是索沃德太太。\n",
      "他总是出人意料。\n",
      "是啊。因为她是同性恋。\n",
      "您可以改变母版上的项目符号，使得每个幻灯片中的项目符号同时改变。方法是：指向“视图\n",
      "不错。看来十分顺利。\n",
      "我们说的始终是在企业号<low_freq>光速飞行时。\n",
      "但家里的其他成员却并不爱<low_freq>。\n",
      "每天喝一两杯酒能够增加你的记忆力，甚至对老年痴呆症有帮助。\n",
      "在最初的几个小时内，记忆会被更稳固化，以抵抗来自竞争信息的干扰。\n",
      "这也就意味着他能够买到那些去了圣马克、哈佛或耶鲁的家伙们抛弃的衣服，所以他\n",
      "我们将三个摄像头的记录拼接在一起。\n",
      "你的眼睛恶化了。\n",
      "大部份软木是<low_freq>，也就是说，每年到了秋季它们的叶也不会脱落。但其中有一个例外，即\n",
      "我以为汤姆•克鲁斯会住在更好的地方呢。\n",
      "这样使钙离子进入细胞，钙的增加控制着精子鞭毛的摆动节奏，可以使精子转<low_freq>或<low_freq>\n",
      "德拉吉必须改变方向。\n",
      "我想她在去他家的路上。\n",
      "关于<low_freq>统治时期的教廷。\n",
      "二战的结果以及随之而来与共产主义阵营之间的冷战，使日本坚定地站到了美国和欧洲一边。\n",
      "我真笨！是吗？\n",
      "宝贝儿，谢谢你。\n",
      "听着，好消息是。\n",
      "卢戈说：“房地产管理并不属于银行业务”，因此它们愿意“以巨大的折扣”进行转让。\n",
      "最有可能的是不会，特别当全球经济正在经历螺旋式的衰退。\n",
      "<low_freq>是垫球的一种。\n",
      "青春是一个短暂的美梦，当你醒来时，它早已消失无踪。\n",
      "把他的手反绑在后面。\n",
      "如果想有自己的生活就该离你越远越好。\n",
      "<low_freq>有个<low_freq>啊！\n",
      "联邦应急管理局局长表示，该部门已经为亚特兰大的飓风季节做好了准备。\n",
      "他已经满足，结果他妈却收了钱把他当做一个实验小白鼠。\n",
      "抱歉，医生工作。\n",
      "引起了多少诉讼与逮捕？\n",
      "我说这不是\n",
      "别给我看她这样的照片。\n",
      "安蒂，在我身上\n",
      "宗教裁判<low_freq>盯着我。\n",
      "你只付了一个钱。那就只得一个。\n",
      "我什么也没有，艾伯特。\n",
      "所以你才会如此的愤怒。\n",
      "我能帮你吗？我来找奥雷格。\n",
      "有时遇到赏赐也跳一次<low_freq>以表谢意。\n",
      "而且，巴菲特表示，恶劣天气仍在美国继续肆虐：8月1日，纽约州长岛发生了反常的<low_freq>\n",
      "几十年来，西班牙海岸一直都是欧洲最理想的海边度假胜地，有钱人会躲到这里的私宅来度假。\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a99b8688d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results[0] size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "pre_train = torch.load('./models_better/time-[2019-01-08-06-27-50]-loss-0.925657809-bleu-0.6640-hidden_dim-256-input_dim-300-epoch-2-batch_size-200-batch_id-[17001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "autoencoder.eval()\n",
    "\n",
    "train_set_size = len(train_set_inputs)\n",
    "sample_num=200\n",
    "batch_id=0\n",
    "bleu_sum=0\n",
    "\n",
    "start_time=time.time()\n",
    "for start_idx in range(0, train_set_size-sample_num, sample_num):\n",
    "    batch_id+=1\n",
    "    \n",
    "    enc_outputs, (enc_hn, enc_cn) = autoencoder.enc(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                        torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]))\n",
    "    predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:start_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[start_idx:start_idx+sample_num]), \n",
    "                                             labels=999, \n",
    "                                             is_train=0, teaching_rate=1)\n",
    "\n",
    "    results=batch_tokens_remove_eos(predicts, vocab)\n",
    "#     results=batch_tokens2words(results, vocab)\n",
    "#     results=batch_words2sentence(results)\n",
    "#     print(results)\n",
    "    \n",
    "    inputs = train_set_inputs[start_idx:start_idx+sample_num]\n",
    "#     inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "#     inputs = batch_tokens2words(inputs, vocab)\n",
    "    inputs_=[]\n",
    "    for tokens in inputs:\n",
    "        x=[]\n",
    "        for token in tokens:\n",
    "            if token!=vocab.word2token['<padding>']:\n",
    "                x.append(token)\n",
    "            else:\n",
    "                break\n",
    "        inputs_.append(x)\n",
    "#     inputs = batch_words2sentence(inputs_)\n",
    "    bleu_scores = batch_tokens_bleu(references=inputs_, candidates=results)\n",
    "    for score in bleu_scores:\n",
    "        bleu_sum+=score\n",
    "    \n",
    "    if batch_id%20==1:\n",
    "        print(batch_id, int(train_set_size/sample_num), 'time: %4.2f mins'%((time.time()-start_time)/60), 'bleu: %2.4f'%(bleu_sum/batch_id/sample_num))\n",
    "    \n",
    "    \n",
    "\n",
    "#    print inputs\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "# print(inputs)\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "for sent in inputs:\n",
    "    print(sent)\n",
    "    \n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print('results[0] size: ', results[0].size())\n",
    "a=torch.cat(results, dim = 2)\n",
    "b=a.view(sample_num*topk, -1)\n",
    "print(a.size(), b.size())\n",
    "c=b.data.cpu().tolist()\n",
    "d=batch_tokens_remove_eos(c, vocab)\n",
    "e=batch_tokens2words(d, vocab)\n",
    "f=batch_words2sentence(e)\n",
    "for sent in f:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1f410946e68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m            )\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bf82403aa28d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, use_cuda, hidden_dim, input_dim, vocab)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#loading pre trained word embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pre_train_word_embedding.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "\n",
    "enc = Encoder(use_cuda=use_cuda, \n",
    "            hidden_dim=hidden_dim, \n",
    "            input_dim=input_dim, \n",
    "            vocab=vocab\n",
    "           )\n",
    "if use_cuda:\n",
    "    enc = enc.cuda()\n",
    "    \n",
    "sample_num = 11\n",
    "print('sentences length: ', train_set_input_lens[0:sample_num])\n",
    "\n",
    "enc_outputs, (enc_hn, enc_cn) = enc(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "                                    torch.LongTensor(train_set_input_lens[0:sample_num]))\n",
    "print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "dec = Decoder(use_cuda=use_cuda, encoder=enc, hidden_dim=hidden_dim, max_length=25)\n",
    "if use_cuda:\n",
    "    dec = dec.cuda()\n",
    "    \n",
    "# loss, predicts = dec(enc_outputs = enc_outputs, \n",
    "#                     h0_and_c0=(enc_hn, enc_cn), \n",
    "#                     sent_lens=train_set_input_lens[0:sample_num], \n",
    "#                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "#                     is_train=1, teaching_rate = 1\n",
    "#                     )\n",
    "# print('loss is %4.7f'%loss.data[0])\n",
    "\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "\n",
    "pre_train = torch.load('./models_better/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "\n",
    "autoencoder.eval()\n",
    "predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "                                     torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "                                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "                                     is_train=0, teaching_rate=1)\n",
    "\n",
    "inputs = train_set_inputs[0:sample_num]\n",
    "inputs = batch_tokens_remove_eos(inputs, vocab)\n",
    "results = batch_tokens_remove_eos(predicts, vocab)\n",
    "inputs = batch_tokens2words(inputs, vocab)\n",
    "results = batch_tokens2words(results, vocab)\n",
    "inputs_=[]\n",
    "for words in inputs:\n",
    "    x=[]\n",
    "    for word in words:\n",
    "        if word!='<padding>':\n",
    "            x.append(word)\n",
    "        else:\n",
    "            break\n",
    "    inputs_.append(x)\n",
    "inputs = batch_words2sentence(inputs_)\n",
    "results = batch_words2sentence(results)\n",
    "for inp, res in zip(inputs,results):\n",
    "    print(inp)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_sent2real_sent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a46203c33948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running time: %.2f mins'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a46203c33948>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(epoch, batch_size, train_set_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mlabel_real_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mreal_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sent2real_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_tokenized_sents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mlabel_real_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sent2real_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_sent2real_sent' is not defined"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "lr=0.005\n",
    "batch_size=200\n",
    "train_set_size=int(len(train_set_inputs)/2)\n",
    "epochs=10\n",
    "train_bleu = 0\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 25)\n",
    "#pre train para\n",
    "#pre_train = torch.load('./models_better/loss-2.099016905-bleu-0.4078-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[7001-[of]-21743]-lr-0.0050')\n",
    "#pre_train = torch.load('./models_better/time-[2019-01-07-16-38-14]-loss-1.881381631-bleu-0.5340-hidden_dim-256-input_dim-300-epoch-0-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "pre_train = torch.load('./models_better/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "\n",
    "autoencoder.load_state_dict(pre_train)\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, autoencoder.parameters()), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def model_train(epoch, batch_size, train_set_size):\n",
    "    batch_id = 0\n",
    "    valid_bleu = 0\n",
    "    for start_idx in range(0, train_set_size-batch_size, batch_size):\n",
    "        batch_id+=1\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        optimizer.zero_grad()#clear\n",
    "        loss, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "                                             torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[start_idx:end_idx]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "        #optimize\n",
    "        loss.backward()#retain_graph=True)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_id%50==1:\n",
    "            autoencoder.eval()\n",
    "            sample_num = 10\n",
    "            rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "            #teaching forcing\n",
    "            loss_, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            tokenized_sents=predicts.tolist()\n",
    "            real_sents=[]\n",
    "            label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "            label_real_sents=[]\n",
    "            for idx, sent in enumerate(tokenized_sents):\n",
    "                real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "            for sent in label_tokenized_sents:\n",
    "                label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "            print('train_set sample: ', rand_idx)\n",
    "            for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "                print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            #no teaching forcing\n",
    "            print('----no teaching forcing----')\n",
    "            predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=0, teaching_rate=1)\n",
    "            tokenized_sents=predicts.tolist()\n",
    "            real_sents=[]\n",
    "            label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "            label_real_sents=[]\n",
    "            for idx, sent in enumerate(tokenized_sents):\n",
    "                real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "            for sent in label_tokenized_sents:\n",
    "                label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "            for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "                print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            info_stamp = 'loss-{:2.9f}-batch_size-{:n}-epoch-{:n}-batch_id-({:n}/{:n})'.format(\n",
    "                              loss.data[0], batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "            print(info_stamp)\n",
    "            #valid_set testing\n",
    "            if batch_id%1000==1:\n",
    "                rand_idx=random.randint(0, len(valid_set_inputs)-batch_size-1-1)\n",
    "                predicts = autoencoder.forward(torch.LongTensor(valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "                                                 torch.LongTensor(valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "                                                 labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "                                                 is_train=0, teaching_rate=1)\n",
    "                tokenized_sents=predicts.tolist()\n",
    "                real_sents=[]\n",
    "                label_tokenized_sents=valid_set_labels[rand_idx:rand_idx+batch_size]\n",
    "                label_real_sents=[]\n",
    "                for idx, sent in enumerate(tokenized_sents):\n",
    "                    real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "                for sent in label_tokenized_sents:\n",
    "                    label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "                bleu_score, valid_num = data_set_bleu(label_real_sents, real_sents)\n",
    "                if valid_num>10:\n",
    "                    valid_bleu = bleu_score/valid_num\n",
    "                       \n",
    "                info_stamp = 'loss-{:2.9f}-bleu-{:1.4f}-hidden_dim-{:n}-input_dim-{:n}-epoch-{:n}-batch_size-{:n}-batch_id-[{:n}-[of]-{:n}]-lr-{:1.4f}'.format(\n",
    "                              loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "                print(valid_num, info_stamp)\n",
    "                now = int(round(time.time()*1000))\n",
    "                time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "                torch.save(autoencoder.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "                \n",
    "            autoencoder.train()\n",
    "            \n",
    "for epoch in range(epochs):\n",
    "    model_train(epoch, batch_size, train_set_size)\n",
    "    \n",
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "pre_train = torch.load('./models_better/loss-3.966628313-bleu-0.3201-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[1001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "print('a')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
