{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def tokenized_sent2real_sent(tokenized_sent, vocab):\n",
    "#     real_sent=[]\n",
    "#     for token in tokenized_sent:\n",
    "#         if token == vocab.word2token['<eos>']:\n",
    "#             break\n",
    "#         else:\n",
    "#             real_sent.append(vocab.token2word[token])\n",
    "#     return ''.join(real_sent)\n",
    "\n",
    "# def reverse_tokenized_sent2real_sent(tokenized_sent, vocab):\n",
    "#     real_sent=[]\n",
    "#     for token in tokenized_sent:\n",
    "#         if token == vocab.word2token['<eos>']:\n",
    "#             break\n",
    "#         else:\n",
    "#             real_sent.append(vocab.token2word[token])\n",
    "#     real_sent.reverse()\n",
    "#     return ''.join(real_sent)\n",
    "\n",
    "# def data_set_bleu(sents1, sents2):\n",
    "#     cnt=0\n",
    "#     bleu_score_sum=0\n",
    "    \n",
    "#     for sent1, sent2 in zip(sents1, sents2):\n",
    "#         if min(len(sent1), len(sent2))<4:\n",
    "#             pass\n",
    "#         else:\n",
    "#             cnt+=1\n",
    "#             bleu_score_sum = sentence_bleu([list(sent1)], list(sent2))+bleu_score_sum\n",
    "            \n",
    "#     return bleu_score_sum, cnt\n",
    "\n",
    "def batch_words2sentence(words_list):\n",
    "    return [''.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./small_data_set/train_set_inputs_10w.pk', 'rb') as f:\n",
    "    train_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/train_set_input_lens_10w.pk', 'rb') as f:\n",
    "    train_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/train_set_labels_10w.pk', 'rb') as f:\n",
    "    train_set_labels = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_inputs_10w.pk', 'rb') as f:\n",
    "    valid_set_inputs = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_input_lens_10w.pk', 'rb') as f:\n",
    "    valid_set_input_lens = pickle.load(f)\n",
    "with open('./small_data_set/valid_set_labels_10w.pk', 'rb') as f:\n",
    "    valid_set_labels = pickle.load(f)\n",
    "\n",
    "\n",
    "# with open('./data_set/train_set_inputs.pk', 'rb') as f:\n",
    "#     train_set_inputs = pickle.load(f)\n",
    "# with open('./data_set/train_set_input_lens.pk', 'rb') as f:\n",
    "#     train_set_input_lens = pickle.load(f)\n",
    "# with open('./data_set/train_set_labels.pk', 'rb') as f:\n",
    "#     train_set_labels = pickle.load(f)\n",
    "    \n",
    "# with open('./data_set/valid_set_inputs.pk', 'rb') as f:\n",
    "#     valid_set_inputs = pickle.load(f)\n",
    "# with open('./data_set/valid_set_input_lens.pk', 'rb') as f:\n",
    "#     valid_set_input_lens = pickle.load(f)\n",
    "# with open('./data_set/valid_set_labels.pk', 'rb') as f:\n",
    "#     valid_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num=100000\n",
    "# train_set_inputs_10w = train_set_inputs[:num]\n",
    "# train_set_input_lens_10w = train_set_input_lens[:num]\n",
    "# train_set_labels_10w = train_set_labels[:num]\n",
    "\n",
    "# valid_set_inputs_10w = valid_set_inputs[:num]\n",
    "# valid_set_input_lens_10w = valid_set_input_lens[:num]\n",
    "# valid_set_labels_10w = valid_set_labels[:num]\n",
    "\n",
    "\n",
    "# with open('./small_data_set/train_set_inputs_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_inputs_10w,f)\n",
    "# with open('./small_data_set/train_set_input_lens_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_input_lens_10w ,f)\n",
    "# with open('./small_data_set/train_set_labels_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_labels_10w ,f)\n",
    "    \n",
    "# with open('./small_data_set/valid_set_inputs_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_inputs_10w ,f)\n",
    "# with open('./small_data_set/valid_set_input_lens_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_input_lens_10w ,f)\n",
    "# with open('./small_data_set/valid_set_labels_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_labels_10w ,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_inputs), len(train_set_input_lens), len(train_set_labels))\n",
    "\n",
    "# for sent_len in valid_set_input_lens:\n",
    "#     if sent_len<=2:\n",
    "#         print('why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('pre_train_word_embedding.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "#         self.embed.weight.requires_grad = False\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "        #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "            cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "            cn = cn.index_select(0, Variable(true_order_ids))\n",
    "            \n",
    "        return outputs, (hn,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, use_cuda, encoder, hidden_dim, max_length=25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.input_dim = encoder.input_dim\n",
    "        self.max_length = max_length\n",
    "        self.vocab = encoder.vocab\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        #self.weight[self.vocab.word2token['<eos>']]=1.01\n",
    "        #self.weight[self.vocab.word2token['<split>']]=1.01\n",
    "        \n",
    "        self.lstmcell = torch.nn.LSTMCell(input_size=self.input_dim, hidden_size=self.hidden_dim*2, bias=True)\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=encoder.embed# reference share\n",
    "        #fcnn: projection for crossentroy loss\n",
    "        self.fcnn = nn.Linear(in_features = self.hidden_dim*2, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.cost_func = nn.CrossEntropyLoss(weight=torch.Tensor(self.weight), reduce=False)\n",
    "#         self.cost_func233 = nn.CrossEntropyLoss(weight=torch.Tensor(self.weight))\n",
    "        \n",
    "        self.log_softmax=nn.LogSoftmax(dim=1)\n",
    "#         self.nll_loss = nn.NLLLoss(reduce=False)\n",
    "\n",
    "        print('init lookup embedding matrix size: ', self.embed.weight.data.size())\n",
    "        \n",
    "#     def init_nll_loss(self):\n",
    "#         self.nll_loss = nn.NLLLoss(weight=torch.FloatTensor(self.weight), reduce=False)\n",
    "        \n",
    "    def forward1(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = 0\n",
    "        predicts = []\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "\n",
    "        for ii in range(self.max_length+1):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,0])\n",
    "                    all_loss+=loss\n",
    "                \n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #print('max_idxs size: ',max_idxs.size(), max_idxs)\n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "                    all_loss+=loss\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "                #print('max_idx size: ', max_idxs.size(), max_idxs)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "        \n",
    "        if is_train:  #training\n",
    "            if self.use_cuda:\n",
    "                return all_loss/(self.max_length+1), predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return all_loss/(self.max_length+1), predicts.data.numpy()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return predicts.data.numpy()\n",
    "    def forward(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = []\n",
    "        predicts = []\n",
    "        max_probs=[]\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "\n",
    "        for ii in range(self.max_length+1):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,0])\n",
    "                    t=torch.mean(loss)\n",
    "                    all_loss.append(torch.unsqueeze(loss, dim=1))\n",
    "                \n",
    "                #last_timestep_output = self.log_softmax(last_timestep_output)\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 max_prob = self.cost_func(last_timestep_output, labels[:,0])\n",
    "#                 max_probs.append(torch.unsqueeze(max_prob, dim=0))\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #print('max_prob size: ', max_prob.size())\n",
    "                #print('max_idxs size: ',max_idxs.size(), max_idxs)\n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "                    t=torch.mean(loss)\n",
    "                    all_loss.append(torch.unsqueeze(loss, dim=1))\n",
    "#                 _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 #print('max_idx size: ', max_idxs.size(), max_idxs)\n",
    "#                 predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #last_timestep_output = self.log_softmax(last_timestep_output)\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 max_prob = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "#                 max_probs.append(torch.unsqueeze(max_prob, dim=0))\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "#         max_probs = torch.cat(max_probs, dim=0)\n",
    "#         max_probs = torch.transpose(max_probs, 0, 1)\n",
    "#         print('max_probs size: ', max_probs.size())\n",
    "        \n",
    "#         sents_prob = torch.mean(max_probs, dim=1)\n",
    "#         print('sents_prob size: ', sents_prob.size())\n",
    "#         loss = torch.mean(sents_prob)\n",
    "#         loss=0\n",
    "#         for i in all_loss:\n",
    "#             loss+=i\n",
    "#         loss/=len(all_loss)\n",
    "        all_loss = torch.cat(all_loss, dim=1)\n",
    "        all_loss = torch.mean(all_loss, dim=1)\n",
    "        loss = torch.mean(all_loss)\n",
    "        #print('loss size: ', loss.size())\n",
    "        #torch.cuda.empty_cache()\n",
    "        if is_train:  #training\n",
    "            if self.use_cuda:\n",
    "                return loss, predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return loss, predicts.data.numpy()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().numpy()\n",
    "            else:\n",
    "                return predicts.data.numpy()\n",
    "#         if is_train:  #training\n",
    "#             if self.use_cuda:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.numpy()\n",
    "#         else:   #testing\n",
    "#             if self.use_cuda:\n",
    "#                 return predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return predicts.data.numpy()\n",
    "    def loss_func_rl(self, logits):\n",
    "        logprobs = self.log_softmax(logits)\n",
    "        max_logprobs, max_ids = torch.max(logprobs, dim=1)\n",
    "        weights = torch.FloatTensor(self.weight)\n",
    "        index = max_ids.data\n",
    "        if self.use_cuda:\n",
    "            index = index.cpu()\n",
    "        weights = Variable(torch.index_select(weights, 0, index), requires_grad=False)\n",
    "        if self.use_cuda:\n",
    "            weights = weights.cuda()\n",
    "        return -max_logprobs*weights\n",
    "    \n",
    "    def token_sample(self, logits, batch_size, topk=100, e_greedy_rate=0.2, dec_seq_num=10):\n",
    "        topk_logits, topk_idxs = torch.topk(logits, k=topk, dim=1)\n",
    "        if self.use_cuda:\n",
    "            ltopk_probs = self.softmax(topk_logits).data.cpu().tolist()\n",
    "            ltopk_idxs = topk_idxs.data.cpu().tolist()\n",
    "        else:\n",
    "            ltopk_probs = self.softmax(topk_logits).data.tolist()\n",
    "            ltopk_idxs = topk_idxs.data.tolist()\n",
    "        batch_results=[]\n",
    "        for probs, idxs in zip(ltopk_probs, ltopk_idxs):\n",
    "            sum_probs = sum(probs)\n",
    "            probs = [x/sum_probs for x in probs]\n",
    "            results=[]\n",
    "            for ii in range(dec_seq_num):\n",
    "                result = np.random.choice(idxs, p=probs)\n",
    "                if random.random()>e_greedy_rate:\n",
    "                    results.append(int(result))\n",
    "                else:\n",
    "                    results.append(random.randint(0, len(self.vocab.word2token)-1))\n",
    "            batch_results.append(results)\n",
    "        return batch_results\n",
    "    \n",
    "    def train_using_rl_2(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        #nll_loss = nn.NLLLoss(weight=torch.FloatTensor(self.weight).cuda(), reduce=False)\n",
    "        \n",
    "        # parameter labels must be python list type\n",
    "        labels_tokens=labels\n",
    "        labels = Variable(torch.LongTensor(labels))\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "        #print('rl...')\n",
    "        loss=0\n",
    "        all_loss = 0\n",
    "        losss=[]\n",
    "        predicts = []\n",
    "        max_probs=[]\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        topk=20\n",
    "        e_greedy_rate=0.3\n",
    "        dec_seq_num=100\n",
    "        vocab_size = len(self.vocab.word2token)\n",
    "        index_bias = torch.LongTensor([vocab_size*ii for ii in range(batch_size)]).unsqueeze(dim=1)\n",
    "        index_bias = index_bias.expand(batch_size, dec_seq_num).contiguous().view(-1)\n",
    "        if self.use_cuda:\n",
    "            index_bias = index_bias.cuda()\n",
    "        for ii in range(self.max_length+1):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "\n",
    "                batch_ltokens = self.token_sample(logits=last_timestep_output, batch_size=batch_size, topk=topk, e_greedy_rate=e_greedy_rate, dec_seq_num=dec_seq_num)\n",
    "                #print('batch_ltokens size: ', torch.LongTensor(batch_ltokens).size())\n",
    "                batch_ltokens=torch.LongTensor(batch_ltokens).view(-1)\n",
    "                if self.use_cuda:\n",
    "                    batch_ltokens = batch_ltokens.cuda()\n",
    "                predicts.append(batch_ltokens.view(-1,1))\n",
    "                if is_train:\n",
    "                    neg_log_prob = -self.log_softmax(last_timestep_output).view(-1)\n",
    "                    #print('batch_ltokens and index_bias size: ', batch_ltokens.size(), index_bias.size())\n",
    "                    indices = (batch_ltokens+index_bias)\n",
    "                    \n",
    "                    loss=neg_log_prob[indices]\n",
    "#                     loss = self.cost_func(last_timestep_output, pseudo_labels)#labels[:,ii])\n",
    "#                     losss.append(torch.unsqueeze(loss, dim=1))\n",
    "#                 if is_train:\n",
    "#                     loss = self.cost_func(last_timestep_output, max_idxs)#labels[:,0])\n",
    "#                     losss.append(torch.unsqueeze(loss, dim=1))\n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                \n",
    "                batch_ltokens = self.token_sample(logits=last_timestep_output, batch_size=batch_size, topk=topk, e_greedy_rate=e_greedy_rate, dec_seq_num=dec_seq_num)\n",
    "                batch_ltokens=torch.LongTensor(batch_ltokens).view(-1)\n",
    "                if self.use_cuda:\n",
    "                    batch_ltokens = batch_ltokens.cuda()\n",
    "                predicts.append(batch_ltokens.view(-1,1))\n",
    "                if is_train:\n",
    "                    neg_log_prob = -self.log_softmax(last_timestep_output).view(-1)\n",
    "                    indices = (batch_ltokens+index_bias)\n",
    "                    \n",
    "                    loss+=neg_log_prob[indices]\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            predicts = predicts.cpu().numpy()\n",
    "        else:\n",
    "            predicts = predicts.numpy()\n",
    "        \n",
    "        predicts_tokens=batch_tokens_remove_eos(predicts.tolist(), self.vocab)\n",
    "        labels_tokens=[x for x in labels_tokens for ii in range(dec_seq_num)]\n",
    "        labels_tokens=batch_tokens_remove_eos(labels_tokens, self.vocab)\n",
    "        bleu_scores = batch_tokens_bleu(references=labels_tokens, candidates=predicts_tokens, smooth_epsilon=0.01)\n",
    "#         predicts_words=batch_tokens2words(predicts_tokens, self.vocab)\n",
    "#         predicts_sents=batch_words2sentence(predicts_words)\n",
    "        labels_words = batch_tokens2words(labels_tokens, self.vocab)\n",
    "        labels_sents = batch_words2sentence(labels_words)\n",
    "#         print(len(labels_sents))\n",
    "#         for ii in labels_sents[:21]:\n",
    "#             print(ii)\n",
    "        \n",
    "        bleu_scores = torch.FloatTensor(bleu_scores).view(batch_size, dec_seq_num)\n",
    "        bleu_mean = torch.mean(bleu_scores, dim=1).unsqueeze(dim=1)\n",
    "        bleu_scores = bleu_scores-bleu_mean\n",
    "        bleu_scores = bleu_scores.view(-1)\n",
    "        \n",
    "        bleu_scores = Variable(bleu_scores, requires_grad = 0)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            bleu_scores = bleu_scores.cuda()\n",
    "            \n",
    "        loss = torch.dot(loss, bleu_scores)/batch_size/dec_seq_num\n",
    "        \n",
    "        return loss, predicts, torch.mean(bleu_mean.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _inflate(tensor, times, dim):\n",
    "    \"\"\"\n",
    "    Examples::\n",
    "        >> a = torch.LongTensor([[1, 2], [3, 4]])\n",
    "        >> a\n",
    "        1   2\n",
    "        3   4\n",
    "        [torch.LongTensor of size 2x2]\n",
    "        >> b = ._inflate(a, 2, dim=1)\n",
    "        >> b\n",
    "        1   2   1   2\n",
    "        3   4   3   4\n",
    "        [torch.LongTensor of size 2x4]\n",
    "    \"\"\"\n",
    "    repeat_dims = [1] * tensor.dim()\n",
    "    repeat_dims[dim] = times\n",
    "    return tensor.repeat(*repeat_dims)\n",
    "\n",
    "class TopKDecoder(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Top-K decoding with beam search.\n",
    "\n",
    "    Args:\n",
    "        decoder_rnn (DecoderRNN): An object of DecoderRNN used for decoding.\n",
    "        k (int): Size of the beam.\n",
    "\n",
    "    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n",
    "        - **inputs** (seq_len, batch, input_size): list of sequences, whose length is the batch size and within which\n",
    "          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default is `None`)\n",
    "        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features\n",
    "          in the hidden state `h` of encoder. Used as the initial hidden state of the decoder.\n",
    "        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n",
    "          Used for attention mechanism (default is `None`).\n",
    "        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n",
    "          (default is `torch.nn.functional.log_softmax`).\n",
    "        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n",
    "          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n",
    "          teacher forcing would be used (default is 0).\n",
    "\n",
    "    Outputs: decoder_outputs, decoder_hidden, ret_dict\n",
    "        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n",
    "          outputs of the decoder.\n",
    "        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n",
    "          state of the decoder.\n",
    "        - **ret_dict**: dictionary containing additional information as follows {*length* : list of integers\n",
    "          representing lengths of output sequences, *topk_length*: list of integers representing lengths of beam search\n",
    "          sequences, *sequence* : list of sequences, where each sequence is a list of predicted token IDs,\n",
    "          *topk_sequence* : list of beam search sequences, each beam is a list of token IDs, *inputs* : target\n",
    "          outputs if provided for decoding}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder_rnn, k):\n",
    "        super(TopKDecoder, self).__init__()\n",
    "        self.rnn = decoder_rnn\n",
    "        self.k = k\n",
    "        self.hidden_size = self.rnn.hidden_dim\n",
    "        self.V = len(self.rnn.vocab.word2token)\n",
    "        self.SOS = self.rnn.vocab.word2token['<sos>']\n",
    "        self.EOS = self.rnn.vocab.word2token['<eos>']\n",
    "        self.use_cuda = self.rnn.use_cuda\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var\n",
    "\n",
    "    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None, function=F.log_softmax,\n",
    "                    teacher_forcing_ratio=0, retain_output_probs=True):\n",
    "        \"\"\"\n",
    "        Forward rnn for MAX_LENGTH steps.  Look at :func:`seq2seq.models.DecoderRNN.DecoderRNN.forward_rnn` for details.\n",
    "        \"\"\"\n",
    "\n",
    "#         inputs, batch_size, max_length = self.rnn._validate_args(inputs, encoder_hidden, encoder_outputs,\n",
    "#                                                                  function, teacher_forcing_ratio)\n",
    "\n",
    "        batch_size = encoder_outputs.size(dim=0)\n",
    "        max_length = self.rnn.max_length+1\n",
    "        \n",
    "        self.pos_index = self._tocuda(Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1))\n",
    "\n",
    "        # Inflate the initial hidden states to be of size: b*k x h\n",
    "#         encoder_hidden = self.rnn._init_state(encoder_hidden)\n",
    "#         if encoder_hidden is None:\n",
    "#             hidden = None\n",
    "#         else:\n",
    "#             if isinstance(encoder_hidden, tuple):\n",
    "#                 hidden = tuple([_inflate(h, self.k, 1) for h in encoder_hidden])\n",
    "#             else:\n",
    "#                 hidden = _inflate(encoder_hidden, self.k, 1)\n",
    "\n",
    "        hidden = tuple([_inflate(h, self.k, 1).view(batch_size*self.k, -1) for h in encoder_hidden])\n",
    "        #print('hidden0 size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "        # ... same idea for encoder_outputs and decoder_outputs\n",
    "#         if self.rnn.use_attention:\n",
    "#             inflated_encoder_outputs = _inflate(encoder_outputs, self.k, 0)\n",
    "#         else:\n",
    "#             inflated_encoder_outputs = None\n",
    "\n",
    "        # Initialize the scores; for the first step,\n",
    "        # ignore the inflated copies to avoid duplicate entries in the top k\n",
    "        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n",
    "        sequence_scores.fill_(-float('Inf'))\n",
    "        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n",
    "        sequence_scores = self._tocuda(Variable(sequence_scores))\n",
    "\n",
    "        # Initialize the input vector\n",
    "        input_var = self._tocuda(Variable(torch.LongTensor([self.SOS] * batch_size * self.k)))\n",
    "\n",
    "        # Store decisions for backtracking\n",
    "        stored_outputs = list()\n",
    "        stored_scores = list()\n",
    "        stored_predecessors = list()\n",
    "        stored_emitted_symbols = list()\n",
    "        stored_hidden = list()\n",
    "\n",
    "        for ii in range(0, max_length):\n",
    "            # Run the RNN one step forward\n",
    "            #print('setp: %s'%ii)\n",
    "            input_vec = self.rnn.embed(input_var)\n",
    "            #print('input_var and input_vec size: ', input_var.size(), input_vec.size())\n",
    "            hidden = self.rnn.lstmcell(input_vec, hidden)\n",
    "            #print('hidden size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "            \n",
    "            log_softmax_output = self.log_softmax(self.rnn.fcnn(hidden[0]))\n",
    "            # If doing local backprop (e.g. supervised training), retain the output layer\n",
    "#             if retain_output_probs:\n",
    "#                 stored_outputs.append(log_softmax_output)\n",
    "\n",
    "            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n",
    "            sequence_scores = _inflate(sequence_scores, self.V, 1)\n",
    "            sequence_scores += log_softmax_output.squeeze(1)\n",
    "            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n",
    "\n",
    "            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n",
    "            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n",
    "            sequence_scores = scores.view(batch_size * self.k, 1)\n",
    "\n",
    "#             if ii ==0:\n",
    "#                 result0 = input_var.data.cpu().tolist()\n",
    "#                 print(result0)\n",
    "#                 result0=batch_tokens2words(result0, vocab)\n",
    "#                 print(result0)\n",
    "            # Update fields for next timestep\n",
    "            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple([h.index_select(0, predecessors.squeeze()) for h in hidden])\n",
    "            else:\n",
    "                hidden = hidden.index_select(0, predecessors.squeeze())\n",
    "\n",
    "            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n",
    "            stored_scores.append(sequence_scores.clone())\n",
    "            eos_indices = input_var.data.eq(self.EOS)\n",
    "            if eos_indices.nonzero().dim() > 0:\n",
    "                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n",
    "\n",
    "            # Cache results for backtracking\n",
    "            stored_predecessors.append(predecessors)\n",
    "            stored_emitted_symbols.append(input_var)\n",
    "            stored_hidden.append(hidden)\n",
    "\n",
    "        # Do backtracking to return the optimal values\n",
    "        output, h_t, h_n, s, l, p = self._backtrack(stored_hidden,\n",
    "                                                    stored_predecessors, stored_emitted_symbols,\n",
    "                                                    stored_scores, batch_size, self.hidden_size)\n",
    "\n",
    "        # Build return objects\n",
    "#         decoder_outputs = [step[:, 0, :] for step in output]\n",
    "        if isinstance(h_n, tuple):\n",
    "            decoder_hidden = tuple([h[:, :, 0, :] for h in h_n])\n",
    "        else:\n",
    "            decoder_hidden = h_n[:, :, 0, :]\n",
    "        metadata = {}\n",
    "        metadata['inputs'] = inputs\n",
    "        metadata['output'] = output\n",
    "        metadata['h_t'] = h_t\n",
    "        metadata['score'] = s\n",
    "        metadata['topk_length'] = l\n",
    "        metadata['topk_sequence'] = p\n",
    "        metadata['length'] = [seq_len[0] for seq_len in l]\n",
    "        metadata['sequence'] = [seq[0] for seq in p]\n",
    "        return decoder_hidden, metadata\n",
    "\n",
    "    def _backtrack(self, nw_hidden, predecessors, symbols, scores, b, hidden_size):\n",
    "        \"\"\"Backtracks over batch to generate optimal k-sequences.\n",
    "\n",
    "        Args:\n",
    "            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n",
    "            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n",
    "            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n",
    "            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n",
    "            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n",
    "            b: Size of the batch\n",
    "            hidden_size: Size of the hidden state\n",
    "\n",
    "        Returns:\n",
    "            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n",
    "\n",
    "            score [batch, k]: A list containing the final scores for all top-k sequences\n",
    "\n",
    "            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n",
    "\n",
    "            p (batch, k, sequence_len): A Tensor containing predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        lstm = isinstance(nw_hidden[0], tuple)\n",
    "\n",
    "        # initialize return variables given different types\n",
    "        output = list()\n",
    "        h_t = list()\n",
    "        p = list()\n",
    "        # Placeholder for last hidden state of top-k sequences.\n",
    "        # If a (top-k) sequence ends early in decoding, `h_n` contains\n",
    "        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n",
    "        # the last hidden state of decoding.\n",
    "        if lstm:\n",
    "            state_size = nw_hidden[0][0].size()\n",
    "            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n",
    "        else:\n",
    "            h_n = torch.zeros(nw_hidden[0].size())\n",
    "        l = [[self.rnn.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n",
    "                                                                # Similar to `h_n`\n",
    "\n",
    "        # the last step output of the beams are not sorted\n",
    "        # thus they are sorted here\n",
    "        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n",
    "        # initialize the sequence scores with the sorted last step beam scores\n",
    "        s = sorted_score.clone()\n",
    "\n",
    "        batch_eos_found = [0] * b   # the number of EOS found\n",
    "                                    # in the backward loop below for each batch\n",
    "\n",
    "        t = self.rnn.max_length - 1\n",
    "        # initialize the back pointer with the sorted order of the last step beams.\n",
    "        # add self.pos_index for indexing variable with b*k as the first dimension.\n",
    "        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n",
    "        while t >= 0:\n",
    "            # Re-order the variables with the back pointer\n",
    "#             current_output = nw_output[t].index_select(0, t_predecessors)\n",
    "            if lstm:\n",
    "                current_hidden = tuple([h.index_select(0, t_predecessors) for h in nw_hidden[t]])\n",
    "            else:\n",
    "                current_hidden = nw_hidden[t].index_select(0, t_predecessors)\n",
    "            current_symbol = symbols[t].index_select(0, t_predecessors)\n",
    "            # Re-order the back pointer of the previous step with the back pointer of\n",
    "            # the current step\n",
    "            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n",
    "\n",
    "            # This tricky block handles dropped sequences that see EOS earlier.\n",
    "            # The basic idea is summarized below:\n",
    "            #\n",
    "            #   Terms:\n",
    "            #       Ended sequences = sequences that see EOS early and dropped\n",
    "            #       Survived sequences = sequences in the last step of the beams\n",
    "            #\n",
    "            #       Although the ended sequences are dropped during decoding,\n",
    "            #   their generated symbols and complete backtracking information are still\n",
    "            #   in the backtracking variables.\n",
    "            #   For each batch, everytime we see an EOS in the backtracking process,\n",
    "            #       1. If there is survived sequences in the return variables, replace\n",
    "            #       the one with the lowest survived sequence score with the new ended\n",
    "            #       sequences\n",
    "            #       2. Otherwise, replace the ended sequence with the lowest sequence\n",
    "            #       score with the new ended sequence\n",
    "            #\n",
    "            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n",
    "            if eos_indices.dim() > 0:\n",
    "                for i in range(eos_indices.size(0)-1, -1, -1):\n",
    "                    # Indices of the EOS symbol for both variables\n",
    "                    # with b*k as the first dimension, and b, k for\n",
    "                    # the first two dimensions\n",
    "                    idx = eos_indices[i]\n",
    "                    b_idx = int(idx[0] / self.k)\n",
    "                    # The indices of the replacing position\n",
    "                    # according to the replacement strategy noted above\n",
    "                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n",
    "                    batch_eos_found[b_idx] += 1\n",
    "                    res_idx = b_idx * self.k + res_k_idx\n",
    "\n",
    "                    # Replace the old information in return variables\n",
    "                    # with the new ended sequence information\n",
    "                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n",
    "#                     current_output[res_idx, :] = nw_output[t][idx[0], :]\n",
    "                    if lstm:\n",
    "                        current_hidden[0][res_idx, :] = nw_hidden[t][0][idx[0], :]\n",
    "                        current_hidden[1][res_idx, :] = nw_hidden[t][1][idx[0], :]\n",
    "                        h_n[0][res_idx, :] = nw_hidden[t][0][idx[0], :].data\n",
    "                        h_n[1][res_idx, :] = nw_hidden[t][1][idx[0], :].data\n",
    "                    else:\n",
    "                        current_hidden[res_idx, :] = nw_hidden[t][idx[0], :]\n",
    "                        h_n[res_idx, :] = nw_hidden[t][idx[0], :].data\n",
    "                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n",
    "                    s[b_idx, res_k_idx] = scores[t][idx[0]].data[0]\n",
    "                    l[b_idx][res_k_idx] = t + 1\n",
    "\n",
    "            # record the back tracked results\n",
    "#             output.append(current_output)\n",
    "            h_t.append(current_hidden)\n",
    "            p.append(current_symbol)\n",
    "\n",
    "            t -= 1\n",
    "\n",
    "        # Sort and re-order again as the added ended sequences may change\n",
    "        # the order (very unlikely)\n",
    "        s, re_sorted_idx = s.topk(self.k)\n",
    "        for b_idx in range(b):\n",
    "            l[b_idx] = [l[b_idx][k_idx.data[0]] for k_idx in re_sorted_idx[b_idx,:]]\n",
    "\n",
    "        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n",
    "\n",
    "        # Reverse the sequences and re-order at the same time\n",
    "        # It is reversed because the backtracking happens in reverse time order\n",
    "#         output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n",
    "        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n",
    "        if lstm:\n",
    "            h_t = [tuple([h.index_select(0, re_sorted_idx).view(-1, b, self.k, hidden_size) for h in step]) for step in reversed(h_t)]\n",
    "            h_n = tuple([self._tocuda(x) for x in h_n])\n",
    "            h_n = tuple([h.index_select(0, re_sorted_idx.data).view(-1, b, self.k, hidden_size) for h in h_n])\n",
    "        else:\n",
    "            h_t = [step.index_select(0, re_sorted_idx).view(-1, b, self.k, hidden_size) for step in reversed(h_t)]\n",
    "            h_n = h_n.index_select(0, re_sorted_idx.data).view(-1, b, self.k, hidden_size)\n",
    "        s = s.data\n",
    "\n",
    "        #    --- fake output ---\n",
    "        output = None\n",
    "        #    --- fake ---\n",
    "        return output, h_t, h_n, s, l, p\n",
    "\n",
    "    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n",
    "            score[idx] = masking_score\n",
    "\n",
    "    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n",
    "        if len(idx.size()) > 0:\n",
    "            indices = idx[:, 0]\n",
    "            tensor.index_fill_(dim, indices, masking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, use_cuda, input_dim, hidden_dim, vocab, max_length = 25):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.enc = Encoder(use_cuda=use_cuda, hidden_dim=hidden_dim, input_dim=input_dim, vocab=vocab)\n",
    "        self.dec = Decoder(use_cuda=use_cuda, encoder=self.enc, hidden_dim=hidden_dim, max_length=max_length)\n",
    "        if use_cuda:\n",
    "            self.enc = self.enc.cuda()\n",
    "            self.dec = self.dec.cuda()\n",
    "    def forward(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        if is_train:\n",
    "            loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                    h0_and_c0=(enc_hn, enc_cn), \n",
    "                                    sent_lens=input_lens,\n",
    "                                    labels=torch.LongTensor(labels), \n",
    "                                    is_train=1, \n",
    "                                    teaching_rate = 1\n",
    "                                    )\n",
    "            return loss, predicts\n",
    "        else:\n",
    "            predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                sent_lens=input_lens,\n",
    "                                labels=torch.LongTensor(labels), \n",
    "                                is_train=0, \n",
    "                                teaching_rate = 1\n",
    "                                )\n",
    "            return predicts\n",
    "    def train_using_rl(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        loss, predicts, bleu_mean = self.dec.train_using_rl_2(enc_outputs = enc_outputs, \n",
    "                                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                                sent_lens=input_lens,\n",
    "                                                labels=labels,\n",
    "                                                is_train=1, \n",
    "                                                teaching_rate = 1\n",
    "                                                )\n",
    "        return loss, predicts, bleu_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use_cuda = 1\n",
    "# hidden_dim = 512\n",
    "# input_dim = 300\n",
    "\n",
    "# enc = Encoder(use_cuda=use_cuda, \n",
    "#             hidden_dim=hidden_dim, \n",
    "#             input_dim=input_dim, \n",
    "#             vocab=vocab\n",
    "#            )\n",
    "# if use_cuda:\n",
    "#     enc = enc.cuda()\n",
    "    \n",
    "# sample_num = 11\n",
    "# print('sentences length: ', train_set_input_lens[0:sample_num])\n",
    "\n",
    "# # enc_outputs, (enc_hn, enc_cn) = enc(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "# #                                     torch.LongTensor(train_set_input_lens[0:sample_num]))\n",
    "# # print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "# # dec = Decoder(use_cuda=use_cuda, encoder=enc, hidden_dim=hidden_dim, max_length=25)\n",
    "# # if use_cuda:\n",
    "# #     dec = dec.cuda()\n",
    "    \n",
    "# # loss, predicts = dec(enc_outputs = enc_outputs, \n",
    "# #                     h0_and_c0=(enc_hn, enc_cn), \n",
    "# #                     sent_lens=train_set_input_lens[0:sample_num], \n",
    "# #                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "# #                     is_train=1, teaching_rate = 1\n",
    "# #                     )\n",
    "# # print('loss is %4.7f'%loss.data[0])\n",
    "\n",
    "# # autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "# # a=time.time()\n",
    "# # loss, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "# #                                      torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "# #                                      labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "# #                                      is_train=1, teaching_rate=1)\n",
    "# # print('autocoder: loss is %4.7f'%loss.data[0])\n",
    "# # print(time.time()-a)\n",
    "\n",
    "# autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 25)\n",
    "# a=time.time()\n",
    "# loss, predicts, bleu_reward = autoencoder.train_using_rl(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "#                                      torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "#                                      labels=train_set_labels[0:sample_num], \n",
    "#                                      is_train=1, teaching_rate=1)\n",
    "# print('autocoder: loss is %4.7f'%loss.data[0], bleu_reward)\n",
    "# print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([98638, 300])\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000002674-train_reward=0.000013242-batch_size-18-epoch-0-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000000000-train_reward=0.000000000-batch_size-18-epoch-1-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000005978-train_reward=0.000019863-batch_size-18-epoch-2-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss-0.000003671-train_reward=0.000139041-batch_size-18-epoch-3-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000089707-train_reward=0.000960048-batch_size-18-epoch-4-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.000358553-train_reward=0.001357309-batch_size-18-epoch-5-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000098872-train_reward=0.001668497-batch_size-18-epoch-6-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.000459223-train_reward=0.001867128-batch_size-18-epoch-7-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss-0.000109833-train_reward=0.001980938-batch_size-18-epoch-8-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000050039-train_reward=0.001992927-batch_size-18-epoch-9-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss-0.000392523-train_reward=0.001661876-batch_size-18-epoch-10-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000246291-train_reward=0.002014043-batch_size-18-epoch-11-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000225095-train_reward=0.001999548-batch_size-18-epoch-12-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.001361213-train_reward=0.001820781-batch_size-18-epoch-13-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000711754-train_reward=0.002146464-batch_size-18-epoch-14-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000208154-train_reward=0.002608682-batch_size-18-epoch-15-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000066479-train_reward=0.003204574-batch_size-18-epoch-16-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001215698-train_reward=0.003178090-batch_size-18-epoch-17-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000745018-train_reward=0.002919870-batch_size-18-epoch-18-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000592686-train_reward=0.003042807-batch_size-18-epoch-19-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.000451142-train_reward=0.003285279-batch_size-18-epoch-20-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000000000-train_reward=0.003310511-batch_size-18-epoch-21-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.000323518-train_reward=0.003244300-batch_size-18-epoch-22-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.000166212-train_reward=0.003027059-batch_size-18-epoch-23-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000095831-train_reward=0.003053543-batch_size-18-epoch-24-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000635316-train_reward=0.003264163-batch_size-18-epoch-25-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.001273704-train_reward=0.003544394-batch_size-18-epoch-26-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000984261-train_reward=0.003565153-batch_size-18-epoch-27-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.002153066-train_reward=0.004035607-batch_size-18-epoch-28-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss-0.003597663-train_reward=0.005117312-batch_size-18-epoch-29-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004062707-train_reward=0.008306507-batch_size-18-epoch-30-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.000465159-train_reward=0.009140648-batch_size-18-epoch-31-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000732396-train_reward=0.008603265-batch_size-18-epoch-32-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001761191-train_reward=0.008238742-batch_size-18-epoch-33-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000720773-train_reward=0.008027046-batch_size-18-epoch-34-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001405194-train_reward=0.008042613-batch_size-18-epoch-35-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss-0.000120756-train_reward=0.008147477-batch_size-18-epoch-36-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.000008993-train_reward=0.008251340-batch_size-18-epoch-37-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000526631-train_reward=0.008386678-batch_size-18-epoch-38-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000547970-train_reward=0.008443818-batch_size-18-epoch-39-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002089998-train_reward=0.008350228-batch_size-18-epoch-40-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.002067450-train_reward=0.008363827-batch_size-18-epoch-41-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000644444-train_reward=0.008365936-batch_size-18-epoch-42-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000725627-train_reward=0.008418048-batch_size-18-epoch-43-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000561827-train_reward=0.008405878-batch_size-18-epoch-44-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000328740-train_reward=0.008413394-batch_size-18-epoch-45-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001305915-train_reward=0.008395142-batch_size-18-epoch-46-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000862777-train_reward=0.008408026-batch_size-18-epoch-47-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000453258-train_reward=0.008433026-batch_size-18-epoch-48-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000573790-train_reward=0.008420196-batch_size-18-epoch-49-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.001004093-train_reward=0.008434691-batch_size-18-epoch-50-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.000241060-train_reward=0.008438449-batch_size-18-epoch-51-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.000658123-train_reward=0.008452574-batch_size-18-epoch-52-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001663085-train_reward=0.008426817-batch_size-18-epoch-53-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000466722-train_reward=0.008432186-batch_size-18-epoch-54-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001748291-train_reward=0.008494065-batch_size-18-epoch-55-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000802974-train_reward=0.008555480-batch_size-18-epoch-56-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002473554-train_reward=0.008900130-batch_size-18-epoch-57-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001039827-train_reward=0.009055010-batch_size-18-epoch-58-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001087549-train_reward=0.008983693-batch_size-18-epoch-59-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.005090258-train_reward=0.008928577-batch_size-18-epoch-60-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000181348-train_reward=0.009091166-batch_size-18-epoch-61-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001319266-train_reward=0.009084391-batch_size-18-epoch-62-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001026480-train_reward=0.008987994-batch_size-18-epoch-63-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001908444-train_reward=0.008956142-batch_size-18-epoch-64-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002332223-train_reward=0.009045078-batch_size-18-epoch-65-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000968018-train_reward=0.009100551-batch_size-18-epoch-66-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000190873-train_reward=0.009123292-batch_size-18-epoch-67-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004391457-train_reward=0.009077101-batch_size-18-epoch-68-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.001502836-train_reward=0.009465995-batch_size-18-epoch-69-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005722701-train_reward=0.010585030-batch_size-18-epoch-70-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.006170094-train_reward=0.011571005-batch_size-18-epoch-71-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.009602826-train_reward=0.010725092-batch_size-18-epoch-72-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.014061137-train_reward=0.010871931-batch_size-18-epoch-73-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004861828-train_reward=0.011185270-batch_size-18-epoch-74-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002161741-train_reward=0.011274558-batch_size-18-epoch-75-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.007506805-train_reward=0.011276547-batch_size-18-epoch-76-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.008766633-train_reward=0.011303807-batch_size-18-epoch-77-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.008193762-train_reward=0.011473079-batch_size-18-epoch-78-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008521328-train_reward=0.011492535-batch_size-18-epoch-79-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006987968-train_reward=0.011174094-batch_size-18-epoch-80-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.014808839-train_reward=0.011350297-batch_size-18-epoch-81-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006576181-train_reward=0.011619610-batch_size-18-epoch-82-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007842470-train_reward=0.011594230-batch_size-18-epoch-83-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011797038-train_reward=0.011534395-batch_size-18-epoch-84-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004770325-train_reward=0.011456733-batch_size-18-epoch-85-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.007865815-train_reward=0.011526521-batch_size-18-epoch-86-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000516527-train_reward=0.011601763-batch_size-18-epoch-87-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010975459-train_reward=0.011563430-batch_size-18-epoch-88-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006335810-train_reward=0.011509611-batch_size-18-epoch-89-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.007926100-train_reward=0.011500938-batch_size-18-epoch-90-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006509383-train_reward=0.011600967-batch_size-18-epoch-91-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.010467967-train_reward=0.011500026-batch_size-18-epoch-92-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003495618-train_reward=0.011484988-batch_size-18-epoch-93-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.006256071-train_reward=0.011400402-batch_size-18-epoch-94-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010239166-train_reward=0.011494575-batch_size-18-epoch-95-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006997695-train_reward=0.011588683-batch_size-18-epoch-96-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007881260-train_reward=0.011566677-batch_size-18-epoch-97-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.012103432-train_reward=0.011542377-batch_size-18-epoch-98-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011667228-train_reward=0.011511786-batch_size-18-epoch-99-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.013295989-train_reward=0.011572263-batch_size-18-epoch-100-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002413583-train_reward=0.011519353-batch_size-18-epoch-101-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.005033303-train_reward=0.011635166-batch_size-18-epoch-102-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.005822915-train_reward=0.011597592-batch_size-18-epoch-103-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.004789728-train_reward=0.011632263-batch_size-18-epoch-104-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007726151-train_reward=0.011624736-batch_size-18-epoch-105-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005211773-train_reward=0.011646806-batch_size-18-epoch-106-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005523490-train_reward=0.011665234-batch_size-18-epoch-107-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.007973347-train_reward=0.011633269-batch_size-18-epoch-108-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001851667-train_reward=0.011651887-batch_size-18-epoch-109-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.003556772-train_reward=0.011627248-batch_size-18-epoch-110-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007763292-train_reward=0.011629088-batch_size-18-epoch-111-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.005295756-train_reward=0.011682875-batch_size-18-epoch-112-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006874104-train_reward=0.011636514-batch_size-18-epoch-113-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004668142-train_reward=0.011645163-batch_size-18-epoch-114-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.002258650-train_reward=0.011643013-batch_size-18-epoch-115-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003166854-train_reward=0.011671504-batch_size-18-epoch-116-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.015121473-train_reward=0.011489709-batch_size-18-epoch-117-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005258391-train_reward=0.011619977-batch_size-18-epoch-118-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003944713-train_reward=0.011681742-batch_size-18-epoch-119-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006043929-train_reward=0.011673752-batch_size-18-epoch-120-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001796914-train_reward=0.011657696-batch_size-18-epoch-121-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.013576137-train_reward=0.011609893-batch_size-18-epoch-122-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007039188-train_reward=0.011663431-batch_size-18-epoch-123-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006843142-train_reward=0.011671469-batch_size-18-epoch-124-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008964289-train_reward=0.011650159-batch_size-18-epoch-125-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.014979880-train_reward=0.011603159-batch_size-18-epoch-126-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004838291-train_reward=0.011668920-batch_size-18-epoch-127-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011433453-train_reward=0.011674595-batch_size-18-epoch-128-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003748014-train_reward=0.011690730-batch_size-18-epoch-129-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002965371-train_reward=0.011680435-batch_size-18-epoch-130-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001388014-train_reward=0.011672510-batch_size-18-epoch-131-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002895475-train_reward=0.011647944-batch_size-18-epoch-132-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000564177-train_reward=0.011692817-batch_size-18-epoch-133-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002046632-train_reward=0.011688308-batch_size-18-epoch-134-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001121499-train_reward=0.011696292-batch_size-18-epoch-135-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003384941-train_reward=0.011646521-batch_size-18-epoch-136-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008409812-train_reward=0.011653309-batch_size-18-epoch-137-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004660922-train_reward=0.011652199-batch_size-18-epoch-138-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.005486205-train_reward=0.011681216-batch_size-18-epoch-139-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002802844-train_reward=0.011683071-batch_size-18-epoch-140-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.003113441-train_reward=0.011701212-batch_size-18-epoch-141-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009252178-train_reward=0.011631576-batch_size-18-epoch-142-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.013636637-train_reward=0.011645996-batch_size-18-epoch-143-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000111566-train_reward=0.011715364-batch_size-18-epoch-144-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.008870757-train_reward=0.011679424-batch_size-18-epoch-145-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004809982-train_reward=0.011690533-batch_size-18-epoch-146-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010906897-train_reward=0.011673211-batch_size-18-epoch-147-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005354596-train_reward=0.011689771-batch_size-18-epoch-148-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000073652-train_reward=0.011705137-batch_size-18-epoch-149-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.006491906-train_reward=0.011677291-batch_size-18-epoch-150-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007830583-train_reward=0.011671042-batch_size-18-epoch-151-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002397395-train_reward=0.011698776-batch_size-18-epoch-152-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005762501-train_reward=0.011693575-batch_size-18-epoch-153-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004593748-train_reward=0.011698736-batch_size-18-epoch-154-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.007935873-train_reward=0.011649291-batch_size-18-epoch-155-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000162383-train_reward=0.011712299-batch_size-18-epoch-156-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007026700-train_reward=0.011669245-batch_size-18-epoch-157-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006133283-train_reward=0.011695971-batch_size-18-epoch-158-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.000003769-train_reward=0.011712018-batch_size-18-epoch-159-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003606760-train_reward=0.011683294-batch_size-18-epoch-160-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004736667-train_reward=0.011675501-batch_size-18-epoch-161-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007522475-train_reward=0.011686931-batch_size-18-epoch-162-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.003797521-train_reward=0.011693988-batch_size-18-epoch-163-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009937505-train_reward=0.011670808-batch_size-18-epoch-164-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003800238-train_reward=0.011652156-batch_size-18-epoch-165-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001471402-train_reward=0.011696742-batch_size-18-epoch-166-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.005105140-train_reward=0.011686694-batch_size-18-epoch-167-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007520079-train_reward=0.011683334-batch_size-18-epoch-168-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007365953-train_reward=0.011621110-batch_size-18-epoch-169-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001274570-train_reward=0.011692198-batch_size-18-epoch-170-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.002572497-train_reward=0.011712518-batch_size-18-epoch-171-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.005382682-train_reward=0.011680489-batch_size-18-epoch-172-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010978045-train_reward=0.011662942-batch_size-18-epoch-173-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.000781377-train_reward=0.011691257-batch_size-18-epoch-174-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.000044984-train_reward=0.011704151-batch_size-18-epoch-175-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010715154-train_reward=0.011683422-batch_size-18-epoch-176-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007256617-train_reward=0.011662451-batch_size-18-epoch-177-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005363053-train_reward=0.011655195-batch_size-18-epoch-178-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007178407-train_reward=0.011683677-batch_size-18-epoch-179-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004976924-train_reward=0.011708743-batch_size-18-epoch-180-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006660428-train_reward=0.011698797-batch_size-18-epoch-181-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003883482-train_reward=0.011685420-batch_size-18-epoch-182-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007385169-train_reward=0.011692798-batch_size-18-epoch-183-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.004936082-train_reward=0.011694269-batch_size-18-epoch-184-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.003399369-train_reward=0.011697373-batch_size-18-epoch-185-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005778034-train_reward=0.011697596-batch_size-18-epoch-186-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011564222-train_reward=0.011675786-batch_size-18-epoch-187-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.002956695-train_reward=0.011698023-batch_size-18-epoch-188-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.000000189-train_reward=0.011714524-batch_size-18-epoch-189-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004234697-train_reward=0.011708873-batch_size-18-epoch-190-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000830704-train_reward=0.011705247-batch_size-18-epoch-191-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004693071-train_reward=0.011691333-batch_size-18-epoch-192-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005920943-train_reward=0.011690229-batch_size-18-epoch-193-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006756104-train_reward=0.011691333-batch_size-18-epoch-194-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.003807541-train_reward=0.011700609-batch_size-18-epoch-195-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008804542-train_reward=0.011695804-batch_size-18-epoch-196-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.009982460-train_reward=0.011673035-batch_size-18-epoch-197-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.020747125-train_reward=0.011639878-batch_size-18-epoch-198-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.008514901-train_reward=0.011691333-batch_size-18-epoch-199-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.002396742-train_reward=0.011705247-batch_size-18-epoch-200-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.000618099-train_reward=0.011703133-batch_size-18-epoch-201-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011992214-train_reward=0.011651817-batch_size-18-epoch-202-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008189654-train_reward=0.011696858-batch_size-18-epoch-203-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002222949-train_reward=0.011706127-batch_size-18-epoch-204-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002077776-train_reward=0.011702892-batch_size-18-epoch-205-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.002755865-train_reward=0.011688770-batch_size-18-epoch-206-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005562770-train_reward=0.011718274-batch_size-18-epoch-207-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001930763-train_reward=0.011706087-batch_size-18-epoch-208-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010601072-train_reward=0.011675752-batch_size-18-epoch-209-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002710841-train_reward=0.011703304-batch_size-18-epoch-210-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005564383-train_reward=0.011701189-batch_size-18-epoch-211-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001891934-train_reward=0.011683222-batch_size-18-epoch-212-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003173022-train_reward=0.011704407-batch_size-18-epoch-213-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000968081-train_reward=0.011693988-batch_size-18-epoch-214-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.009782309-train_reward=0.011677510-batch_size-18-epoch-215-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011911133-train_reward=0.011659121-batch_size-18-epoch-216-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002613557-train_reward=0.011704060-batch_size-18-epoch-217-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006031009-train_reward=0.011701005-batch_size-18-epoch-218-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001726384-train_reward=0.011710678-batch_size-18-epoch-219-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004693371-train_reward=0.011688156-batch_size-18-epoch-220-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005607219-train_reward=0.011687403-batch_size-18-epoch-221-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004196295-train_reward=0.011700763-batch_size-18-epoch-222-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008863340-train_reward=0.011685464-batch_size-18-epoch-223-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002289829-train_reward=0.011710897-batch_size-18-epoch-224-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008008139-train_reward=0.011716422-batch_size-18-epoch-225-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007825020-train_reward=0.011692845-batch_size-18-epoch-226-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002050792-train_reward=0.011668252-batch_size-18-epoch-227-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.004056924-train_reward=0.011698652-batch_size-18-epoch-228-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.012162964-train_reward=0.011682509-batch_size-18-epoch-229-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001010009-train_reward=0.011703264-batch_size-18-epoch-230-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000142602-train_reward=0.011709885-batch_size-18-epoch-231-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003807818-train_reward=0.011712708-batch_size-18-epoch-232-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002299338-train_reward=0.011704145-batch_size-18-epoch-233-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.000097981-train_reward=0.011717346-batch_size-18-epoch-234-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001035263-train_reward=0.011710689-batch_size-18-epoch-235-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.001214101-train_reward=0.011721145-batch_size-18-epoch-236-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002111895-train_reward=0.011705700-batch_size-18-epoch-237-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008672222-train_reward=0.011692308-batch_size-18-epoch-238-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004794404-train_reward=0.011696011-batch_size-18-epoch-239-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.008293889-train_reward=0.011705473-batch_size-18-epoch-240-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005469976-train_reward=0.011719742-batch_size-18-epoch-241-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000816807-train_reward=0.011709885-batch_size-18-epoch-242-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.014285480-train_reward=0.011684712-batch_size-18-epoch-243-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002191441-train_reward=0.011701040-batch_size-18-epoch-244-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003023334-train_reward=0.011700609-batch_size-18-epoch-245-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.002458928-train_reward=0.011709885-batch_size-18-epoch-246-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003367105-train_reward=0.011705897-batch_size-18-epoch-247-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007858327-train_reward=0.011694638-batch_size-18-epoch-248-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.000325666-train_reward=0.011698419-batch_size-18-epoch-249-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006937406-train_reward=0.011703092-batch_size-18-epoch-250-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002905954-train_reward=0.011708442-batch_size-18-epoch-251-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002926001-train_reward=0.011685164-batch_size-18-epoch-252-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.007752494-train_reward=0.011697651-batch_size-18-epoch-253-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000000244-train_reward=0.011714524-batch_size-18-epoch-254-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.008162752-train_reward=0.011693732-batch_size-18-epoch-255-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009895257-train_reward=0.011700287-batch_size-18-epoch-256-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005262232-train_reward=0.011690059-batch_size-18-epoch-257-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.005098079-train_reward=0.011708201-batch_size-18-epoch-258-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss-0.000137525-train_reward=0.011705624-batch_size-18-epoch-259-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.007161550-train_reward=0.011694718-batch_size-18-epoch-260-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006296780-train_reward=0.011710918-batch_size-18-epoch-261-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.004271385-train_reward=0.011703994-batch_size-18-epoch-262-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.012833006-train_reward=0.011685854-batch_size-18-epoch-263-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006551904-train_reward=0.011701201-batch_size-18-epoch-264-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011709712-train_reward=0.011666071-batch_size-18-epoch-265-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005265701-train_reward=0.011702615-batch_size-18-epoch-266-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008340266-train_reward=0.011709885-batch_size-18-epoch-267-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004003922-train_reward=0.011696815-batch_size-18-epoch-268-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003705169-train_reward=0.011695229-batch_size-18-epoch-269-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001888821-train_reward=0.011710413-batch_size-18-epoch-270-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.010737604-train_reward=0.011688017-batch_size-18-epoch-271-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004495653-train_reward=0.011715494-batch_size-18-epoch-272-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.014404035-train_reward=0.011692735-batch_size-18-epoch-273-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.006590839-train_reward=0.011703264-batch_size-18-epoch-274-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009792651-train_reward=0.011697483-batch_size-18-epoch-275-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.006232354-train_reward=0.011678894-batch_size-18-epoch-276-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000393502-train_reward=0.011710725-batch_size-18-epoch-277-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005288383-train_reward=0.011695971-batch_size-18-epoch-278-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.004612626-train_reward=0.011700609-batch_size-18-epoch-279-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011017716-train_reward=0.011699429-batch_size-18-epoch-280-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007672408-train_reward=0.011702847-batch_size-18-epoch-281-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.004101795-train_reward=0.011698103-batch_size-18-epoch-282-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000278427-train_reward=0.011703264-batch_size-18-epoch-283-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002390611-train_reward=0.011709885-batch_size-18-epoch-284-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007383193-train_reward=0.011713121-batch_size-18-epoch-285-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004352476-train_reward=0.011707902-batch_size-18-epoch-286-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004315840-train_reward=0.011708633-batch_size-18-epoch-287-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.010332533-train_reward=0.011701102-batch_size-18-epoch-288-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.004252660-train_reward=0.011706897-batch_size-18-epoch-289-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000215879-train_reward=0.011696121-batch_size-18-epoch-290-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.012088290-train_reward=0.011681779-batch_size-18-epoch-291-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005107013-train_reward=0.011701862-batch_size-18-epoch-292-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.004429823-train_reward=0.011708766-batch_size-18-epoch-293-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.011171469-train_reward=0.011691056-batch_size-18-epoch-294-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005185599-train_reward=0.011696375-batch_size-18-epoch-295-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.012224277-train_reward=0.011688770-batch_size-18-epoch-296-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005234199-train_reward=0.011701796-batch_size-18-epoch-297-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004400490-train_reward=0.011697896-batch_size-18-epoch-298-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005804463-train_reward=0.011700649-batch_size-18-epoch-299-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.009244101-train_reward=0.011693878-batch_size-18-epoch-300-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010363060-train_reward=0.011696081-batch_size-18-epoch-301-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002657726-train_reward=0.011709473-batch_size-18-epoch-302-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002635980-train_reward=0.011700609-batch_size-18-epoch-303-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009890145-train_reward=0.011695971-batch_size-18-epoch-304-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.009022883-train_reward=0.011680681-batch_size-18-epoch-305-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.001177085-train_reward=0.011711868-batch_size-18-epoch-306-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005894651-train_reward=0.011702654-batch_size-18-epoch-307-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.013486199-train_reward=0.011689931-batch_size-18-epoch-308-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010757480-train_reward=0.011700609-batch_size-18-epoch-309-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.010671258-train_reward=0.011695310-batch_size-18-epoch-310-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.003363314-train_reward=0.011704407-batch_size-18-epoch-311-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010284002-train_reward=0.011661016-batch_size-18-epoch-312-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005087868-train_reward=0.011699079-batch_size-18-epoch-313-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000634468-train_reward=0.011721145-batch_size-18-epoch-314-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008663679-train_reward=0.011711656-batch_size-18-epoch-315-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009599090-train_reward=0.011689970-batch_size-18-epoch-316-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011389822-train_reward=0.011701710-batch_size-18-epoch-317-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.015295655-train_reward=0.011676861-batch_size-18-epoch-318-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.000296538-train_reward=0.011683814-batch_size-18-epoch-319-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006007345-train_reward=0.011705247-batch_size-18-epoch-320-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010031843-train_reward=0.011702289-batch_size-18-epoch-321-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009465402-train_reward=0.011692845-batch_size-18-epoch-322-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.011902013-train_reward=0.011681472-batch_size-18-epoch-323-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.024654882-train_reward=0.011679201-batch_size-18-epoch-324-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006962782-train_reward=0.011703911-batch_size-18-epoch-325-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009373303-train_reward=0.011699466-batch_size-18-epoch-326-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002337770-train_reward=0.011710977-batch_size-18-epoch-327-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000000315-train_reward=0.011714524-batch_size-18-epoch-328-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011437408-train_reward=0.011695310-batch_size-18-epoch-329-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.007081307-train_reward=0.011706087-batch_size-18-epoch-330-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000879261-train_reward=0.011707990-batch_size-18-epoch-331-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.007683371-train_reward=0.011706500-batch_size-18-epoch-332-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.019795312-train_reward=0.011683229-batch_size-18-epoch-333-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.012048361-train_reward=0.011697786-batch_size-18-epoch-334-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.017713286-train_reward=0.011682966-batch_size-18-epoch-335-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006206166-train_reward=0.011707340-batch_size-18-epoch-336-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009714614-train_reward=0.011681199-batch_size-18-epoch-337-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010913070-train_reward=0.011692944-batch_size-18-epoch-338-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.016488893-train_reward=0.011691391-batch_size-18-epoch-339-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.004225288-train_reward=0.011715254-batch_size-18-epoch-340-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.015811125-train_reward=0.011691982-batch_size-18-epoch-341-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007469576-train_reward=0.011703264-batch_size-18-epoch-342-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010084528-train_reward=0.011693859-batch_size-18-epoch-343-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002055563-train_reward=0.011700440-batch_size-18-epoch-344-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011976387-train_reward=0.011694718-batch_size-18-epoch-345-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.004172012-train_reward=0.011697851-batch_size-18-epoch-346-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007906885-train_reward=0.011697048-batch_size-18-epoch-347-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.014391005-train_reward=0.011673492-batch_size-18-epoch-348-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004012109-train_reward=0.011708371-batch_size-18-epoch-349-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.025225017-train_reward=0.011652199-batch_size-18-epoch-350-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.003788594-train_reward=0.011714348-batch_size-18-epoch-351-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001944032-train_reward=0.011699586-batch_size-18-epoch-352-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.016840214-train_reward=0.011674868-batch_size-18-epoch-353-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007267704-train_reward=0.011700854-batch_size-18-epoch-354-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.009355934-train_reward=0.011700092-batch_size-18-epoch-355-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002569676-train_reward=0.011710725-batch_size-18-epoch-356-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001391023-train_reward=0.011710725-batch_size-18-epoch-357-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.006864725-train_reward=0.011706984-batch_size-18-epoch-358-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.006284634-train_reward=0.011708422-batch_size-18-epoch-359-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.013676138-train_reward=0.011696811-batch_size-18-epoch-360-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.013391322-train_reward=0.011682302-batch_size-18-epoch-361-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006888139-train_reward=0.011717156-batch_size-18-epoch-362-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.008316358-train_reward=0.011703155-batch_size-18-epoch-363-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003833020-train_reward=0.011712628-batch_size-18-epoch-364-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009057424-train_reward=0.011709987-batch_size-18-epoch-365-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003652438-train_reward=0.011707661-batch_size-18-epoch-366-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.015608258-train_reward=0.011678675-batch_size-18-epoch-367-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001268871-train_reward=0.011700390-batch_size-18-epoch-368-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.003350635-train_reward=0.011703345-batch_size-18-epoch-369-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001336441-train_reward=0.011714111-batch_size-18-epoch-370-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.010811649-train_reward=0.011693368-batch_size-18-epoch-371-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006186869-train_reward=0.011706500-batch_size-18-epoch-372-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.004075486-train_reward=0.011704407-batch_size-18-epoch-373-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.020085959-train_reward=0.011693414-batch_size-18-epoch-374-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.006243752-train_reward=0.011691895-batch_size-18-epoch-375-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004617878-train_reward=0.011716506-batch_size-18-epoch-376-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.009913548-train_reward=0.011694999-batch_size-18-epoch-377-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.006819070-train_reward=0.011705897-batch_size-18-epoch-378-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.018465638-train_reward=0.011687860-batch_size-18-epoch-379-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001968480-train_reward=0.011707942-batch_size-18-epoch-380-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004977785-train_reward=0.011707340-batch_size-18-epoch-381-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007012470-train_reward=0.011704104-batch_size-18-epoch-382-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.000396707-train_reward=0.011705247-batch_size-18-epoch-383-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.007716780-train_reward=0.011703626-batch_size-18-epoch-384-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss-0.000213125-train_reward=0.011709885-batch_size-18-epoch-385-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.010275809-train_reward=0.011695631-batch_size-18-epoch-386-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009384432-train_reward=0.011700283-batch_size-18-epoch-387-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.008699846-train_reward=0.011703351-batch_size-18-epoch-388-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.001611394-train_reward=0.011709885-batch_size-18-epoch-389-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010403279-train_reward=0.011690149-batch_size-18-epoch-390-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.009910715-train_reward=0.011694551-batch_size-18-epoch-391-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.021618022-train_reward=0.011655904-batch_size-18-epoch-392-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000884236-train_reward=0.011708451-batch_size-18-epoch-393-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.018977230-train_reward=0.011674862-batch_size-18-epoch-394-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.009896780-train_reward=0.011694586-batch_size-18-epoch-395-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.010201097-train_reward=0.011686665-batch_size-18-epoch-396-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.006910508-train_reward=0.011701412-batch_size-18-epoch-397-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006065429-train_reward=0.011699466-batch_size-18-epoch-398-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss-0.004789178-train_reward=0.011712501-batch_size-18-epoch-399-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.019983739-train_reward=0.011679642-batch_size-18-epoch-400-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.021898426-train_reward=0.011640166-batch_size-18-epoch-401-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.013454018-train_reward=0.011695602-batch_size-18-epoch-402-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010302416-train_reward=0.011699879-batch_size-18-epoch-403-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.014805364-train_reward=0.011695474-batch_size-18-epoch-404-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.022148361-train_reward=0.011675262-batch_size-18-epoch-405-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.012665404-train_reward=0.011681171-batch_size-18-epoch-406-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.013227347-train_reward=0.011692122-batch_size-18-epoch-407-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.007634860-train_reward=0.011715905-batch_size-18-epoch-408-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.023296172-train_reward=0.011639577-batch_size-18-epoch-409-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.042086728-train_reward=0.011624273-batch_size-18-epoch-410-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.021719784-train_reward=0.011688030-batch_size-18-epoch-411-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.015121000-train_reward=0.011700196-batch_size-18-epoch-412-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.002250414-train_reward=0.011696178-batch_size-18-epoch-413-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.017378036-train_reward=0.011667410-batch_size-18-epoch-414-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.013442590-train_reward=0.011695702-batch_size-18-epoch-415-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.018835623-train_reward=0.011690902-batch_size-18-epoch-416-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.006813396-train_reward=0.011690659-batch_size-18-epoch-417-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.024802780-train_reward=0.011651422-batch_size-18-epoch-418-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002343412-train_reward=0.011713382-batch_size-18-epoch-419-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005682538-train_reward=0.011702511-batch_size-18-epoch-420-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.012030673-train_reward=0.011698495-batch_size-18-epoch-421-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.005901307-train_reward=0.011708914-batch_size-18-epoch-422-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003313051-train_reward=0.011680752-batch_size-18-epoch-423-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.018297471-train_reward=0.011697352-batch_size-18-epoch-424-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.015291678-train_reward=0.011697293-batch_size-18-epoch-425-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003386655-train_reward=0.011697786-batch_size-18-epoch-426-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.014626933-train_reward=0.011700166-batch_size-18-epoch-427-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.007722552-train_reward=0.011714001-batch_size-18-epoch-428-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.016180767-train_reward=0.011704789-batch_size-18-epoch-429-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.001606792-train_reward=0.011711868-batch_size-18-epoch-430-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010654436-train_reward=0.011718770-batch_size-18-epoch-431-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.022140419-train_reward=0.011683790-batch_size-18-epoch-432-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.028238796-train_reward=0.011673642-batch_size-18-epoch-433-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011739395-train_reward=0.011696161-batch_size-18-epoch-434-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000849637-train_reward=0.011713430-batch_size-18-epoch-435-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000604496-train_reward=0.011698652-batch_size-18-epoch-436-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.018350473-train_reward=0.011677089-batch_size-18-epoch-437-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010690111-train_reward=0.011673000-batch_size-18-epoch-438-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.006557680-train_reward=0.011696216-batch_size-18-epoch-439-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.000479646-train_reward=0.011717869-batch_size-18-epoch-440-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.016511738-train_reward=0.011685179-batch_size-18-epoch-441-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.024105664-train_reward=0.011642230-batch_size-18-epoch-442-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.006478656-train_reward=0.011705247-batch_size-18-epoch-443-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.019773521-train_reward=0.011697249-batch_size-18-epoch-444-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.074271739-train_reward=0.011263904-batch_size-18-epoch-445-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.020754680-train_reward=0.011639992-batch_size-18-epoch-446-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.090493970-train_reward=0.010960563-batch_size-18-epoch-447-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.072320879-train_reward=0.011207264-batch_size-18-epoch-448-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.017651254-train_reward=0.011682936-batch_size-18-epoch-449-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.154074654-train_reward=0.010065293-batch_size-18-epoch-450-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.015825318-train_reward=0.011676988-batch_size-18-epoch-451-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.070508137-train_reward=0.011414700-batch_size-18-epoch-452-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.037171803-train_reward=0.011196103-batch_size-18-epoch-453-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008706224-train_reward=0.011687227-batch_size-18-epoch-454-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.018638941-train_reward=0.011663898-batch_size-18-epoch-455-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.137770131-train_reward=0.010996831-batch_size-18-epoch-456-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.016569521-train_reward=0.011680319-batch_size-18-epoch-457-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.041536674-train_reward=0.011666334-batch_size-18-epoch-458-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.070228539-train_reward=0.011399816-batch_size-18-epoch-459-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.055884995-train_reward=0.011516229-batch_size-18-epoch-460-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005072988-train_reward=0.011673620-batch_size-18-epoch-461-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.041356716-train_reward=0.011645576-batch_size-18-epoch-462-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.106784612-train_reward=0.011319305-batch_size-18-epoch-463-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.012090278-train_reward=0.011694788-batch_size-18-epoch-464-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.028166210-train_reward=0.011667980-batch_size-18-epoch-465-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.017200479-train_reward=0.011675697-batch_size-18-epoch-466-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.041929454-train_reward=0.011631498-batch_size-18-epoch-467-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.034871213-train_reward=0.011642115-batch_size-18-epoch-468-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.016952481-train_reward=0.011698202-batch_size-18-epoch-469-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.010763079-train_reward=0.011710203-batch_size-18-epoch-470-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003914861-train_reward=0.011710725-batch_size-18-epoch-471-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.014010542-train_reward=0.011675486-batch_size-18-epoch-472-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.017273171-train_reward=0.011673445-batch_size-18-epoch-473-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.044163842-train_reward=0.011679686-batch_size-18-epoch-474-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.016580291-train_reward=0.011695131-batch_size-18-epoch-475-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.023371112-train_reward=0.011691333-batch_size-18-epoch-476-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.005647507-train_reward=0.011713056-batch_size-18-epoch-477-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.004348767-train_reward=0.011727787-batch_size-18-epoch-478-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.028177788-train_reward=0.011677823-batch_size-18-epoch-479-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.002343280-train_reward=0.011717733-batch_size-18-epoch-480-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.010956119-train_reward=0.011695189-batch_size-18-epoch-481-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.013304734-train_reward=0.011702172-batch_size-18-epoch-482-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.002537766-train_reward=0.011708633-batch_size-18-epoch-483-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.006344994-train_reward=0.011706087-batch_size-18-epoch-484-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.001473189-train_reward=0.011716426-batch_size-18-epoch-485-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.003261382-train_reward=0.011706500-batch_size-18-epoch-486-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011543680-train_reward=0.011706671-batch_size-18-epoch-487-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.016846251-train_reward=0.011692822-batch_size-18-epoch-488-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss-0.001183529-train_reward=0.011714104-batch_size-18-epoch-489-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.000000541-train_reward=0.011714524-batch_size-18-epoch-490-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.018205177-train_reward=0.011701212-batch_size-18-epoch-491-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.020171518-train_reward=0.011690712-batch_size-18-epoch-492-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.021229329-train_reward=0.011699466-batch_size-18-epoch-493-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.008452823-train_reward=0.011675439-batch_size-18-epoch-494-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.017304663-train_reward=0.011689240-batch_size-18-epoch-495-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.014803682-train_reward=0.011683569-batch_size-18-epoch-496-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.019744154-train_reward=0.011705002-batch_size-18-epoch-497-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.012727343-train_reward=0.011705401-batch_size-18-epoch-498-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.030121598-train_reward=0.011654287-batch_size-18-epoch-499-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006946349-train_reward=0.011710057-batch_size-18-epoch-500-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.007999871-train_reward=0.011694458-batch_size-18-epoch-501-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.006635014-train_reward=0.011709392-batch_size-18-epoch-502-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.021280391-train_reward=0.011684712-batch_size-18-epoch-503-batch_id-(1/1)\n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ...\n",
      "----no teaching forcing----\n",
      "loss--0.014574308-train_reward=0.011695971-batch_size-18-epoch-504-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.033950794-train_reward=0.011680505-batch_size-18-epoch-505-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      "----no teaching forcing----\n",
      "loss--0.039733373-train_reward=0.011640047-batch_size-18-epoch-506-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      "----no teaching forcing----\n",
      "loss--0.019636650-train_reward=0.011701989-batch_size-18-epoch-507-batch_id-(1/1)\n",
      " ----<o_o>---- <low_freq>\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.011845403-train_reward=0.011702702-batch_size-18-epoch-508-batch_id-(1/1)\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- ,\n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      " ----<o_o>---- \n",
      "----no teaching forcing----\n",
      "loss--0.004739244-train_reward=0.011711006-batch_size-18-epoch-509-batch_id-(1/1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9f03e224b477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running time: %.2f mins'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9f03e224b477>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(epoch, batch_size, train_set_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                              \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_input_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                              \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                              is_train=1, teaching_rate=1)\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dee7ea727ae3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, input_lens, labels, is_train, teaching_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteaching_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0menc_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menc_hn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_cn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2289193a259c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, inputs_len)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_order_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, device, async)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 300\n",
    "lr=0.005\n",
    "batch_size=18\n",
    "train_set_size=int(len(train_set_inputs)/5000)\n",
    "epochs=10000\n",
    "train_bleu = 0\n",
    "autoencoder = AutoEncoder(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 25)\n",
    "#pre train para\n",
    "#pre_train = torch.load('./models_better/loss-2.099016905-bleu-0.4078-hidden_dim-512-input_dim-300-epoch-0-batch_size-200-batch_id-[7001-[of]-21743]-lr-0.0050')\n",
    "# pre_train = torch.load('./models_saved/time-[2019-01-07-23-18-32]-loss-1.005809546-bleu-0.6937-hidden_dim-256-input_dim-300-epoch-1-batch_size-200-batch_id-[6001-[of]-21743]-lr-0.0050', map_location = 'cpu')\n",
    "# # pre_train = torch.load('./models_better/try')\n",
    "# # pre_train = torch.load('./models_saved/train_bleu-0.0000-hidden_dim-256-input_dim-300-epoch-14-batch_size-200-lr-0.0050')\n",
    "\n",
    "# autoencoder.load_state_dict(pre_train)\n",
    "\n",
    "# autoencoder.dec.init_nll_loss()\n",
    "\n",
    "if use_cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, autoencoder.parameters()), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def model_train(epoch, batch_size, train_set_size):\n",
    "    batch_id = 0\n",
    "    valid_bleu = 0\n",
    "    train_reward = 0\n",
    "    for start_idx in range(0, train_set_size-batch_size, batch_size):\n",
    "#         print('batch id: ', batch_id)\n",
    "            \n",
    "        batch_id+=1\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        optimizer.zero_grad()#clear\n",
    "#         loss, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "#                                      torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "#                                      labels=torch.LongTensor(train_set_labels[start_idx:end_idx]), \n",
    "#                                      is_train=1, teaching_rate=1)\n",
    "        \n",
    "        loss, predicts, train_reward = autoencoder.train_using_rl(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "                                                                 torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "                                                                 labels=train_set_labels[start_idx:end_idx], \n",
    "                                                                 is_train=1, teaching_rate=1)\n",
    "#         torch.cuda.empty_cache()\n",
    "        #optimize\n",
    "        loss.backward()#retain_graph=True)\n",
    "        optimizer.step()\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_id%50==1:\n",
    "            autoencoder.eval()\n",
    "            sample_num = 10\n",
    "            rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "            #teaching forcing\n",
    "            loss_, predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            \n",
    "            predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "            labels = batch_tokens_remove_eos(train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "            predicts = batch_tokens2words(predicts, vocab)\n",
    "            labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "            predicts_sents = batch_words2sentence(predicts)\n",
    "            labels_sents = batch_words2sentence(labels)\n",
    "            \n",
    "            for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "                print(predict_sent, '----<o_o>----', label_sent)\n",
    "                \n",
    "            #no teaching forcing\n",
    "            print('----no teaching forcing----')\n",
    "#             predicts = autoencoder.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "#                                              torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "#                                              labels=torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "#                                              is_train=0, teaching_rate=1)\n",
    "#             tokenized_sents=predicts.tolist()\n",
    "#             real_sents=[]\n",
    "#             label_tokenized_sents=train_set_labels[rand_idx:rand_idx+sample_num]\n",
    "#             label_real_sents=[]\n",
    "#             for idx, sent in enumerate(tokenized_sents):\n",
    "#                 real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "#             for sent in label_tokenized_sents:\n",
    "#                 label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "#             for (real_sent, label_real_sent) in zip(real_sents, label_real_sents):\n",
    "#                 print(real_sent, '----<o_o>----', label_real_sent)\n",
    "                \n",
    "            info_stamp = 'loss-{:2.9f}-train_reward={:2.9f}-batch_size-{:n}-epoch-{:n}-batch_id-({:n}/{:n})'.format(\n",
    "                              loss.data[0], train_reward, batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "            print(info_stamp)\n",
    "#             #valid_set testing\n",
    "#             if batch_id%1000==1:\n",
    "#                 rand_idx=random.randint(0, len(valid_set_inputs)-batch_size-1-1)\n",
    "#                 predicts = autoencoder.forward(torch.LongTensor(valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "#                                                  torch.LongTensor(valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "#                                                  labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "#                                                  is_train=0, teaching_rate=1)\n",
    "#                 tokenized_sents=predicts.tolist()\n",
    "#                 real_sents=[]\n",
    "#                 label_tokenized_sents=valid_set_labels[rand_idx:rand_idx+batch_size]\n",
    "#                 label_real_sents=[]\n",
    "#                 for idx, sent in enumerate(tokenized_sents):\n",
    "#                     real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "#                 for sent in label_tokenized_sents:\n",
    "#                     label_real_sents.append(tokenized_sent2real_sent(sent, autoencoder.enc.vocab))\n",
    "\n",
    "#                 bleu_score, valid_num = data_set_bleu(label_real_sents, real_sents)\n",
    "#                 if valid_num>10:\n",
    "#                     valid_bleu = bleu_score/valid_num\n",
    "                       \n",
    "#                 info_stamp = 'loss-{:2.9f}-bleu-{:1.4f}-hidden_dim-{:n}-input_dim-{:n}-epoch-{:n}-batch_size-{:n}-batch_id-[{:n}-[of]-{:n}]-lr-{:1.4f}'.format(\n",
    "#                               loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "#                 print(valid_num, info_stamp)\n",
    "#                 now = int(round(time.time()*1000))\n",
    "#                 time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "#                 torch.save(autoencoder.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "                \n",
    "            autoencoder.train()\n",
    "            \n",
    "for epoch in range(epochs):\n",
    "    model_train(epoch, batch_size, train_set_size)\n",
    "    \n",
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
