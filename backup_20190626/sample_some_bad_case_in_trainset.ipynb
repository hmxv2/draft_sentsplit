{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from Vocab import Vocab\n",
    "from LanguageModel import LanguageModel\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "print('import over')\n",
    "\n",
    "copy_thres=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [' '.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('data_set/vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "\n",
    "    \n",
    "def seqs_split(seqs, vocab):\n",
    "    seqs = batch_tokens_remove_eos(seqs, vocab)\n",
    "    simple_sent1s=[]\n",
    "    simple_sent2s=[]\n",
    "    for seq in seqs:\n",
    "        simple_sent1=[]\n",
    "        simple_sent2=[]\n",
    "        sent=simple_sent1\n",
    "        for token in seq:\n",
    "            if token==vocab.word2token['<split>']:\n",
    "                sent=simple_sent2\n",
    "            else:\n",
    "                sent.append(token)\n",
    "        simple_sent1s.append(simple_sent1)\n",
    "        simple_sent2s.append(simple_sent2)\n",
    "        \n",
    "    return simple_sent1s, simple_sent2s\n",
    "\n",
    "def simple_sents_concat(simple_sent1s, simple_sent2s, vocab, max_length):\n",
    "    simple_sent_lens=[]\n",
    "    simple_sents=simple_sent1s\n",
    "    for i, sent in enumerate(simple_sent2s):\n",
    "        simple_sents[i].append(vocab.word2token['<split>'])\n",
    "        for token in sent:\n",
    "            simple_sents[i].append(token)\n",
    "\n",
    "        #if there is no <split> in simple_sent1s and simple_sent2s, then the length of sents_concat will be longer than max_length\n",
    "        if len(simple_sents[i])>max_length:\n",
    "            simple_sents[i] = simple_sents[i][:max_length]\n",
    "            \n",
    "        simple_sent_lens.append(len(simple_sents[i]))\n",
    "            \n",
    "        while(len(simple_sents[i])<max_length):\n",
    "            simple_sents[i].append(vocab.word2token['<padding>'])\n",
    "            \n",
    "    return simple_sents, simple_sent_lens\n",
    "\n",
    "\n",
    "def get_lm_inputs_and_labels(sents, vocab, max_length):\n",
    "    lm_inputs=copy.deepcopy(sents)\n",
    "    lm_labels=copy.deepcopy(sents)\n",
    "    lm_input_lens=[]\n",
    "    \n",
    "    for sent in lm_inputs:\n",
    "        if len(sent)>=max_length:\n",
    "            sent=sent[:max_length-1]\n",
    "        sent.insert(0, vocab.word2token['<sos>'])\n",
    "        lm_input_lens.append(len(sent))\n",
    "        while(len(sent)<max_length):\n",
    "            sent.append(vocab.word2token['<padding>'])\n",
    "\n",
    "    for sent in lm_labels:\n",
    "        if len(sent)>=max_length:\n",
    "            sent = sent[:max_length-1]\n",
    "        sent.append(vocab.word2token['<eos>'])\n",
    "        while(len(sent)<max_length):\n",
    "            sent.append(vocab.word2token['<padding>'])\n",
    "        \n",
    "    return lm_inputs, lm_input_lens, lm_labels\n",
    "\n",
    "\n",
    "def duplicate_reconstruct_labels(sents, topk):\n",
    "    return [x for x in sents for ii in range(topk)]\n",
    "\n",
    "\n",
    "def batch_tokens_bleu_split_version(references, candidates, vocab, smooth_epsilon=0.001):\n",
    "    # needn't remove '<sos>' token before calling this function, which is different from the 'batch_token_bleu()' version\n",
    "    #\n",
    "    ref1, ref2 = seqs_split(references, vocab)\n",
    "    cand1, cand2 = seqs_split(candidates, vocab)\n",
    "    bleu_simple_sent1s = batch_tokens_bleu(ref1, cand1)\n",
    "    bleu_simple_sent2s = batch_tokens_bleu(ref2, cand2)\n",
    "#     print(bleu_simple_sent1s)\n",
    "#     print(bleu_simple_sent2s)\n",
    "    bleu=[]\n",
    "    for idx in range(len(bleu_simple_sent1s)):\n",
    "        bleu.append((bleu_simple_sent1s[idx]+bleu_simple_sent2s[idx])/2)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def set_model_grad(model, is_grad):\n",
    "    for param in model.parameters():\n",
    "         param.requires_grad = is_grad\n",
    "            \n",
    "def batch_tokens_remove_padding(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<padding>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data set\n",
    "\n",
    "with open('./data_set2/split_data_set/train_complex_sents.pk', 'rb') as f:\n",
    "    split_train_set_inputs = pickle.load(f)\n",
    "with open('./data_set2/split_data_set/train_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_train_set_input_lens = pickle.load(f)\n",
    "with open('./data_set2/split_data_set/train_pseudo_labels.pk', 'rb') as f:\n",
    "    split_pseudo_train_set_labels = pickle.load(f)\n",
    "    \n",
    "# with open('./data_set2/split_data_set/train_complex_sents_supervised.pk', 'rb') as f:\n",
    "#     split_train_set_inputs_supervised = pickle.load(f)\n",
    "# with open('./data_set2/split_data_set/train_complex_sent_lens_supervised.pk', 'rb') as f:\n",
    "#     split_train_set_input_lens_supervised = pickle.load(f)\n",
    "# with open('./data_set2/split_data_set/train_labels_supervised.pk', 'rb') as f:\n",
    "#     split_train_set_labels_supervised = pickle.load(f)\n",
    "    \n",
    "with open('./data_set2/split_data_set/train_labels.pk', 'rb') as f:\n",
    "    split_train_set_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791956 791956\n"
     ]
    }
   ],
   "source": [
    "print(len(split_train_set_inputs), len(split_train_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['residents of the neighborhood said two brothers who were hamas fighters were in the area at the time of the attack but that the mortar fire had not come from the school compound , but from elsewhere in the neighborhood .']\n",
      "['residents of the neighborhood said two brothers who were hamas fighters were in the area at the time of the attack . <split> but the residents also said the mortar fire had not come from the school compound , but from elsewhere in the neighborhood .']\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(split_train_set_inputs)-1)\n",
    "a=split_train_set_inputs[idx]\n",
    "label = split_train_set_labels[idx]\n",
    "\n",
    "a = batch_tokens_remove_padding([a], vocab)\n",
    "label = batch_tokens_remove_eos([label], vocab)\n",
    "a = batch_tokens2words(a, vocab)\n",
    "label = batch_tokens2words(label, vocab)\n",
    "a = batch_words2sentence(a)\n",
    "label = batch_words2sentence(label)\n",
    "print(a)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha\n"
     ]
    }
   ],
   "source": [
    "a=batch_tokens_remove_padding(split_train_set_inputs, vocab)\n",
    "label = batch_tokens_remove_eos(split_train_set_labels, vocab)\n",
    "\n",
    "bleus = batch_tokens_bleu(references=label, candidates=a)\n",
    "print('haha')\n",
    "a=batch_tokens2words(a, vocab)\n",
    "label=batch_tokens2words(label, vocab)\n",
    "inputs=batch_words2sentence(a)\n",
    "labels=batch_words2sentence(label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795\n"
     ]
    }
   ],
   "source": [
    "indices=[]\n",
    "cnt=0\n",
    "for idx, bleu in enumerate(bleus):\n",
    "    if bleu>0.3 and bleu<0.45 and len(a[idx])<30 and len(label[idx])<30 and '<low_freq>' not in a[idx] and '<low_freq>' not in label[idx]:\n",
    "        indices.append(idx)\n",
    "        cnt+=1\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cape breton north was a provincial electoral district in cape breton , nova scotia , canada , that elected one member of the nova scotia house of assembly .\n",
      "cape breton north is a former provincial electoral district in nova scotia , canada . <split> it elected one member to the nova scotia house of assembly .\n"
     ]
    }
   ],
   "source": [
    "idx = random.choice(indices)\n",
    "print(inputs[idx])\n",
    "print(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3175938828949709]\n"
     ]
    }
   ],
   "source": [
    "kk='the colony grew quickly upon its founding , but internal disputes and lack of funding spelled its demise by 1850 .'\n",
    "kkk='the colony grew quickly upon its founding . <split> internal disputes , lack of funding and the draw of urban jobs led to its decline by 1850 .'\n",
    "print(batch_tokens_bleu(references=[kkk.split(' ')], candidates=[kk.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.41148498148887\n",
      "37.89943001883943\n"
     ]
    }
   ],
   "source": [
    "# length analyse, inputs\n",
    "lengths=0\n",
    "for ii in range(len(a)):\n",
    "    lengths+=len(a[ii])\n",
    "print(lengths/len(a))\n",
    "\n",
    "# length analyse, labels\n",
    "lengths=0\n",
    "for ii in range(len(a)):\n",
    "    lengths+=len(label[ii])\n",
    "print(lengths/len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6569447777209572\n"
     ]
    }
   ],
   "source": [
    "#BLEU score of pseudo label and true label\n",
    "bleu_score = batch_tokens_bleu_split_version(references=split_train_set_labels, candidates=split_pseudo_train_set_labels, vocab=vocab)\n",
    "\n",
    "s=0\n",
    "for bleu in bleu_score:\n",
    "    s+=bleu\n",
    "print(s/len(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6568139995249009\n"
     ]
    }
   ],
   "source": [
    "num=len(split_train_set_labels)-1\n",
    "\n",
    "references=split_train_set_labels[:num]\n",
    "candidates=split_pseudo_train_set_labels[:num]\n",
    "\n",
    "ref1, ref2 = seqs_split(references, vocab)\n",
    "cand1, cand2 = seqs_split(candidates, vocab)\n",
    "bleu_simple_sent1s = batch_tokens_bleu(ref1, cand1, smooth_epsilon=0.0001)\n",
    "bleu_simple_sent2s = batch_tokens_bleu(ref2, cand2, smooth_epsilon=0.0001)\n",
    "\n",
    "# print(bleu_simple_sent1s)\n",
    "# print(bleu_simple_sent2s)\n",
    "\n",
    "bleu=[]\n",
    "for idx in range(len(bleu_simple_sent1s)):\n",
    "    bleu.append((bleu_simple_sent1s[idx]+bleu_simple_sent2s[idx])/2)\n",
    "\n",
    "s=0\n",
    "for x in bleu:\n",
    "    s+=x\n",
    "print(s/len(bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data set\n",
    "\n",
    "with open('./data_set2/split_data_set/test_complex_sents.pk', 'rb') as f:\n",
    "    split_test_set_inputs = pickle.load(f)\n",
    "with open('./data_set2/split_data_set/test_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_test_set_input_lens = pickle.load(f)\n",
    "with open('./data_set2/split_data_set/test_labels.pk', 'rb') as f:\n",
    "    split_test_set_labels = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "print(len(split_test_set_labels), len(split_test_set_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[11206, 4309, 41089, 10934, 14725, 40533, 21467, 19385, 3, 28954, 10934, 872, 30803, 13264, 25932, 27513, 31824, 25168, 38312, 10934, 29270, 500, 13264, 25932, 13264, 3, 13264, 13069, 37734, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[11206, 4309, 41089, 10934, 14725, 40533, 21467, 19385, 3, 28954, 10934, 872, 30803, 13264, 37734, 5, 25932, 27513, 31824, 25168, 38312, 10934, 29270, 500, 13264, 25932, 13264, 3, 13264, 13069, 37734] 31 29 50\n"
     ]
    }
   ],
   "source": [
    "pseudo_labels=[]\n",
    "for idx, split_test_set_input in enumerate(split_test_set_inputs):\n",
    "    pseudo_label=[]\n",
    "    cut_idx = int(split_test_set_input_lens[idx]/2)\n",
    "    for ii in range(split_test_set_input_lens[idx]):\n",
    "        pseudo_label.append(split_test_set_input[ii])\n",
    "        if ii==cut_idx-1:\n",
    "            pseudo_label.append(vocab.word2token['.'])\n",
    "            pseudo_label.append(vocab.word2token['<split>'])\n",
    "            \n",
    "    pseudo_labels.append(pseudo_label)\n",
    "    if (len(pseudo_label)-split_test_set_input_lens[idx])!=2:\n",
    "        print(len(pseudo_label)-split_test_set_input_lens[idx])\n",
    "    if idx==110:\n",
    "        print(cut_idx)\n",
    "        print(split_test_set_inputs[idx])\n",
    "        print(pseudo_label, len(pseudo_label), split_test_set_input_lens[idx], len(split_test_set_inputs[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6552914947849449\n"
     ]
    }
   ],
   "source": [
    "scores = batch_tokens_bleu_split_version(references=split_test_set_labels, candidates=pseudo_labels, vocab=vocab)\n",
    "s=0\n",
    "for score in scores:\n",
    "    s+=score\n",
    "print(s/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "0.6501506333778686\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for label in split_test_set_labels:\n",
    "    tmp=[]\n",
    "    for token in label:\n",
    "        if token!=vocab.word2token['<low_freq>']:\n",
    "            tmp.append(token)\n",
    "    a.append(tmp)\n",
    "    \n",
    "for label in pseudo_labels:\n",
    "    tmp=[]\n",
    "    for token in label:\n",
    "        if token!=vocab.word2token['<low_freq>']:\n",
    "            tmp.append(token)\n",
    "    b.append(tmp)\n",
    "    \n",
    "print('h')\n",
    "\n",
    "scores = batch_tokens_bleu_split_version(references=a, candidates=b, vocab=vocab)\n",
    "s=0\n",
    "for score in scores:\n",
    "    s+=score\n",
    "print(s/len(scores))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
