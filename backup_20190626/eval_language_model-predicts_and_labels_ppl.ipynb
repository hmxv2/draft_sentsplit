{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [' '.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('data_set/vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "\n",
    "    \n",
    "def seqs_split(seqs, vocab):\n",
    "    seqs = batch_tokens_remove_eos(seqs, vocab)\n",
    "    simple_sent1s=[]\n",
    "    simple_sent2s=[]\n",
    "    for seq in seqs:\n",
    "        simple_sent1=[]\n",
    "        simple_sent2=[]\n",
    "        sent=simple_sent1\n",
    "        for token in seq:\n",
    "            if token==vocab.word2token['<split>']:\n",
    "                sent=simple_sent2\n",
    "            else:\n",
    "                sent.append(token)\n",
    "        simple_sent1s.append(simple_sent1)\n",
    "        simple_sent2s.append(simple_sent2)\n",
    "        \n",
    "    return simple_sent1s, simple_sent2s\n",
    "\n",
    "def simple_sents_concat(simple_sent1s, simple_sent2s, vocab, max_length):\n",
    "    simple_sent_lens=[]\n",
    "    simple_sents=simple_sent1s\n",
    "    for i, sent in enumerate(simple_sent2s):\n",
    "        simple_sents[i].append(vocab.word2token['<split>'])\n",
    "        for token in sent:\n",
    "            simple_sents[i].append(token)\n",
    "\n",
    "        #if there is no <split> in simple_sent1s and simple_sent2s, then the length of sents_concat will be longer than max_length\n",
    "        if len(simple_sents[i])>max_length:\n",
    "            simple_sents[i] = simple_sents[i][:max_length]\n",
    "            \n",
    "        simple_sent_lens.append(len(simple_sents[i]))\n",
    "            \n",
    "        while(len(simple_sents[i])<max_length):\n",
    "            simple_sents[i].append(vocab.word2token['<padding>'])\n",
    "            \n",
    "    return simple_sents, simple_sent_lens\n",
    "\n",
    "\n",
    "def get_lm_inputs_and_labels(sents, vocab, max_length):\n",
    "    lm_inputs=copy.deepcopy(sents)\n",
    "    lm_labels=copy.deepcopy(sents)\n",
    "    lm_input_lens=[]\n",
    "    \n",
    "    for sent in lm_inputs:\n",
    "        if len(sent)>=max_length:\n",
    "            sent=sent[:max_length-1]\n",
    "        sent.insert(0, vocab.word2token['<sos>'])\n",
    "        lm_input_lens.append(len(sent))\n",
    "        while(len(sent)<max_length):\n",
    "            sent.append(vocab.word2token['<padding>'])\n",
    "\n",
    "    for sent in lm_labels:\n",
    "        if len(sent)>=max_length:\n",
    "            sent = sent[:max_length-1]\n",
    "        sent.append(vocab.word2token['<eos>'])\n",
    "        while(len(sent)<max_length):\n",
    "            sent.append(vocab.word2token['<padding>'])\n",
    "        \n",
    "    return lm_inputs, lm_input_lens, lm_labels\n",
    "\n",
    "\n",
    "def duplicate_reconstruct_labels(sents, topk):\n",
    "    return [x for x in sents for ii in range(topk)]\n",
    "\n",
    "\n",
    "def batch_tokens_bleu_split_version(references, candidates, vocab, smooth_epsilon=0.001):\n",
    "    # needn't remove '<sos>' token before calling this function, which is different from the 'batch_token_bleu()' version\n",
    "    #\n",
    "    ref1, ref2 = seqs_split(references, vocab)\n",
    "    cand1, cand2 = seqs_split(candidates, vocab)\n",
    "    bleu_simple_sent1s = batch_tokens_bleu(ref1, cand1)\n",
    "    bleu_simple_sent2s = batch_tokens_bleu(ref2, cand2)\n",
    "#     print(bleu_simple_sent1s)\n",
    "#     print(bleu_simple_sent2s)\n",
    "    bleu=[]\n",
    "    for idx in range(len(bleu_simple_sent1s)):\n",
    "        bleu.append((bleu_simple_sent1s[idx]+bleu_simple_sent2s[idx])/2)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def set_model_grad(model, is_grad):\n",
    "    for param in model.parameters():\n",
    "         param.requires_grad = is_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 15000\n"
     ]
    }
   ],
   "source": [
    "with open('./data_set/split_data_set/validation_labels.pk', 'rb') as f:\n",
    "    split_valid_set_labels = pickle.load(f)\n",
    "with open('./data_set/split_data_set/validation_predicts-topk=3.pk', 'rb') as f:\n",
    "    split_valid_set_predicts = pickle.load(f)\n",
    "    \n",
    "    \n",
    "print(len(split_valid_set_labels), len(split_valid_set_predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=False,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('data_set/pre_trained_token_embedding_300d.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "#         self.embed.weight.requires_grad = False\n",
    "\n",
    "\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        self.cost_func = nn.CrossEntropyLoss(weight=torch.Tensor(self.weight), reduce=True)\n",
    "        self.fcnn=nn.Linear(in_features = self.hidden_dim, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "#         hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "#         cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "#         #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "#         if self.use_cuda:\n",
    "#             hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "#             cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "#         else:\n",
    "#             hn = hn.index_select(0, Variable(true_order_ids))\n",
    "#             cn = cn.index_select(0, Variable(true_order_ids))\n",
    "        logits = self.fcnn(outputs)\n",
    "        return logits\n",
    "    \n",
    "    def get_loss(self, logits, labels):\n",
    "        labels = self._tocuda(Variable(labels))\n",
    "        sent_len = logits.size(dim=1)\n",
    "        labels = labels[:, :sent_len]\n",
    "        labels = labels.contiguous().view(-1)\n",
    "        logits = logits.view(-1, len(self.vocab.word2token))\n",
    "#         print('logits size: ', logits.size())\n",
    "        \n",
    "        return self.cost_func(logits, labels)\n",
    "    \n",
    "    def get_sentences_ppl(self, inputs, inputs_len, labels):\n",
    "        \n",
    "        vocab_size=len(self.vocab.word2token)\n",
    "        batch_size=inputs.size(0)\n",
    "        \n",
    "        logits = self.forward(inputs, inputs_len)\n",
    "        \n",
    "        labels = self._tocuda(Variable(labels))\n",
    "        sent_len = logits.size(dim=1)\n",
    "        labels = labels[:, :sent_len]\n",
    "        labels = labels.contiguous()\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        log_probs= self.log_softmax(logits).view(-1)\n",
    "        \n",
    "        pos_bias=torch.LongTensor([i*vocab_size for i in range(sent_len)]).view(1,-1)\n",
    "        pos_bias = pos_bias.expand(batch_size, pos_bias.size(1))\n",
    "#         print(pos_bias.size())\n",
    "\n",
    "        batch_bias = torch.LongTensor([i*vocab_size*sent_len for i in range(batch_size)]).view(-1,1)\n",
    "        batch_bias = batch_bias.expand(batch_bias.size(0), sent_len)\n",
    "#         print(batch_bias.size())\n",
    "\n",
    "        pos_bias = self._tocuda(Variable(pos_bias, requires_grad=0))\n",
    "        batch_bias = self._tocuda(Variable(batch_bias, requires_grad=0))\n",
    "        \n",
    "        indices=labels+pos_bias+batch_bias\n",
    "        indices = indices.view(-1)\n",
    "\n",
    "        results=log_probs[indices].view(batch_size, sent_len)\n",
    "        \n",
    "        sents_ppl=[]\n",
    "        for idx, result in enumerate(results):\n",
    "            mean_log_prob = torch.mean(result[:inputs_len[idx]])\n",
    "            sents_ppl.append(mean_log_prob)\n",
    "\n",
    "        sents_ppl = torch.cat(sents_ppl, dim=0)\n",
    "        sents_ppl = torch.exp(-sents_ppl)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            return sents_ppl.cpu().data.tolist()\n",
    "        else:\n",
    "            return sents_ppl.data.tolist()\n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 512\n",
    "input_dim = 300\n",
    "lr=0.005\n",
    "batch_size=100\n",
    "\n",
    "epochs=10000\n",
    "train_bleu_mean=-1\n",
    "train_bleu_max=-1\n",
    "\n",
    "language_model = LanguageModel(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab)\n",
    "#512\n",
    "model_path = './models_language_model/time-[2019-02-26-10-13-36]-info=[language_model]-loss=4.188151360-bleu=-1.0000-hidden_dim=512-input_dim=300-epoch=19-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "model_path = './models_language_model/time-[2019-02-26-11-27-36]-info=[language_model]-loss=3.414012194-bleu=-1.0000-hidden_dim=512-input_dim=300-epoch=21-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "model_path = './models_language_model/time-[2019-02-26-13-18-56]-info=[language_model]-loss=4.003012180-bleu=-1.0000-hidden_dim=512-input_dim=300-epoch=24-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "#2048\n",
    "# model_path = './models_language_model/time-[2019-02-28-02-41-29]-info=[language_model]-loss=4.125045300-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=2-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-28-07-04-08]-info=[language_model]-loss=3.475848675-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=4-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-28-09-15-24]-info=[language_model]-loss=4.169310093-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=5-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-28-13-37-55]-info=[language_model]-loss=4.279534817-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=7-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "#1024\n",
    "# model_path = './models_language_model/time-[2019-02-27-16-28-32]-info=[language_model]-loss=4.221434593-bleu=-1.0000-hidden_dim=1024-input_dim=300-epoch=1-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# mdoel_path = './models_language_model/time-[2019-02-27-18-39-43]-info=[language_model]-loss=4.328499317-bleu=-1.0000-hidden_dim=1024-input_dim=300-epoch=3-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-27-21-58-23]-info=[language_model]-loss=4.111208439-bleu=-1.0000-hidden_dim=1024-input_dim=300-epoch=6-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "\n",
    "pre_train = torch.load(model_path, map_location='cpu')\n",
    "language_model.load_state_dict(pre_train)\n",
    "\n",
    "if use_cuda:\n",
    "    language_model = language_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "67.02538807309371\n"
     ]
    }
   ],
   "source": [
    "batch_id=0\n",
    "all_rewards=[]\n",
    "for idx in range(0, len(split_valid_set_labels), batch_size):\n",
    "    batch_id+=1\n",
    "    simple_sent1s, simple_sent2s = seqs_split(split_valid_set_labels[idx:idx+batch_size], vocab)\n",
    "        \n",
    "    lm_input1s, lm_input1_lens, lm_label1s = get_lm_inputs_and_labels(simple_sent1s, vocab, 61)\n",
    "    simple_sent1s_ppl = language_model.get_sentences_ppl(torch.LongTensor(lm_input1s), \n",
    "                                                  torch.LongTensor(lm_input1_lens), \n",
    "                                                  torch.LongTensor(lm_label1s)\n",
    "                                                )\n",
    "    lm_input2s, lm_input2_lens, lm_label2s = get_lm_inputs_and_labels(simple_sent2s, vocab, 61)\n",
    "    simple_sent2s_ppl = language_model.get_sentences_ppl(torch.LongTensor(lm_input2s), \n",
    "                                                  torch.LongTensor(lm_input2_lens), \n",
    "                                                  torch.LongTensor(lm_label2s)\n",
    "                                                )\n",
    "    for ii in range(len(simple_sent1s_ppl)):\n",
    "        all_rewards.append((1/simple_sent1s_ppl[ii]+1/simple_sent2s_ppl[ii])/2)\n",
    "        \n",
    "print(batch_id)\n",
    "s=0\n",
    "for reward in all_rewards:\n",
    "    s+=reward\n",
    "print(1/(s/len(all_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "101.69841964367605\n"
     ]
    }
   ],
   "source": [
    "batch_id=0\n",
    "all_rewards=[]\n",
    "\n",
    "predicts=[]\n",
    "for ii in range(len(split_valid_set_predicts)):\n",
    "    if ii%3==0:\n",
    "        predicts.append(split_valid_set_predicts[ii])\n",
    "split_valid_set_predicts=predicts\n",
    "for idx in range(0, len(split_valid_set_predicts), batch_size):\n",
    "    batch_id+=1\n",
    "    simple_sent1s, simple_sent2s = seqs_split(split_valid_set_predicts[idx:idx+batch_size], vocab)\n",
    "        \n",
    "    lm_input1s, lm_input1_lens, lm_label1s = get_lm_inputs_and_labels(simple_sent1s, vocab, 61)\n",
    "    simple_sent1s_ppl = language_model.get_sentences_ppl(torch.LongTensor(lm_input1s), \n",
    "                                                  torch.LongTensor(lm_input1_lens), \n",
    "                                                  torch.LongTensor(lm_label1s)\n",
    "                                                )\n",
    "    lm_input2s, lm_input2_lens, lm_label2s = get_lm_inputs_and_labels(simple_sent2s, vocab, 61)\n",
    "    simple_sent2s_ppl = language_model.get_sentences_ppl(torch.LongTensor(lm_input2s), \n",
    "                                                  torch.LongTensor(lm_input2_lens), \n",
    "                                                  torch.LongTensor(lm_label2s)\n",
    "                                                )\n",
    "    for ii in range(len(simple_sent1s_ppl)):\n",
    "        all_rewards.append((1/simple_sent1s_ppl[ii]+1/simple_sent2s_ppl[ii])/2)\n",
    "        \n",
    "print(batch_id)\n",
    "s=0\n",
    "for reward in all_rewards:\n",
    "    s+=reward\n",
    "print(1/(s/len(all_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452.750244140625, 56.67298889160156]\n",
      "<low_freq> the <low_freq> 's , to <low_freq> slaves , the <low_freq> is <low_freq> were to and led been a a is now the - <low_freq> , , and been different - ,\n",
      "<low_freq> the <low_freq> was the <low_freq> , of the , the were were not as the <low_freq> of of the , and the the province population of the <low_freq> of china . the , <low_freq> . and is the part minorities separate of the province\n",
      "\n",
      "\n",
      "since the slave masters spoke southern american english , the english the slaves learned , which has developed into what is now african american vernacular english , had many sae features .\n",
      "since the <low_freq> of the taiwan province in 1998 , these subdivisions are regarded as the principal subdivisions of taiwan , and cover the current jurisdiction of the republic of china except <low_freq> and <low_freq> , which are historically and administratively part of fujian .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_range = int((len(split_valid_set_inputs)-1))\n",
    "rand_idx=random.randint(0, data_range)\n",
    "sample_num=2\n",
    "language_model.eval()\n",
    "sents_ppl = language_model.get_sentences_ppl(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                          torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+sample_num]),\n",
    "                                          torch.LongTensor(split_valid_set_labels[rand_idx:rand_idx+sample_num])\n",
    "                                          )\n",
    "\n",
    "print(sents_ppl)\n",
    "\n",
    "\n",
    "logits = language_model.forward(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                     torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+sample_num]),\n",
    "                                                )\n",
    "\n",
    "_, predicts = logits.max(dim=2)\n",
    "predicts = predicts.cpu().data.tolist()\n",
    "predicts = batch_tokens_remove_eos(tokens_list=predicts, vocab=vocab)\n",
    "words_list = batch_tokens2words(predicts, vocab)\n",
    "sents = batch_words2sentence(words_list=words_list)\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#labels\n",
    "labels=split_valid_set_labels[rand_idx:rand_idx+sample_num]\n",
    "labels = batch_tokens_remove_eos(tokens_list=labels, vocab=vocab)\n",
    "words_list = batch_tokens2words(labels, vocab)\n",
    "sents = batch_words2sentence(words_list=words_list)\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    \n",
    "language_model.train()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.02750500671863\n"
     ]
    }
   ],
   "source": [
    "def model_test(model, batch_size, dataset_size, test_set_inputs, test_set_input_lens, test_set_labels):\n",
    "    ppl_sum=0\n",
    "    all_ppl=[]\n",
    "    for start_idx in range(0, dataset_size, batch_size):\n",
    "        sents_ppl = language_model.get_sentences_ppl(torch.LongTensor(test_set_inputs[start_idx:start_idx+batch_size]), \n",
    "                                                      torch.LongTensor(test_set_input_lens[start_idx:start_idx+batch_size]), \n",
    "                                                      torch.LongTensor(test_set_labels[start_idx:start_idx+batch_size])\n",
    "                                                    )\n",
    "        for p in sents_ppl:\n",
    "            ppl_sum+=p\n",
    "            all_ppl.append(p)\n",
    "        \n",
    "    return ppl_sum/dataset_size, all_ppl\n",
    "\n",
    "batch_size = 200\n",
    "dataset_size=len(split_valid_set_inputs)\n",
    "ppl_mean, all_ppl = model_test(language_model, batch_size, dataset_size, split_valid_set_inputs, split_valid_set_input_lens, split_valid_set_labels)\n",
    "print(ppl_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mess sents up. \n",
      "904.5610524456024\n"
     ]
    }
   ],
   "source": [
    "def mess_tokens_up(test_set_inputs, test_set_input_lens, test_set_labels, mess_times):\n",
    "    mess_inputs=copy.deepcopy(test_set_inputs)\n",
    "    mess_labels=copy.deepcopy(test_set_labels)\n",
    "    mess_input_lens=copy.deepcopy(test_set_input_lens)\n",
    "    \n",
    "    for ii, (mess_input, mess_label) in enumerate(zip(mess_inputs, mess_labels)):\n",
    "        for mess_time in range(mess_times):\n",
    "            start_idx=random.randint(1, test_set_input_lens[ii]-1)\n",
    "            end_idx=random.randint(1, test_set_input_lens[ii]-1)\n",
    "            mess_input[start_idx], mess_input[end_idx]=mess_input[end_idx], mess_input[start_idx]\n",
    "            mess_label[start_idx-1], mess_label[end_idx-1]=mess_label[end_idx-1], mess_label[start_idx-1]\n",
    "    return mess_inputs, mess_input_lens, mess_labels\n",
    "\n",
    "mess_inputs, mess_input_lens, mess_labels = mess_tokens_up(split_valid_set_inputs, split_valid_set_input_lens, \n",
    "                                                           split_valid_set_labels, mess_times=2)\n",
    "print('mess sents up. ')\n",
    "\n",
    "ppl_mean, all_ppl = model_test(language_model, batch_size, dataset_size, mess_inputs, mess_input_lens, mess_labels)\n",
    "print(ppl_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0347608346071 0.0139677994815\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "x = scipy.array(all_ppl)\n",
    "y = scipy.array(split_valid_set_input_lens)\n",
    "\n",
    "r_row, p_value = pearsonr(x, y)\n",
    "print(r_row, p_value)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
