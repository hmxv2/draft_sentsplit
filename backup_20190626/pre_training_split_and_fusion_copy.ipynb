{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    " \n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "# torch.cuda.set_device(0)\n",
    "\n",
    "print('import over')\n",
    "\n",
    "copy_thres=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [' '.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def seqs_split(seqs, vocab):\n",
    "    seqs = batch_tokens_remove_eos(seqs, vocab)\n",
    "    simple_sent1s=[]\n",
    "    simple_sent2s=[]\n",
    "    for seq in seqs:\n",
    "        simple_sent1=[]\n",
    "        simple_sent2=[]\n",
    "        sent=simple_sent1\n",
    "        for token in seq:\n",
    "            if token==vocab.word2token['<split>']:\n",
    "                sent=simple_sent2\n",
    "            else:\n",
    "                sent.append(token)\n",
    "        simple_sent1s.append(simple_sent1)\n",
    "        simple_sent2s.append(simple_sent2)\n",
    "        \n",
    "    return simple_sent1s, simple_sent2s\n",
    "\n",
    "def batch_tokens_bleu_split_version(references, candidates, vocab, smooth_epsilon=0.001):\n",
    "    # needn't remove '<sos>' token before calling this function, which is different from the 'batch_token_bleu()' version\n",
    "    #\n",
    "    ref1, ref2 = seqs_split(references, vocab)\n",
    "    cand1, cand2 = seqs_split(candidates, vocab)\n",
    "    bleu_simple_sent1s = batch_tokens_bleu(ref1, cand1)\n",
    "    bleu_simple_sent2s = batch_tokens_bleu(ref2, cand2)\n",
    "#     print(bleu_simple_sent1s)\n",
    "#     print(bleu_simple_sent2s)\n",
    "    bleu=[]\n",
    "    for idx in range(len(bleu_simple_sent1s)):\n",
    "        bleu.append((bleu_simple_sent1s[idx]+bleu_simple_sent2s[idx])/2)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "\n",
    "with open('data_set/vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data_set/fusion_data_set/train_pseudo_simple_sents.pk', 'rb') as f:\n",
    "    fusion_pseudo_train_set_inputs = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/train_pseudo_simple_sent_lens.pk', 'rb') as f:\n",
    "    fusion_pseudo_train_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/train_pseudo_labels.pk', 'rb') as f:\n",
    "    fusion_pseudo_train_set_labels = pickle.load(f)\n",
    "    \n",
    "with open('./data_set/fusion_data_set/validation_simple_sents.pk', 'rb') as f:\n",
    "    fusion_pseudo_valid_set_inputs = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/validation_simple_sent_lens.pk', 'rb') as f:\n",
    "    fusion_pseudo_valid_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/validation_labels.pk', 'rb') as f:\n",
    "    fusion_pseudo_valid_set_labels = pickle.load(f)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "with open('./data_set/split_data_set/train_complex_sents.pk', 'rb') as f:\n",
    "    split_train_set_inputs = pickle.load(f)\n",
    "with open('./data_set/split_data_set/train_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_train_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/split_data_set/train_pseudo_labels.pk', 'rb') as f:\n",
    "    split_pseudo_train_set_labels = pickle.load(f)\n",
    "    \n",
    "with open('./data_set/split_data_set/validation_complex_sents.pk', 'rb') as f:\n",
    "    split_valid_set_inputs = pickle.load(f)\n",
    "with open('./data_set/split_data_set/validation_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_valid_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/split_data_set/validation_labels.pk', 'rb') as f:\n",
    "    split_pseudo_valid_set_labels = pickle.load(f)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989944 989944 989944\n",
      "989944 989944 989944\n"
     ]
    }
   ],
   "source": [
    "print(len(split_train_set_inputs), len(split_train_set_input_lens), len(split_pseudo_train_set_labels))\n",
    "print(len(fusion_pseudo_train_set_inputs), len(fusion_pseudo_train_set_input_lens), len(fusion_pseudo_train_set_labels))\n",
    "# for sent_len in valid_set_input_lens:\n",
    "#     if sent_len<=2:\n",
    "#         print('why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('data_set/pre_trained_token_embedding.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "#         self.embed.weight.requires_grad = False\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "        #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "            cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "            cn = cn.index_select(0, Variable(true_order_ids))\n",
    "            \n",
    "        return outputs, (hn,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _inflate(tensor, times, dim):\n",
    "    \"\"\"\n",
    "    Examples::\n",
    "        >> a = torch.LongTensor([[1, 2], [3, 4]])\n",
    "        >> a\n",
    "        1   2\n",
    "        3   4\n",
    "        [torch.LongTensor of size 2x2]\n",
    "        >> b = ._inflate(a, 2, dim=1)\n",
    "        >> b\n",
    "        1   2   1   2\n",
    "        3   4   3   4\n",
    "        [torch.LongTensor of size 2x4]\n",
    "    \"\"\"\n",
    "    repeat_dims = [1] * tensor.dim()\n",
    "    repeat_dims[dim] = times\n",
    "    return tensor.repeat(*repeat_dims)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, use_cuda, encoder, hidden_dim, max_length=25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.input_dim = encoder.input_dim\n",
    "        self.max_length = max_length\n",
    "        self.vocab = encoder.vocab\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        #self.weight[self.vocab.word2token['<eos>']]=1.01\n",
    "        #self.weight[self.vocab.word2token['<split>']]=1.01\n",
    "        \n",
    "        self.hidden_size = self.hidden_dim\n",
    "        self.V = len(self.vocab.word2token)\n",
    "        self.SOS = self.vocab.word2token['<sos>']\n",
    "        self.EOS = self.vocab.word2token['<eos>']\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lstmcell = torch.nn.LSTMCell(input_size=self.input_dim, hidden_size=self.hidden_dim*2, bias=True)\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=encoder.embed# reference share\n",
    "        #fcnn: projection for crossentroy loss\n",
    "        self.fcnn = nn.Linear(in_features = self.hidden_dim*2, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.cost_func = nn.CrossEntropyLoss(weight=torch.Tensor(self.weight), reduce=False)\n",
    "        self.nll_loss = nn.NLLLoss(weight=torch.Tensor(self.weight), reduce=False)\n",
    "\n",
    "        print('init lookup embedding matrix size: ', self.embed.weight.data.size())\n",
    "        \n",
    "        #copy\n",
    "        out_features_dim=self.hidden_dim\n",
    "        self.attent_wh = nn.Linear(in_features = self.hidden_dim*2, out_features = out_features_dim, bias = 0)\n",
    "        self.attent_ws = nn.Linear(in_features = self.hidden_dim*2, out_features = out_features_dim, bias = 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.attent_vt = nn.Linear(in_features = out_features_dim, out_features = 1, bias=0)\n",
    "        \n",
    "        self.prob_wh = nn.Linear(in_features = self.hidden_dim*2, out_features = 1, bias=0)\n",
    "        self.prob_ws = nn.Linear(in_features = self.hidden_dim*2, out_features = 1, bias=0)\n",
    "        self.prob_wx = nn.Linear(in_features = self.input_dim, out_features = 1, bias=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def copy_mechanism(self, enc_outputs, this_timestep_input, dec_state, inputs_one_hot):\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        \n",
    "        wh = self.attent_wh(enc_outputs)\n",
    "        ws = self.attent_ws(dec_state).unsqueeze(dim=1)\n",
    "#         print('wh, ws size: ', wh.size(), ws.size())\n",
    "        ws = ws.expand(ws.size(0), wh.size(1), ws.size(2))\n",
    "#         print('ws size: ', ws.size())\n",
    "        weight = self.attent_vt(self.tanh(wh+ws))\n",
    "#         print('weight size: ', weight.size())\n",
    "        weight = self.softmax(weight.squeeze(dim=2))\n",
    "#         print('weight size: ', weight.size())\n",
    "        context_v = torch.bmm(weight.unsqueeze(dim=1), enc_outputs)\n",
    "#         print('context_v size: ', context_v.size())\n",
    "        context_v = context_v.squeeze(dim=1)\n",
    "        \n",
    "        p_wh = self.prob_wh(context_v)\n",
    "        p_ws = self.prob_ws(dec_state)\n",
    "        p_wx = self.prob_wx(this_timestep_input)\n",
    "        if_copy = self.sigmoid(p_wh+p_ws+p_wx)\n",
    "#         if_copy = 0.3*if_copy\n",
    "#         if_copy = self._tocuda(Variable(torch.ones(batch_size, 1), requires_grad=0))\n",
    "#         print('if_copy size: ', if_copy.size())\n",
    "        \n",
    "        prob_copy = torch.bmm(inputs_one_hot, weight.unsqueeze(dim=2))\n",
    "        prob_copy = prob_copy.squeeze(dim=2)\n",
    "#         prob_copy = self._tocuda(Variable(torch.rand(batch_size, len(self.vocab.word2token)), requires_grad=0))\n",
    "#         prob_copy = self.softmax(prob_copy)\n",
    "\n",
    "#         print('prob_copy size: ', prob_copy.size())\n",
    "#         print(torch.sum(prob_copy, dim=1))\n",
    "#         print(torch.mean(if_copy))\n",
    "        \n",
    "#         if random.random()<0.005:\n",
    "#             print('if_copy mean: ', torch.mean(if_copy))\n",
    "#             _, max_ids = torch.max(prob_copy, dim=1)\n",
    "#             print(self.vocab.token2word[max_ids.data[0]], self.vocab.token2word[max_ids.data[1]], self.vocab.token2word[max_ids.data[2]])\n",
    "            \n",
    "            \n",
    "        return if_copy, prob_copy\n",
    "\n",
    "    def forward(self, enc_outputs, sent_lens, h0_and_c0, labels, inputs, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = 0\n",
    "        predicts = []\n",
    "        max_probs=[]\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "#         print('enc_outputs size:', enc_outputs.size())\n",
    "\n",
    "        sents_len = enc_outputs.size(1)\n",
    "        inputs = inputs[:,:sents_len].unsqueeze(dim=2)\n",
    "        one_hot = torch.FloatTensor(batch_size, sents_len, len(self.vocab.word2token)).zero_()\n",
    "        one_hot.scatter_(2, inputs, 1)\n",
    "        one_hot = one_hot.transpose(1,2)\n",
    "        one_hot = self._tocuda(Variable(one_hot, requires_grad = 0))\n",
    "#         print('one_hot size: ', one_hot.size())\n",
    "        \n",
    "        for ii in range(self.max_length):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('last_timestep_hidden_state: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                \n",
    "                logits = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                #copy or not\n",
    "                copy_control=random.random()\n",
    "                if copy_control<copy_thres:\n",
    "                    if_copy, prob_copy = self.copy_mechanism(enc_outputs=enc_outputs, this_timestep_input=zero_timestep_input, \n",
    "                                                            dec_state = last_timestep_hidden_state, inputs_one_hot = one_hot)\n",
    "                    score = (1-if_copy)*self.softmax(logits)+if_copy*prob_copy\n",
    "                    score = torch.clamp(score, min=10**(-30), max=1)\n",
    "                \n",
    "                #for saving time: no training, no loss calculating\n",
    "                if is_train:\n",
    "                    if copy_control<copy_thres:\n",
    "                        loss = self.nll_loss(torch.log(score), labels[:,0])\n",
    "                    else:\n",
    "                        loss = self.cost_func(logits, labels[:,0])\n",
    "                    all_loss+=loss\n",
    "                \n",
    "                #get predicts\n",
    "                if copy_control<copy_thres:\n",
    "                    _, max_idxs = torch.max(score, dim=1)\n",
    "                else:\n",
    "                    _, max_idxs = torch.max(logits, dim=1)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                \n",
    "                \n",
    "                logits = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                #copy or not\n",
    "                copy_control=random.random()\n",
    "                if copy_control<copy_thres:\n",
    "                    if_copy, prob_copy = self.copy_mechanism(enc_outputs=enc_outputs, this_timestep_input=this_timestep_input, \n",
    "                                                            dec_state = last_timestep_hidden_state, inputs_one_hot = one_hot)\n",
    "                    score = (1-if_copy)*self.softmax(logits)+if_copy*prob_copy\n",
    "                    score = torch.clamp(score, min=10**(-30), max=1)\n",
    "                \n",
    "                #for saving time: no training, no loss calculating\n",
    "                if is_train:\n",
    "                    if copy_control<copy_thres:\n",
    "                        loss = self.nll_loss(torch.log(score), labels[:,ii])\n",
    "                    else:\n",
    "                        loss = self.cost_func(logits, labels[:,ii])\n",
    "                    all_loss+=loss\n",
    "                \n",
    "                #get predicts\n",
    "                if copy_control<copy_thres:\n",
    "                    _, max_idxs = torch.max(score, dim=1)\n",
    "                else:\n",
    "                    _, max_idxs = torch.max(logits, dim=1)\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "    \n",
    "        if is_train:  #training\n",
    "#             all_loss = torch.cat(all_loss, dim=1)\n",
    "#             all_loss = torch.mean(all_loss, dim=1)\n",
    "#             loss = torch.mean(all_loss)\n",
    "            loss = all_loss/self.max_length\n",
    "            #print('loss size: ', loss.size())\n",
    "            #torch.cuda.empty_cache()\n",
    "            if self.use_cuda:\n",
    "                return loss, predicts.data.cpu().tolist()\n",
    "            else:\n",
    "                return loss, predicts.data.tolist()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().tolist()\n",
    "            else:\n",
    "                return predicts.data.tolist()\n",
    "#         if is_train:  #training\n",
    "#             if self.use_cuda:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.numpy()\n",
    "#         else:   #testing\n",
    "#             if self.use_cuda:\n",
    "#                 return predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return predicts.data.numpy()\n",
    "    \n",
    "    \n",
    "    def decode_topk_seqs(self, inputs, input_lens, topk=3):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(inputs, input_lens)\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        metadata = self.decode_by_beamsearch(encoder_hidden=(enc_hn, enc_cn), encoder_outputs=enc_outputs, topk = topk)\n",
    "        results = metadata['topk_sequence']\n",
    "        results =torch.cat(results, dim = 2)\n",
    "        results=results.view(batch_size*topk, -1)\n",
    "        if self.use_cuda:\n",
    "            results = results.data.cpu().tolist()\n",
    "        else:\n",
    "            results = results.data.tolist()\n",
    "#         results=batch_tokens_remove_eos(results, self.vocab)\n",
    "\n",
    "#         labels = [x for x in labels for ii in range(topk)]\n",
    "#         labels = batch_tokens_remove_eos(labels, self.vocab)\n",
    "#         bleu_scores = batch_tokens_bleu(references=labels, candidates=results, smooth_epsilon=0.01)\n",
    "        \n",
    "#         bleu_scores = torch.FloatTensor(bleu_scores).view(batch_size, topk)\n",
    "#         bleu_max, _ = torch.max(bleu_scores, dim=1)\n",
    "        \n",
    "#         bleu_mean = torch.mean(bleu_scores, dim=1).unsqueeze(dim=1)\n",
    "#         bleu_scores = bleu_scores-bleu_mean\n",
    "#         bleu_scores = bleu_scores.view(-1)\n",
    "        \n",
    "#         bleu_scores = self._tocuda(Variable(bleu_scores, requires_grad = 0))\n",
    "#         log_probs = metadata['score']\n",
    "#         log_probs = log_probs.view(batch_size*topk)\n",
    "#         loss = -torch.dot(log_probs, bleu_scores)/batch_size/topk\n",
    "#         return loss, results, torch.mean(bleu_mean.squeeze()), torch.mean(bleu_max)\n",
    "\n",
    "        log_probs = metadata['score']\n",
    "        log_probs = log_probs.view(batch_size*topk)\n",
    "        \n",
    "        return results, log_probs\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var\n",
    "    def decode_by_beamsearch(self, encoder_hidden=None, encoder_outputs=None, topk = 10):\n",
    "        self.k = topk\n",
    "        batch_size = encoder_outputs.size(dim=0)\n",
    "        \n",
    "        self.pos_index = self._tocuda(Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1))\n",
    "\n",
    "        hidden = tuple([_inflate(h, self.k, 1).view(batch_size*self.k, -1) for h in encoder_hidden])\n",
    "        #print('hidden0 size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "\n",
    "        # Initialize the scores; for the first step,\n",
    "        # ignore the inflated copies to avoid duplicate entries in the top k\n",
    "        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n",
    "        sequence_scores.fill_(-float('Inf'))\n",
    "        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n",
    "        sequence_scores = self._tocuda(Variable(sequence_scores))\n",
    "\n",
    "        # Initialize the input vector\n",
    "        input_var = self._tocuda(Variable(torch.LongTensor([self.SOS] * batch_size * self.k)))\n",
    "\n",
    "        # Store decisions for backtracking\n",
    "        stored_outputs = list()\n",
    "        stored_scores = list()\n",
    "        stored_predecessors = list()\n",
    "        stored_emitted_symbols = list()\n",
    "        stored_hidden = list()\n",
    "\n",
    "        for ii in range(0, self.max_length):\n",
    "            # Run the RNN one step forward\n",
    "            #print('setp: %s'%ii)\n",
    "            input_vec = self.embed(input_var)\n",
    "            #print('input_var and input_vec size: ', input_var.size(), input_vec.size())\n",
    "            hidden = self.lstmcell(input_vec, hidden)\n",
    "            #print('hidden size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "            \n",
    "            log_softmax_output = self.log_softmax(self.fcnn(hidden[0]))\n",
    "\n",
    "            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n",
    "            sequence_scores = _inflate(sequence_scores, self.V, 1)\n",
    "            sequence_scores += log_softmax_output.squeeze(1)\n",
    "            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n",
    "\n",
    "            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n",
    "            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n",
    "            sequence_scores = scores.view(batch_size * self.k, 1)\n",
    "\n",
    "            # Update fields for next timestep\n",
    "            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple([h.index_select(0, predecessors.squeeze()) for h in hidden])\n",
    "            else:\n",
    "                hidden = hidden.index_select(0, predecessors.squeeze())\n",
    "\n",
    "            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n",
    "            stored_scores.append(sequence_scores.clone())\n",
    "            eos_indices = input_var.data.eq(self.EOS)\n",
    "            if eos_indices.nonzero().dim() > 0:\n",
    "                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n",
    "\n",
    "            # Cache results for backtracking\n",
    "            stored_predecessors.append(predecessors)\n",
    "            stored_emitted_symbols.append(input_var)\n",
    "#             stored_hidden.append(hidden)\n",
    "\n",
    "        # Do backtracking to return the optimal values\n",
    "        output, h_t, h_n, s, l, p = self._backtrack(hidden,\n",
    "                                                    stored_predecessors, stored_emitted_symbols,\n",
    "                                                    stored_scores, batch_size, self.hidden_size)\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        metadata['score'] = s\n",
    "        metadata['topk_length'] = l\n",
    "        metadata['topk_sequence'] = p\n",
    "        metadata['length'] = [seq_len[0] for seq_len in l]\n",
    "        metadata['sequence'] = [seq[0] for seq in p]\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def _backtrack(self, hidden, predecessors, symbols, scores, b, hidden_size):\n",
    "        \"\"\"Backtracks over batch to generate optimal k-sequences.\n",
    "\n",
    "        Args:\n",
    "            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n",
    "            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n",
    "            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n",
    "            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n",
    "            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n",
    "            b: Size of the batch\n",
    "            hidden_size: Size of the hidden state\n",
    "\n",
    "        Returns:\n",
    "            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n",
    "\n",
    "            score [batch, k]: A list containing the final scores for all top-k sequences\n",
    "\n",
    "            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n",
    "\n",
    "            p (batch, k, sequence_len): A Tensor containing predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        lstm = isinstance(hidden, tuple)\n",
    "\n",
    "        # initialize return variables given different types\n",
    "        output = list()\n",
    "        h_t = list()\n",
    "        p = list()\n",
    "        # Placeholder for last hidden state of top-k sequences.\n",
    "        # If a (top-k) sequence ends early in decoding, `h_n` contains\n",
    "        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n",
    "        # the last hidden state of decoding.\n",
    "        if lstm:\n",
    "            state_size = hidden[0].size()\n",
    "            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n",
    "        else:\n",
    "            h_n = torch.zeros(nw_hidden[0].size())\n",
    "        l = [[self.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n",
    "                                                                # Similar to `h_n`\n",
    "\n",
    "        # the last step output of the beams are not sorted\n",
    "        # thus they are sorted here\n",
    "        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n",
    "        # initialize the sequence scores with the sorted last step beam scores\n",
    "        s = sorted_score.clone()\n",
    "\n",
    "        batch_eos_found = [0] * b   # the number of EOS found\n",
    "                                    # in the backward loop below for each batch\n",
    "\n",
    "        t = self.max_length - 1\n",
    "        # initialize the back pointer with the sorted order of the last step beams.\n",
    "        # add self.pos_index for indexing variable with b*k as the first dimension.\n",
    "        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n",
    "        while t >= 0:\n",
    "            # Re-order the variables with the back pointer\n",
    "            current_symbol = symbols[t].index_select(0, t_predecessors)\n",
    "            # Re-order the back pointer of the previous step with the back pointer of\n",
    "            # the current step\n",
    "            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n",
    "\n",
    "            # This tricky block handles dropped sequences that see EOS earlier.\n",
    "            # The basic idea is summarized below:\n",
    "            #\n",
    "            #   Terms:\n",
    "            #       Ended sequences = sequences that see EOS early and dropped\n",
    "            #       Survived sequences = sequences in the last step of the beams\n",
    "            #\n",
    "            #       Although the ended sequences are dropped during decoding,\n",
    "            #   their generated symbols and complete backtracking information are still\n",
    "            #   in the backtracking variables.\n",
    "            #   For each batch, everytime we see an EOS in the backtracking process,\n",
    "            #       1. If there is survived sequences in the return variables, replace\n",
    "            #       the one with the lowest survived sequence score with the new ended\n",
    "            #       sequences\n",
    "            #       2. Otherwise, replace the ended sequence with the lowest sequence\n",
    "            #       score with the new ended sequence\n",
    "            #\n",
    "            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n",
    "            if eos_indices.dim() > 0:\n",
    "                for i in range(eos_indices.size(0)-1, -1, -1):\n",
    "                    # Indices of the EOS symbol for both variables\n",
    "                    # with b*k as the first dimension, and b, k for\n",
    "                    # the first two dimensions\n",
    "                    idx = eos_indices[i]\n",
    "                    b_idx = int(idx[0] / self.k)\n",
    "                    # The indices of the replacing position\n",
    "                    # according to the replacement strategy noted above\n",
    "                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n",
    "                    batch_eos_found[b_idx] += 1\n",
    "                    res_idx = b_idx * self.k + res_k_idx\n",
    "\n",
    "                    # Replace the old information in return variables\n",
    "                    # with the new ended sequence information\n",
    "                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n",
    "\n",
    "                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n",
    "                    s[b_idx, res_k_idx] = scores[t][idx[0]]\n",
    "                    l[b_idx][res_k_idx] = t + 1\n",
    "\n",
    "            # record the back tracked results\n",
    "            p.append(current_symbol)\n",
    "            t -= 1\n",
    "\n",
    "        # Sort and re-order again as the added ended sequences may change\n",
    "        # the order (very unlikely)\n",
    "        s, re_sorted_idx = s.topk(self.k)\n",
    "        for b_idx in range(b):\n",
    "            l[b_idx] = [l[b_idx][k_idx.data[0]] for k_idx in re_sorted_idx[b_idx,:]]\n",
    "\n",
    "        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n",
    "\n",
    "        # Reverse the sequences and re-order at the same time\n",
    "        # It is reversed because the backtracking happens in reverse time order\n",
    "#         output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n",
    "        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n",
    "        #    --- fake output ---\n",
    "        output = None\n",
    "        #    --- fake ---\n",
    "        return output, h_t, h_n, s, l, p\n",
    "\n",
    "    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n",
    "            score[idx] = masking_score\n",
    "\n",
    "    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n",
    "        if len(idx.size()) > 0:\n",
    "            indices = idx[:, 0]\n",
    "            tensor.index_fill_(dim, indices, masking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, use_cuda, input_dim, hidden_dim, vocab, max_length = 25):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.enc = Encoder(use_cuda=use_cuda, hidden_dim=hidden_dim, input_dim=input_dim, vocab=vocab)\n",
    "        self.dec = Decoder(use_cuda=use_cuda, encoder=self.enc, hidden_dim=hidden_dim, max_length=max_length)\n",
    "        if use_cuda:\n",
    "            self.enc = self.enc.cuda()\n",
    "            self.dec = self.dec.cuda()\n",
    "    def forward(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        if is_train:\n",
    "            loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                    h0_and_c0=(enc_hn, enc_cn), \n",
    "                                    sent_lens=input_lens,\n",
    "                                    labels=torch.LongTensor(labels), \n",
    "                                    is_train=1, \n",
    "                                    teaching_rate = 1,\n",
    "                                    inputs = inputs\n",
    "                                    )\n",
    "            return loss, predicts\n",
    "        else:\n",
    "            predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                sent_lens=input_lens,\n",
    "                                labels=torch.LongTensor(labels), \n",
    "                                is_train=0, \n",
    "                                teaching_rate = 1,\n",
    "                                inputs = inputs\n",
    "                                )\n",
    "            return predicts\n",
    "#     def train_using_rl(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "#         enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "#         loss, predicts, bleu_mean = self.dec.train_using_rl_2(enc_outputs = enc_outputs, \n",
    "#                                                 h0_and_c0=(enc_hn, enc_cn), \n",
    "#                                                 sent_lens=input_lens,\n",
    "#                                                 labels=labels,\n",
    "#                                                 is_train=1, \n",
    "#                                                 teaching_rate = 1\n",
    "#                                                 )\n",
    "#         return loss, predicts, bleu_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use_cuda = 1\n",
    "# hidden_dim = 256\n",
    "# input_dim = 100\n",
    "# sample_num = 11\n",
    "\n",
    "# split_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "#                           vocab = vocab, max_length = 61)\n",
    "# split_optimizer = optim.Adam(filter(lambda p: p.requires_grad, split_model.parameters()), lr=0.005)\n",
    "\n",
    "# if use_cuda:\n",
    "#     split_model = split_model.cuda()\n",
    "    \n",
    "# a = time.time()\n",
    "\n",
    "# split_optimizer.zero_grad()#clear\n",
    "# split_loss, predicts = split_model.forward(torch.LongTensor(split_train_set_inputs[0:sample_num]), \n",
    "#                                      torch.LongTensor(split_train_set_input_lens[0:sample_num]), \n",
    "#                                      labels=torch.LongTensor(split_pseudo_train_set_labels[0:sample_num]), \n",
    "#                                      is_train=1, teaching_rate=1)\n",
    "# split_loss.backward()#retain_graph=True)\n",
    "# split_optimizer.step()\n",
    "\n",
    "# print('split_model: loss is %4.7f'%split_loss.data[0])\n",
    "\n",
    "# print(time.time()-a)\n",
    "\n",
    "# # split_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 61)\n",
    "# # a=time.time()\n",
    "# # loss, predicts, train_bleu_mean, train_bleu_max = split_model.train_using_rl(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "# #                                                                              torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "# #                                                                              labels=train_set_labels[0:sample_num], \n",
    "# #                                                                              is_train=1, teaching_rate=1\n",
    "# #                                                                             )\n",
    "# # print('split_model: loss is %4.7f'%loss.data[0], train_bleu_mean)\n",
    "# # print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([44380, 100])\n",
      "init lookup embedding matrix size:  torch.Size([44380, 100])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensors not supported in scatter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-50e0936f7e7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_train_set_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running time: %.2f mins'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-50e0936f7e7b>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(epoch, batch_size, train_set_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m                                      \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_train_set_input_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                      \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_pseudo_train_set_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                                      is_train=1, teaching_rate=1)\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0msplit_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscatter_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34mr\"\"\"Scatter with support for kwargs dictionary\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# None, clearing the cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mscatter_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mscatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tensors not supported in scatter.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscatter_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscatter_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/hmx/anaconda3/lib/python3.5/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mscatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mScatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tensors not supported in scatter.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscatter_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensors not supported in scatter."
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 100\n",
    "lr=0.005\n",
    "batch_size=220\n",
    "split_train_set_size=int(len(split_train_set_inputs)/1)\n",
    "epochs=10000\n",
    "train_bleu_mean=-1\n",
    "train_bleu_max=-1\n",
    "split_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 61)\n",
    "\n",
    "fusion_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 51)\n",
    "#pre train para\n",
    "# pre_train = torch.load('./models_saved/time-[2019-03-08-21-23-06]-info=[pre-trained_split_model-pseudo_label]-loss=0.689030886-bleu=0.5018-hidden_dim=256-input_dim=100-epoch=2-batch_size=120-batch_id=[1-[of]-82]-lr=0.0050', map_location='cpu')\n",
    "# split_model.load_state_dict(pre_train)\n",
    "# pre_train = torch.load('./models_saved/time-[2019-03-08-21-23-08]-info=[pre-trained_fusion_model-pseudo_label]-loss=0.975539625-bleu=0.5113-hidden_dim=256-input_dim=100-epoch=2-batch_size=120-batch_id=[1-[of]-82]-lr=0.0050', map_location='cpu')\n",
    "# fusion_model.load_state_dict(pre_train)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    split_model = split_model.cuda(0)\n",
    "    fusion_model = fusion_model.cuda(1)\n",
    "split_model = DataParallel(split_model,[0,1])\n",
    "fusion_model = DataParallel(fusion_model, [0,1])\n",
    "\n",
    "split_optimizer = optim.Adam(filter(lambda p: p.requires_grad, split_model.parameters()), lr=lr)\n",
    "fusion_optimizer = optim.Adam(filter(lambda p: p.requires_grad, fusion_model.parameters()), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def model_train(epoch, batch_size, train_set_size):\n",
    "    batch_id = 0\n",
    "    valid_bleu = 0\n",
    "    for start_idx in range(0, train_set_size-batch_size+1, batch_size):\n",
    "#         print('batch id: ', batch_id)\n",
    "            \n",
    "        batch_id+=1\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        split_optimizer.zero_grad()#clear  \n",
    "        split_loss, predicts = split_model.forward(torch.LongTensor(split_train_set_inputs[start_idx:end_idx]), \n",
    "                                     torch.LongTensor(split_train_set_input_lens[start_idx:end_idx]), \n",
    "                                     labels=torch.LongTensor(split_pseudo_train_set_labels[start_idx:end_idx]), \n",
    "                                     is_train=1, teaching_rate=1)\n",
    "        split_loss=torch.mean(split_loss)\n",
    "\n",
    "        #optimize\n",
    "        split_loss.backward()#retain_graph=True)\n",
    "        split_optimizer.step()\n",
    "        \n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "        fusion_optimizer.zero_grad()#clear\n",
    "        fusion_loss, predicts = fusion_model.forward(torch.LongTensor(fusion_pseudo_train_set_inputs[start_idx:end_idx]), \n",
    "                                     torch.LongTensor(fusion_pseudo_train_set_input_lens[start_idx:end_idx]), \n",
    "                                     labels=torch.LongTensor(fusion_pseudo_train_set_labels[start_idx:end_idx]), \n",
    "                                     is_train=1, teaching_rate=1)\n",
    "        fusion_loss = torch.mean(fusion_loss)\n",
    "        fusion_loss.backward()#retain_graph=True)\n",
    "        fusion_optimizer.step()\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_id%50==1:\n",
    "            split_model.eval()\n",
    "            fusion_model.eval()\n",
    "            \n",
    "            sample_num = 5\n",
    "            rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "            \n",
    "            print('--------split model training sampling display--------')\n",
    "            #teaching forcing\n",
    "            loss_, predicts = split_model.forward(torch.LongTensor(split_train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(split_train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(split_pseudo_train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            \n",
    "            predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "            labels = batch_tokens_remove_eos(split_pseudo_train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "            predicts = batch_tokens2words(predicts, vocab)\n",
    "            labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "            predicts_sents = batch_words2sentence(predicts)\n",
    "            labels_sents = batch_words2sentence(labels)\n",
    "            \n",
    "            for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "                print(' 1----> ', predict_sent)\n",
    "                print(' 2----> ', label_sent)\n",
    "                print('\\n')\n",
    "                \n",
    "            print('--------fusion model training sampling display--------')\n",
    "            loss_, predicts = fusion_model.forward(torch.LongTensor(fusion_pseudo_train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(fusion_pseudo_train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(fusion_pseudo_train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            \n",
    "            predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "            labels = batch_tokens_remove_eos(fusion_pseudo_train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "            predicts = batch_tokens2words(predicts, vocab)\n",
    "            labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "            predicts_sents = batch_words2sentence(predicts)\n",
    "            labels_sents = batch_words2sentence(labels)\n",
    "            \n",
    "            for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "                print(' 1----> ', predict_sent)\n",
    "                print(' 2----> ', label_sent)\n",
    "                print('\\n')\n",
    "                \n",
    "#             #no teaching forcing\n",
    "            print('----no teaching forcing----')\n",
    "            predicts = split_model.forward(torch.LongTensor(fusion_pseudo_train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(fusion_pseudo_train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=[],\n",
    "                                             is_train=0, teaching_rate=1)\n",
    "            predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "            labels = batch_tokens_remove_eos(split_pseudo_train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "            predicts = batch_tokens2words(predicts, vocab)\n",
    "            labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "            predicts_sents = batch_words2sentence(predicts)\n",
    "            labels_sents = batch_words2sentence(labels)\n",
    "\n",
    "            for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "                print(' 1----> ', predict_sent)\n",
    "                print(' 2----> ', label_sent)\n",
    "                print('\\n')\n",
    "                \n",
    "            info_stamp = 'split_loss={:2.9f}-train_bleu_mean={:2.9f}-train_bleu_max={:2.9f}-batch_size={:n}-epoch={:n}-batch_id=({:n}/{:n})'.format(\n",
    "                              split_loss.data[0], train_bleu_mean, train_bleu_max, batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "            print(info_stamp)\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "# #             #valid_set testing\n",
    "            if batch_id%200==1:\n",
    "                rand_idx=random.randint(0, len(split_valid_set_inputs)-batch_size-1-1)\n",
    "                predicts = split_model.forward(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "                                                 torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "                                                 labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "                                                 is_train=0, teaching_rate=1)\n",
    "#                 predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "#                 labels = batch_tokens_remove_eos(split_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size], vocab)\n",
    "                \n",
    "#                 bleu_scores = batch_tokens_bleu(references=labels, candidates=predicts, smooth_epsilon=0.001)\n",
    "                bleu_scores = batch_tokens_bleu_split_version(references = split_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size],\n",
    "                                                             candidates = predicts,\n",
    "                                                             smooth_epsilon=0.001,\n",
    "                                                             vocab=vocab)\n",
    "\n",
    "                valid_bleu = 0\n",
    "                for x in bleu_scores:\n",
    "                    valid_bleu+=x\n",
    "                valid_bleu/=len(bleu_scores)\n",
    "                       \n",
    "                info_stamp = 'info=[{:s}]-loss={:2.9f}-bleu={:1.4f}-hidden_dim={:n}-input_dim={:n}-epoch={:n}-batch_size={:n}-batch_id=[{:n}-[of]-{:n}]-lr={:1.4f}'.format(\n",
    "                              'pre-trained_split_model-pseudo_label', split_loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "                print(info_stamp, valid_bleu)\n",
    "                now = int(round(time.time()*1000))\n",
    "                time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "                torch.save(split_model.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "\n",
    "\n",
    "                rand_idx=random.randint(0, len(fusion_pseudo_valid_set_inputs)-batch_size-1-1)\n",
    "                predicts = fusion_model.forward(torch.LongTensor(fusion_pseudo_valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "                                                 torch.LongTensor(fusion_pseudo_valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "                                                 labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "                                                 is_train=0, teaching_rate=1)\n",
    "#                 predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "#                 labels = batch_tokens_remove_eos(fusion_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size], vocab)\n",
    "                \n",
    "#                 bleu_scores = batch_tokens_bleu(references=labels, candidates=predicts, smooth_epsilon=0.001)\n",
    "                bleu_scores = batch_tokens_bleu_split_version(references = fusion_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size],\n",
    "                                                             candidates = predicts,\n",
    "                                                             smooth_epsilon=0.001,\n",
    "                                                             vocab=vocab)\n",
    "                valid_bleu = 0\n",
    "                for x in bleu_scores:\n",
    "                    valid_bleu+=x\n",
    "                valid_bleu/=len(bleu_scores)\n",
    "                info_stamp = 'info=[{:s}]-loss={:2.9f}-bleu={:1.4f}-hidden_dim={:n}-input_dim={:n}-epoch={:n}-batch_size={:n}-batch_id=[{:n}-[of]-{:n}]-lr={:1.4f}'.format(\n",
    "                              'pre-trained_fusion_model-pseudo_label', fusion_loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "                print(info_stamp, valid_bleu)\n",
    "                now = int(round(time.time()*1000))\n",
    "                time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "                torch.save(fusion_model.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "#                 torch.cuda.empty_cache()\n",
    "            split_model.train()\n",
    "            fusion_model.train()\n",
    "            \n",
    "for epoch in range(epochs):\n",
    "    model_train(epoch, batch_size, split_train_set_size)\n",
    "    \n",
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### torch.save(fusion_model.state_dict(), ''.join(['./models_saved/', 'time-[2019-02-24-21-50-49]-fusion_model']))\n",
    "### torch.save(split_model.state_dict(), ''.join(['./models_saved/', 'time-[2019-02-24-21-50-49]-split_model']))\n",
    "\n",
    "#300d\n",
    "split_model_path='./models_saved/time-[2019-02-24-14-50-48]-info=[split_model]-loss=1.512540460-bleu=0.0136-hidden_dim=512-input_dim=300-epoch=48-batch_size=200-batch_id=[1-[of]-494]-lr=0.0050'\n",
    "fusion_model_path='./models_saved/time-[2019-02-24-14-50-49]-info=[fusion_model]-loss=1.444324136-bleu=0.0243-hidden_dim=512-input_dim=300-epoch=48-batch_size=200-batch_id=[1-[of]-494]-lr=0.0050'\n",
    "\n",
    "#300d\n",
    "split_model_path='./models_saved/time-[2019-02-24-06-03-39]-info=[split_model]-loss=1.667227983-bleu=0.0144-hidden_dim=512-input_dim=300-epoch=22-batch_size=200-batch_id=[1-[of]-494]-lr=0.0050'\n",
    "fusion_model_path='./models_saved/time-[2019-02-24-06-03-39]-info=[fusion_model]-loss=1.542479157-bleu=0.0147-hidden_dim=512-input_dim=300-epoch=22-batch_size=200-batch_id=[1-[of]-494]-lr=0.0050'\n",
    "\n",
    "#100d\n",
    "# split_model_path='./models_saved/time-[2019-01-24-17-17-59]-info=[split_model]-loss=1.704468250-bleu=0.0485-hidden_dim=256-input_dim=100-epoch=9-batch_size=200-batch_id=[1-[of]-4949]-lr=0.0050'\n",
    "# fusion_model_path='./models_saved/time-[2019-01-24-17-17-59]-info=[fusion_model]-loss=2.401571035-bleu=0.0334-hidden_dim=256-input_dim=100-epoch=9-batch_size=200-batch_id=[1-[of]-4949]-lr=0.0050'\n",
    "\n",
    "#100d\n",
    "split_model_path = './models_saved/time-[2019-02-27-00-35-30]-info=[split_model]-loss=1.177916527-bleu=0.0128-hidden_dim=256-input_dim=100-epoch=0-batch_size=150-batch_id=[1001-[of]-6599]-lr=0.0050'\n",
    "fusion_model_path = './models_saved/time-[2019-02-24-21-50-49]-fusion_model'\n",
    "\n",
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 100\n",
    "lr=0.005\n",
    "batch_size=200\n",
    "split_train_set_size=int(len(split_train_set_inputs)/10)\n",
    "epochs=10000\n",
    "train_bleu_mean=-1\n",
    "train_bleu_max=-1\n",
    "split_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 61)\n",
    "\n",
    "fusion_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 51)\n",
    "#pre train para\n",
    "pre_train = torch.load(split_model_path, map_location='cpu')\n",
    "split_model.load_state_dict(pre_train)\n",
    "pre_train = torch.load(fusion_model_path, map_location='cpu')\n",
    "fusion_model.load_state_dict(pre_train)\n",
    "\n",
    "if use_cuda:\n",
    "    split_model = split_model.cuda()\n",
    "    fusion_model = fusion_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copy_thres=1\n",
    "rand_idx=0\n",
    "sample_num=80\n",
    "\n",
    "loss_, predicts = split_model.forward(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(split_pseudo_valid_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "del loss_\n",
    "\n",
    "predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "labels = batch_tokens_remove_eos(split_pseudo_valid_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "\n",
    "bleu_scores = batch_tokens_bleu(references=labels, candidates=predicts, smooth_epsilon=0.001)\n",
    "\n",
    "train_bleu = 0\n",
    "for x in bleu_scores:\n",
    "    train_bleu+=x\n",
    "train_bleu/=len(bleu_scores)\n",
    "print(train_bleu)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
