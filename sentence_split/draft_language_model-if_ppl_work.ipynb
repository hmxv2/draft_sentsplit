{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [' '.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('data_set/vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989944 989944\n",
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "def get_labels(sents, lens, vocab):\n",
    "    labels = copy.deepcopy(sents)\n",
    "    for idx, sent in enumerate(labels):\n",
    "        sent.insert(lens[idx], vocab.word2token['<eos>'])\n",
    "        \n",
    "    return labels\n",
    "\n",
    "def get_inputs(sents, lens, vocab):\n",
    "    inputs = copy.deepcopy(sents)\n",
    "    lens_=copy.deepcopy(lens)\n",
    "    \n",
    "    for sent in inputs:\n",
    "        sent.insert(0, vocab.word2token['<sos>'])\n",
    "    for idx in range(len(lens_)):\n",
    "        lens_[idx]+=1\n",
    "    \n",
    "    return inputs, lens_\n",
    "\n",
    "with open('./data_set/split_data_set/train_complex_sents.pk', 'rb') as f:\n",
    "    split_train_set_inputs = pickle.load(f)\n",
    "with open('./data_set/split_data_set/train_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_train_set_input_lens = pickle.load(f)\n",
    "    \n",
    "    \n",
    "with open('./data_set/split_data_set/validation_complex_sents.pk', 'rb') as f:\n",
    "    split_valid_set_inputs = pickle.load(f)\n",
    "with open('./data_set/split_data_set/validation_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_valid_set_input_lens = pickle.load(f)\n",
    "\n",
    "\n",
    "split_train_set_labels = get_labels(split_train_set_inputs, split_train_set_input_lens, vocab)\n",
    "split_train_set_inputs, split_train_set_input_lens = get_inputs(split_train_set_inputs, split_train_set_input_lens, vocab)\n",
    "print(len(split_train_set_inputs), len(split_train_set_input_lens))\n",
    "\n",
    "split_valid_set_labels = get_labels(split_valid_set_inputs, split_valid_set_input_lens, vocab)\n",
    "split_valid_set_inputs, split_valid_set_input_lens = get_inputs(split_valid_set_inputs, split_valid_set_input_lens, vocab)\n",
    "print(len(split_valid_set_inputs), len(split_valid_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=False,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('data_set/pre_trained_token_embedding_300d.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "#         self.embed.weight.requires_grad = False\n",
    "\n",
    "\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        self.cost_func = nn.CrossEntropyLoss(weight=torch.Tensor(self.weight), reduce=True)\n",
    "        self.fcnn=nn.Linear(in_features = self.hidden_dim, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "#         hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "#         cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "#         #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "#         if self.use_cuda:\n",
    "#             hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "#             cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "#         else:\n",
    "#             hn = hn.index_select(0, Variable(true_order_ids))\n",
    "#             cn = cn.index_select(0, Variable(true_order_ids))\n",
    "        logits = self.fcnn(outputs)\n",
    "        return logits\n",
    "    \n",
    "    def get_loss(self, logits, labels):\n",
    "        labels = self._tocuda(Variable(labels))\n",
    "        sent_len = logits.size(dim=1)\n",
    "        labels = labels[:, :sent_len]\n",
    "        labels = labels.contiguous().view(-1)\n",
    "        logits = logits.view(-1, len(self.vocab.word2token))\n",
    "#         print('logits size: ', logits.size())\n",
    "        \n",
    "        return self.cost_func(logits, labels)\n",
    "    \n",
    "    def get_sentences_ppl(self, inputs, inputs_len, labels):\n",
    "        \n",
    "        vocab_size=len(self.vocab.word2token)\n",
    "        batch_size=inputs.size(0)\n",
    "        \n",
    "        logits = self.forward(inputs, inputs_len)\n",
    "        \n",
    "        labels = self._tocuda(Variable(labels))\n",
    "        sent_len = logits.size(dim=1)\n",
    "        labels = labels[:, :sent_len]\n",
    "        labels = labels.contiguous()\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        log_probs= self.log_softmax(logits).view(-1)\n",
    "        \n",
    "        pos_bias=torch.LongTensor([i*vocab_size for i in range(sent_len)]).view(1,-1)\n",
    "        pos_bias = pos_bias.expand(batch_size, pos_bias.size(1))\n",
    "#         print(pos_bias.size())\n",
    "\n",
    "        batch_bias = torch.LongTensor([i*vocab_size*sent_len for i in range(batch_size)]).view(-1,1)\n",
    "        batch_bias = batch_bias.expand(batch_bias.size(0), sent_len)\n",
    "#         print(batch_bias.size())\n",
    "\n",
    "        pos_bias = self._tocuda(Variable(pos_bias, requires_grad=0))\n",
    "        batch_bias = self._tocuda(Variable(batch_bias, requires_grad=0))\n",
    "        \n",
    "        indices=labels+pos_bias+batch_bias\n",
    "        indices = indices.view(-1)\n",
    "\n",
    "        results=log_probs[indices].view(batch_size, sent_len)\n",
    "        \n",
    "        sents_ppl=[]\n",
    "        for idx, result in enumerate(results):\n",
    "            mean_log_prob = torch.mean(result[:inputs_len[idx]])\n",
    "            sents_ppl.append(mean_log_prob)\n",
    "\n",
    "        sents_ppl = torch.cat(sents_ppl, dim=0)\n",
    "        sents_ppl = torch.exp(-sents_ppl)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            return sents_ppl.cpu().data.tolist()\n",
    "        else:\n",
    "            return sents_ppl.data.tolist()\n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_labels(sents, lens, vocab):\n",
    "#     labels = copy.deepcopy(sents)\n",
    "#     for idx, sent in enumerate(labels):\n",
    "#         sent.insert(lens[idx], vocab.word2token['<eos>'])\n",
    "        \n",
    "#     return labels\n",
    "\n",
    "# def get_inputs(sents, lens, vocab):\n",
    "#     inputs = copy.deepcopy(sents)\n",
    "#     lens_=copy.deepcopy(lens)\n",
    "    \n",
    "#     for sent in inputs:\n",
    "#         sent.insert(0, vocab.word2token['<sos>'])\n",
    "#     for idx in range(len(lens_)):\n",
    "#         lens_[idx]+=1\n",
    "    \n",
    "#     return inputs, lens_\n",
    "\n",
    "# id1=random.randint(0, len(split_train_set_inputs)-1)\n",
    "# id2=random.randint(0, len(split_train_set_inputs)-1)\n",
    "# sents=[split_train_set_inputs[id1], split_train_set_inputs[id2]]\n",
    "# lens=[split_train_set_input_lens[id1], split_train_set_input_lens[id2]]\n",
    "# print(sents, lens)\n",
    "# labels = get_labels(sents, lens, vocab)\n",
    "# inputs, lens = get_inputs(sents, lens, vocab)\n",
    "# print(inputs, lens)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use_cuda = 1\n",
    "# hidden_dim = 256\n",
    "# input_dim = 100\n",
    "# sample_num = 11\n",
    "\n",
    "# language_model = LanguageModel(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab)\n",
    "# lm_optimizer = optim.Adam(filter(lambda p: p.requires_grad, language_model.parameters()), lr=0.005)\n",
    "\n",
    "# if use_cuda:\n",
    "#     language_model = language_model.cuda()\n",
    "    \n",
    "# a = time.time()\n",
    "\n",
    "# lm_optimizer.zero_grad()#clear\n",
    "# outputs = language_model.forward(torch.LongTensor(split_train_set_inputs[0:sample_num]), \n",
    "#                                              torch.LongTensor(split_train_set_input_lens[0:sample_num])\n",
    "#                                               )\n",
    "# loss = language_model.get_loss(outputs, torch.LongTensor(split_train_set_labels[:sample_num]))\n",
    "# loss.backward()#retain_graph=True)\n",
    "# lm_optimizer.step()\n",
    "\n",
    "# print('language_model: loss is %4.7f'%loss.data[0])\n",
    "\n",
    "# print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use_cuda = 1\n",
    "# hidden_dim = 2048\n",
    "# input_dim = 300\n",
    "# lr=0.005\n",
    "# batch_size=100\n",
    "# split_train_set_size=int(len(split_train_set_inputs)/1)\n",
    "# epochs=10000\n",
    "# train_bleu_mean=-1\n",
    "# train_bleu_max=-1\n",
    "\n",
    "# language_model = LanguageModel(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab)\n",
    "# if use_cuda:\n",
    "#     language_model = language_model.cuda()\n",
    "# #pre train para\n",
    "# # pre_train = torch.load('./models_saved/time-[2019-01-24-17-17-59]-info=[split_model]-loss=1.704468250-bleu=0.0485-hidden_dim=256-input_dim=100-epoch=9-batch_size=200-batch_id=[1-[of]-4949]-lr=0.0050', map_location='cpu')\n",
    "# # split_model.load_state_dict(pre_train)\n",
    "# # pre_train = torch.load('./models_saved/time-[2019-01-24-17-17-59]-info=[fusion_model]-loss=2.401571035-bleu=0.0334-hidden_dim=256-input_dim=100-epoch=9-batch_size=200-batch_id=[1-[of]-4949]-lr=0.0050', map_location='cpu')\n",
    "# # fusion_model.load_state_dict(pre_train)\n",
    "\n",
    "# lm_optimizer = optim.Adam(filter(lambda p: p.requires_grad, language_model.parameters()), lr=0.005)\n",
    "\n",
    "\n",
    "# def model_train(epoch, batch_size, train_set_size):\n",
    "#     batch_id = 0\n",
    "#     valid_bleu = 0\n",
    "#     for start_idx in range(0, train_set_size-batch_size+1, batch_size):\n",
    "# #         print('batch id: ', batch_id)\n",
    "            \n",
    "#         batch_id+=1\n",
    "#         end_idx = start_idx + batch_size\n",
    "        \n",
    "#         #split model\n",
    "#         lm_optimizer.zero_grad()#clear\n",
    "#         outputs = language_model.forward(torch.LongTensor(split_train_set_inputs[start_idx:end_idx]), \n",
    "#                                      torch.LongTensor(split_train_set_input_lens[start_idx:end_idx]),\n",
    "#                                                 )\n",
    "#         loss = language_model.get_loss(outputs, torch.LongTensor(split_train_set_labels[start_idx:end_idx]))\n",
    "        \n",
    "#         #optimize\n",
    "#         loss.backward()#retain_graph=True)\n",
    "#         lm_optimizer.step()\n",
    "        \n",
    "        \n",
    "#         if batch_id%50==1:\n",
    "#             language_model.eval()\n",
    "            \n",
    "#             sample_num = 5\n",
    "#             rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "#             outputs = language_model.forward(torch.LongTensor(split_train_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "#                                      torch.LongTensor(split_train_set_input_lens[rand_idx:rand_idx+batch_size]),\n",
    "#                                                 )\n",
    "#             loss = language_model.get_loss(outputs, torch.LongTensor(split_train_set_labels[rand_idx:rand_idx+batch_size]))\n",
    "                \n",
    "#             info_stamp = 'split_loss={:2.9f}-train_bleu_mean={:2.9f}-train_bleu_max={:2.9f}-batch_size={:n}-epoch={:n}-batch_id=({:n}/{:n})'.format(\n",
    "#                               loss.data[0], train_bleu_mean, train_bleu_max, batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "#             print(info_stamp)\n",
    "            \n",
    "#             if batch_id%50000==1:\n",
    "#                 valid_bleu=-1\n",
    "#                 info_stamp = 'info=[{:s}]-loss={:2.9f}-bleu={:1.4f}-hidden_dim={:n}-input_dim={:n}-epoch={:n}-batch_size={:n}-batch_id=[{:n}-[of]-{:n}]-lr={:1.4f}'.format(\n",
    "#                               'language_model', loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "#                 now = int(round(time.time()*1000))\n",
    "#                 time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "#                 torch.save(language_model.state_dict(), ''.join(['./models_language_model/', time_stamp, info_stamp]))\n",
    "\n",
    "# # #             #valid_set testing\n",
    "# #             if batch_id%1000==1:\n",
    "# #                 rand_idx=random.randint(0, len(split_valid_set_inputs)-batch_size-1-1)\n",
    "# #                 predicts = split_model.forward(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "# #                                                  torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "# #                                                  labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "# #                                                  is_train=0, teaching_rate=1)\n",
    "# #                 predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "# #                 labels = batch_tokens_remove_eos(split_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size], vocab)\n",
    "                \n",
    "# #                 bleu_scores = batch_tokens_bleu(references=labels, candidates=predicts, smooth_epsilon=0.001)\n",
    "\n",
    "# #                 valid_bleu = 0\n",
    "# #                 for x in bleu_scores:\n",
    "# #                     valid_bleu+=x\n",
    "# #                 valid_bleu/=len(bleu_scores)\n",
    "                       \n",
    "# #                 info_stamp = 'info=[{:s}]-loss={:2.9f}-bleu={:1.4f}-hidden_dim={:n}-input_dim={:n}-epoch={:n}-batch_size={:n}-batch_id=[{:n}-[of]-{:n}]-lr={:1.4f}'.format(\n",
    "# #                               'split_model', split_loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "# #                 print(info_stamp, valid_bleu)\n",
    "# #                 now = int(round(time.time()*1000))\n",
    "# #                 time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "# #                 torch.save(split_model.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "\n",
    "\n",
    "# #                 rand_idx=random.randint(0, len(fusion_pseudo_valid_set_inputs)-batch_size-1-1)\n",
    "# #                 predicts = fusion_model.forward(torch.LongTensor(fusion_pseudo_valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "# #                                                  torch.LongTensor(fusion_pseudo_valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "# #                                                  labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "# #                                                  is_train=0, teaching_rate=1)\n",
    "# #                 predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "# #                 labels = batch_tokens_remove_eos(fusion_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size], vocab)\n",
    "                \n",
    "# #                 bleu_scores = batch_tokens_bleu(references=labels, candidates=predicts, smooth_epsilon=0.001)\n",
    "\n",
    "# #                 valid_bleu = 0\n",
    "# #                 for x in bleu_scores:\n",
    "# #                     valid_bleu+=x\n",
    "# #                 valid_bleu/=len(bleu_scores)\n",
    "# #                 info_stamp = 'info=[{:s}]-loss={:2.9f}-bleu={:1.4f}-hidden_dim={:n}-input_dim={:n}-epoch={:n}-batch_size={:n}-batch_id=[{:n}-[of]-{:n}]-lr={:1.4f}'.format(\n",
    "# #                               'fusion_model', fusion_loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "# #                 print(info_stamp, valid_bleu)\n",
    "# #                 now = int(round(time.time()*1000))\n",
    "# #                 time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "# #                 torch.save(fusion_model.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "                \n",
    "#             language_model.train()\n",
    "\n",
    "            \n",
    "# for epoch in range(epochs):\n",
    "#     model_train(epoch, batch_size, split_train_set_size)\n",
    "    \n",
    "# print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-30d9529ff51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 512\n",
    "input_dim = 300\n",
    "lr=0.005\n",
    "batch_size=100\n",
    "split_train_set_size=int(len(split_train_set_inputs)/1)\n",
    "epochs=10000\n",
    "train_bleu_mean=-1\n",
    "train_bleu_max=-1\n",
    "\n",
    "language_model = LanguageModel(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab)\n",
    "#512\n",
    "model_path = './models_language_model/time-[2019-02-26-10-13-36]-info=[language_model]-loss=4.188151360-bleu=-1.0000-hidden_dim=512-input_dim=300-epoch=19-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "model_path = './models_language_model/time-[2019-02-26-11-27-36]-info=[language_model]-loss=3.414012194-bleu=-1.0000-hidden_dim=512-input_dim=300-epoch=21-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "model_path = './models_language_model/time-[2019-02-26-13-18-56]-info=[language_model]-loss=4.003012180-bleu=-1.0000-hidden_dim=512-input_dim=300-epoch=24-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "#2048\n",
    "# model_path = './models_language_model/time-[2019-02-28-02-41-29]-info=[language_model]-loss=4.125045300-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=2-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-28-07-04-08]-info=[language_model]-loss=3.475848675-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=4-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-28-09-15-24]-info=[language_model]-loss=4.169310093-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=5-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-28-13-37-55]-info=[language_model]-loss=4.279534817-bleu=-1.0000-hidden_dim=2048-input_dim=300-epoch=7-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "#1024\n",
    "# model_path = './models_language_model/time-[2019-02-27-16-28-32]-info=[language_model]-loss=4.221434593-bleu=-1.0000-hidden_dim=1024-input_dim=300-epoch=1-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# mdoel_path = './models_language_model/time-[2019-02-27-18-39-43]-info=[language_model]-loss=4.328499317-bleu=-1.0000-hidden_dim=1024-input_dim=300-epoch=3-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "# model_path = './models_language_model/time-[2019-02-27-21-58-23]-info=[language_model]-loss=4.111208439-bleu=-1.0000-hidden_dim=1024-input_dim=300-epoch=6-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050'\n",
    "\n",
    "pre_train = torch.load(model_path, map_location='cpu')\n",
    "language_model.load_state_dict(pre_train)\n",
    "\n",
    "if use_cuda:\n",
    "    language_model = language_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452.750244140625, 56.67298889160156]\n",
      "<low_freq> the <low_freq> 's , to <low_freq> slaves , the <low_freq> is <low_freq> were to and led been a a is now the - <low_freq> , , and been different - ,\n",
      "<low_freq> the <low_freq> was the <low_freq> , of the , the were were not as the <low_freq> of of the , and the the province population of the <low_freq> of china . the , <low_freq> . and is the part minorities separate of the province\n",
      "\n",
      "\n",
      "since the slave masters spoke southern american english , the english the slaves learned , which has developed into what is now african american vernacular english , had many sae features .\n",
      "since the <low_freq> of the taiwan province in 1998 , these subdivisions are regarded as the principal subdivisions of taiwan , and cover the current jurisdiction of the republic of china except <low_freq> and <low_freq> , which are historically and administratively part of fujian .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_range = int((len(split_valid_set_inputs)-1))\n",
    "rand_idx=random.randint(0, data_range)\n",
    "sample_num=2\n",
    "language_model.eval()\n",
    "sents_ppl = language_model.get_sentences_ppl(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                          torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+sample_num]),\n",
    "                                          torch.LongTensor(split_valid_set_labels[rand_idx:rand_idx+sample_num])\n",
    "                                          )\n",
    "\n",
    "print(sents_ppl)\n",
    "\n",
    "\n",
    "logits = language_model.forward(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                     torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+sample_num]),\n",
    "                                                )\n",
    "\n",
    "_, predicts = logits.max(dim=2)\n",
    "predicts = predicts.cpu().data.tolist()\n",
    "predicts = batch_tokens_remove_eos(tokens_list=predicts, vocab=vocab)\n",
    "words_list = batch_tokens2words(predicts, vocab)\n",
    "sents = batch_words2sentence(words_list=words_list)\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#labels\n",
    "labels=split_valid_set_labels[rand_idx:rand_idx+sample_num]\n",
    "labels = batch_tokens_remove_eos(tokens_list=labels, vocab=vocab)\n",
    "words_list = batch_tokens2words(labels, vocab)\n",
    "sents = batch_words2sentence(words_list=words_list)\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    \n",
    "language_model.train()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.02750500671863\n"
     ]
    }
   ],
   "source": [
    "def model_test(model, batch_size, dataset_size, test_set_inputs, test_set_input_lens, test_set_labels):\n",
    "    ppl_sum=0\n",
    "    all_ppl=[]\n",
    "    for start_idx in range(0, dataset_size, batch_size):\n",
    "        sents_ppl = language_model.get_sentences_ppl(torch.LongTensor(test_set_inputs[start_idx:start_idx+batch_size]), \n",
    "                                                      torch.LongTensor(test_set_input_lens[start_idx:start_idx+batch_size]), \n",
    "                                                      torch.LongTensor(test_set_labels[start_idx:start_idx+batch_size])\n",
    "                                                    )\n",
    "        for p in sents_ppl:\n",
    "            ppl_sum+=p\n",
    "            all_ppl.append(p)\n",
    "        \n",
    "    return ppl_sum/dataset_size, all_ppl\n",
    "\n",
    "batch_size = 200\n",
    "dataset_size=len(split_valid_set_inputs)\n",
    "ppl_mean, all_ppl = model_test(language_model, batch_size, dataset_size, split_valid_set_inputs, split_valid_set_input_lens, split_valid_set_labels)\n",
    "print(ppl_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mess sents up. \n",
      "904.5610524456024\n"
     ]
    }
   ],
   "source": [
    "def mess_tokens_up(test_set_inputs, test_set_input_lens, test_set_labels, mess_times):\n",
    "    mess_inputs=copy.deepcopy(test_set_inputs)\n",
    "    mess_labels=copy.deepcopy(test_set_labels)\n",
    "    mess_input_lens=copy.deepcopy(test_set_input_lens)\n",
    "    \n",
    "    for ii, (mess_input, mess_label) in enumerate(zip(mess_inputs, mess_labels)):\n",
    "        for mess_time in range(mess_times):\n",
    "            start_idx=random.randint(1, test_set_input_lens[ii]-1)\n",
    "            end_idx=random.randint(1, test_set_input_lens[ii]-1)\n",
    "            mess_input[start_idx], mess_input[end_idx]=mess_input[end_idx], mess_input[start_idx]\n",
    "            mess_label[start_idx-1], mess_label[end_idx-1]=mess_label[end_idx-1], mess_label[start_idx-1]\n",
    "    return mess_inputs, mess_input_lens, mess_labels\n",
    "\n",
    "mess_inputs, mess_input_lens, mess_labels = mess_tokens_up(split_valid_set_inputs, split_valid_set_input_lens, \n",
    "                                                           split_valid_set_labels, mess_times=2)\n",
    "print('mess sents up. ')\n",
    "\n",
    "ppl_mean, all_ppl = model_test(language_model, batch_size, dataset_size, mess_inputs, mess_input_lens, mess_labels)\n",
    "print(ppl_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0347608346071 0.0139677994815\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "x = scipy.array(all_ppl)\n",
    "y = scipy.array(split_valid_set_input_lens)\n",
    "\n",
    "r_row, p_value = pearsonr(x, y)\n",
    "print(r_row, p_value)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
