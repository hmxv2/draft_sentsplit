{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "import os\n",
    "import copy\n",
    "from Vocab import Vocab\n",
    "\n",
    "file_group = 'validation'    #availabe:  train    test    validation    tune\n",
    "\n",
    "with open('vocab.pk', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pseudo labels for split\n",
    "with open(''.join(['split_data_set/', file_group, '_complex_sents.pk']), 'rb') as f:\n",
    "    complex_sents = pickle.load(f)\n",
    "pseudo_labels=copy.deepcopy(complex_sents)\n",
    "with open(''.join(['split_data_set/', file_group, '_complex_sent_lens.pk']), 'rb') as f:\n",
    "    complex_sent_lens = pickle.load(f)\n",
    "    \n",
    "# generate pseudo labels by inserting three tokens: '.', '<split>' and 'eos'\n",
    "split_range=0\n",
    "for idx, sent in enumerate(pseudo_labels):\n",
    "    split_idx = int(complex_sent_lens[idx]/2)\n",
    "    split_idx = random.randint(split_idx-split_range, split_idx+split_range)\n",
    "    sent.insert(split_idx, vocab.word2token['.'])\n",
    "    sent.insert(split_idx+1, vocab.word2token['<split>'])\n",
    "    sent.insert(complex_sent_lens[idx]+2, vocab.word2token['<eos>'])\n",
    "    sent_len = len(sent)\n",
    "    for _ in range(61-sent_len):\n",
    "        sent.append(vocab.word2token['<padding>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28541, 3990, 15364, 9194, 789, 23563, 25, 27945, 789, 27513, 21467, 38225, 11673, 17835, 21354, 21467, 18327, 43713, 19863, 33147, 12697, 40533, 23661, 25168, 434, 33942, 27513, 21467, 12697, 3990, 25377, 40017, 4786, 11883, 37734, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 50\n",
      "[28541, 3990, 15364, 9194, 789, 23563, 25, 27945, 789, 27513, 21467, 38225, 11673, 17835, 21354, 21467, 18327, 37734, 5, 43713, 19863, 33147, 12697, 40533, 23661, 25168, 434, 33942, 27513, 21467, 12697, 3990, 25377, 40017, 4786, 11883, 37734, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 61\n"
     ]
    }
   ],
   "source": [
    "rand_idx = random.randint(0, len(complex_sents)-1)\n",
    "print(complex_sents[rand_idx], len(complex_sents[idx]))\n",
    "print(pseudo_labels[rand_idx], len(pseudo_labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(''.join(['split_data_set/', file_group, '_pseudo_labels.pk']), 'wb') as f:\n",
    "    pickle.dump(pseudo_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pseudo labels for fusion\n",
    "with open(''.join(['split_data_set/', file_group, '_complex_sents.pk']), 'rb') as f:\n",
    "    complex_sents = pickle.load(f)\n",
    "pseudo_simple_sents=copy.deepcopy(complex_sents)\n",
    "pseudo_labels=copy.deepcopy(complex_sents)\n",
    "with open(''.join(['split_data_set/', file_group, '_complex_sent_lens.pk']), 'rb') as f:\n",
    "    complex_sent_lens = pickle.load(f)\n",
    "pseudo_simple_sent_lens = copy.deepcopy(complex_sent_lens)\n",
    "\n",
    "# generate pseudo simple sentences by inserting two tokens: '.', '<split>'\n",
    "split_range=0\n",
    "for idx, sent in enumerate(pseudo_simple_sents):\n",
    "    split_idx = int(pseudo_simple_sent_lens[idx]/2)\n",
    "    split_idx = random.randint(split_idx-split_range, split_idx+split_range)\n",
    "    sent.insert(split_idx, vocab.word2token['.'])\n",
    "    sent.insert(split_idx+1, vocab.word2token['<split>'])\n",
    "    \n",
    "    pseudo_simple_sent_lens[idx] = pseudo_simple_sent_lens[idx]+2\n",
    "    \n",
    "for idx, sent in enumerate(pseudo_labels):\n",
    "    if sent[-1]!=vocab.word2token['<padding>']:\n",
    "        sent.append(vocab.word2token['<eos>'])\n",
    "    else:\n",
    "        for ii in range(49, -1, -1):\n",
    "            if sent[ii]!=vocab.word2token['<padding>']:\n",
    "                sent.insert(ii+1, vocab.word2token['<eos>'])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 25168, 30341, 29740, 13264, 31006, 13264, 32498, 21467, 21092, 18637, 27513, 22111, 13264, 28248, 21467, 26817, 789, 3, 789, 13126, 27567, 34917, 14895, 37734, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 50 25\n",
      "[3, 25168, 30341, 29740, 13264, 31006, 13264, 32498, 21467, 21092, 18637, 27513, 37734, 5, 22111, 13264, 28248, 21467, 26817, 789, 3, 789, 13126, 27567, 34917, 14895, 37734, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 27\n",
      "[3, 25168, 30341, 29740, 13264, 31006, 13264, 32498, 21467, 21092, 18637, 27513, 22111, 13264, 28248, 21467, 26817, 789, 3, 789, 13126, 27567, 34917, 14895, 37734, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 51\n"
     ]
    }
   ],
   "source": [
    "rand_idx = random.randint(0, len(complex_sents)-1)\n",
    "print(complex_sents[rand_idx], len(complex_sents[rand_idx]), complex_sent_lens[rand_idx])\n",
    "\n",
    "print(pseudo_simple_sents[rand_idx], pseudo_simple_sent_lens[rand_idx])\n",
    "print(pseudo_labels[rand_idx], len(pseudo_labels[rand_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(''.join(['fusion_data_set/', file_group, '_pseudo_simple_sent_lens.pk']), 'wb') as f:\n",
    "    pickle.dump(pseudo_simple_sent_lens, f)\n",
    "with open(''.join(['fusion_data_set/', file_group, '_pseudo_simple_sents.pk']), 'wb') as f:\n",
    "    pickle.dump(pseudo_simple_sents, f)\n",
    "with open(''.join(['fusion_data_set/', file_group, '_pseudo_labels.pk']), 'wb') as f:\n",
    "    pickle.dump(pseudo_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # pseudo labels for fusion\n",
    "# with open(''.join(['fusion_data_set/', file_group, '_simple_sents.pk']), 'rb') as f:\n",
    "#     simple_sents = pickle.load(f)\n",
    "\n",
    "# with open(''.join(['fusion_data_set/', file_group, '_simple_sent_lens.pk']), 'rb') as f:\n",
    "#     simple_sent_lens = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pseudo_labels=copy.deepcopy(simple_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # generate pseudo labels by removing two tokens: '.', '<split>' and inserting three tokens:'eos'\n",
    "# for sent_idx, sent in enumerate(pseudo_labels):\n",
    "#     for idx, token in enumerate(sent):\n",
    "#         if token == vocab.word2token['<split>']:\n",
    "#             split_idx = idx\n",
    "#             break\n",
    "#     sent.pop(split_idx-1)\n",
    "#     sent.pop(split_idx-1)\n",
    "#     sent.insert(simple_sent_lens[sent_idx]-1-2+1, vocab.word2token['<eos>'])\n",
    "#     if len(sent)!=59:\n",
    "#         print(sent_idx, len(sent))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21467, 25133, 24152, 36047, 11687, 40850, 11138, 3, 5327, 35165, 11138, 3, 5327, 25161, 13264, 11732, 27513, 11382, 13264, 752, 32201, 3990, 36519, 24152, 36047, 37734, 5, 28541, 7447, 9194, 11206, 39575, 29243, 36047, 13264, 25546, 43775, 32514, 25168, 11951, 32514, 40017, 21467, 1400, 25168, 23842, 30705, 21354, 21467, 25133, 24152, 21354, 32201, 11687, 8558, 25161, 37734, 1, 1, 1] 60\n",
      "[21467, 25133, 24152, 36047, 11687, 40850, 11138, 3, 5327, 35165, 11138, 3, 5327, 25161, 13264, 11732, 27513, 11382, 13264, 752, 32201, 3990, 36519, 24152, 36047, 28541, 7447, 9194, 11206, 39575, 29243, 36047, 13264, 25546, 43775, 32514, 25168, 11951, 32514, 40017, 21467, 1400, 25168, 23842, 30705, 21354, 21467, 25133, 24152, 21354, 32201, 11687, 8558, 25161, 37734, 2, 1, 1, 1] 59\n"
     ]
    }
   ],
   "source": [
    "# rand_idx = random.randint(0, len(simple_sents)-1)\n",
    "# print(simple_sents[rand_idx], len(simple_sents[idx]))\n",
    "# print(pseudo_labels[rand_idx], len(pseudo_labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(''.join(['fusion_data_set/', file_group, '_pseudo_labels.pk']), 'wb') as f:\n",
    "    pickle.dump(pseudo_labels, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
