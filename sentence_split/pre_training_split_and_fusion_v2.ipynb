{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "\n",
    "from Vocab import Vocab\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021744100219015735]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [' '.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('data_set/vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "    \n",
    "batch_tokens_bleu([[1,2,3,4,5,6]], [[2,3,1,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data_set/fusion_data_set/train_pseudo_simple_sents.pk', 'rb') as f:\n",
    "    fusion_pseudo_train_set_inputs = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/train_pseudo_simple_sent_lens.pk', 'rb') as f:\n",
    "    fusion_pseudo_train_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/train_pseudo_labels.pk', 'rb') as f:\n",
    "    fusion_pseudo_train_set_labels = pickle.load(f)\n",
    "    \n",
    "with open('./data_set/fusion_data_set/validation_pseudo_simple_sents.pk', 'rb') as f:\n",
    "    fusion_pseudo_valid_set_inputs = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/validation_pseudo_simple_sent_lens.pk', 'rb') as f:\n",
    "    fusion_pseudo_valid_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/fusion_data_set/validation_pseudo_labels.pk', 'rb') as f:\n",
    "    fusion_pseudo_valid_set_labels = pickle.load(f)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "with open('./data_set/split_data_set/train_complex_sents.pk', 'rb') as f:\n",
    "    split_train_set_inputs = pickle.load(f)\n",
    "with open('./data_set/split_data_set/train_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_train_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/split_data_set/train_pseudo_labels.pk', 'rb') as f:\n",
    "    split_pseudo_train_set_labels = pickle.load(f)\n",
    "    \n",
    "with open('./data_set/split_data_set/validation_complex_sents.pk', 'rb') as f:\n",
    "    split_valid_set_inputs = pickle.load(f)\n",
    "with open('./data_set/split_data_set/validation_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_valid_set_input_lens = pickle.load(f)\n",
    "with open('./data_set/split_data_set/validation_labels.pk', 'rb') as f:\n",
    "    split_pseudo_valid_set_labels = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num=100000\n",
    "# train_set_inputs_10w = train_set_inputs[:num]\n",
    "# train_set_input_lens_10w = train_set_input_lens[:num]\n",
    "# train_set_labels_10w = train_set_labels[:num]\n",
    "\n",
    "# valid_set_inputs_10w = valid_set_inputs[:num]\n",
    "# valid_set_input_lens_10w = valid_set_input_lens[:num]\n",
    "# valid_set_labels_10w = valid_set_labels[:num]\n",
    "\n",
    "\n",
    "# with open('./small_data_set/train_set_inputs_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_inputs_10w,f)\n",
    "# with open('./small_data_set/train_set_input_lens_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_input_lens_10w ,f)\n",
    "# with open('./small_data_set/train_set_labels_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(train_set_labels_10w ,f)\n",
    "    \n",
    "# with open('./small_data_set/valid_set_inputs_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_inputs_10w ,f)\n",
    "# with open('./small_data_set/valid_set_input_lens_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_input_lens_10w ,f)\n",
    "# with open('./small_data_set/valid_set_labels_10w.pk', 'bw') as f:\n",
    "#     pickle.dump(valid_set_labels_10w ,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989944 989944 989944\n",
      "989944 989944 989944\n"
     ]
    }
   ],
   "source": [
    "print(len(split_train_set_inputs), len(split_train_set_input_lens), len(split_pseudo_train_set_labels))\n",
    "print(len(fusion_pseudo_train_set_inputs), len(fusion_pseudo_train_set_input_lens), len(fusion_pseudo_train_set_labels))\n",
    "# for sent_len in valid_set_input_lens:\n",
    "#     if sent_len<=2:\n",
    "#         print('why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_cuda, hidden_dim, input_dim, vocab):#, pre_train_weight, is_fix_word_vector = 1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_dim=input_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.lstm=torch.nn.LSTM(input_size=self.input_dim, \n",
    "                                hidden_size= self.hidden_dim, \n",
    "                                bidirectional=True,\n",
    "                                batch_first=True\n",
    "                               )\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=nn.Embedding(len(self.vocab.word2token), input_dim)\n",
    "        #loading pre trained word embedding\n",
    "        with open('data_set/pre_trained_token_embedding.pk', 'rb') as f:\n",
    "            pre_train_word_embedding = pickle.load(f)\n",
    "            \n",
    "        self.embed.weight.data.copy_(torch.FloatTensor(pre_train_word_embedding))\n",
    "#         self.embed.weight.requires_grad = False\n",
    "        \n",
    "    def order(self, inputs, inputs_len):    #inputs: tensor, inputs_len: 1D tensor\n",
    "        inputs_len, sort_ids = torch.sort(inputs_len, dim=0, descending=True)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids).cuda())\n",
    "        else:\n",
    "            inputs = inputs.index_select(0, Variable(sort_ids))\n",
    "        \n",
    "        _, true_order_ids = torch.sort(sort_ids, dim=0, descending=False)\n",
    "        \n",
    "        return inputs, inputs_len, true_order_ids\n",
    "    #\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        inputs = Variable(inputs)\n",
    "        if self.use_cuda:\n",
    "            inputs=inputs.cuda()\n",
    "            \n",
    "        inputs, sort_len, true_order_ids = self.order(inputs, inputs_len)\n",
    "\n",
    "        in_vecs=self.embed(inputs)\n",
    "\n",
    "        packed = rnn_utils.pack_padded_sequence(input=in_vecs, lengths=list(sort_len), batch_first =True)\n",
    "        \n",
    "        outputs, (hn,cn) = self.lstm(packed)\n",
    "        outputs, sent_lens = rnn_utils.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        outputs = outputs.transpose(0,1)  #transpose is necessary\n",
    "        #print('outpurs size, hn size and cn size: ', outputs.size(), hn.size(), cn.size())\n",
    "        \n",
    "        #warnning: outputs, hn and cn have been sorted by sentences length so the order is wrong, now to sort them\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            outputs = outputs.index_select(0, Variable(true_order_ids))\n",
    "        \n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        cn = torch.cat((cn[0], cn[1]), dim=1)\n",
    "        #print('hn size and cn size: ', hn.size(), cn.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids).cuda())\n",
    "            cn = cn.index_select(0, Variable(true_order_ids).cuda())\n",
    "        else:\n",
    "            hn = hn.index_select(0, Variable(true_order_ids))\n",
    "            cn = cn.index_select(0, Variable(true_order_ids))\n",
    "            \n",
    "        return outputs, (hn,cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _inflate(tensor, times, dim):\n",
    "    \"\"\"\n",
    "    Examples::\n",
    "        >> a = torch.LongTensor([[1, 2], [3, 4]])\n",
    "        >> a\n",
    "        1   2\n",
    "        3   4\n",
    "        [torch.LongTensor of size 2x2]\n",
    "        >> b = ._inflate(a, 2, dim=1)\n",
    "        >> b\n",
    "        1   2   1   2\n",
    "        3   4   3   4\n",
    "        [torch.LongTensor of size 2x4]\n",
    "    \"\"\"\n",
    "    repeat_dims = [1] * tensor.dim()\n",
    "    repeat_dims[dim] = times\n",
    "    return tensor.repeat(*repeat_dims)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, use_cuda, encoder, hidden_dim, max_length=25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.input_dim = encoder.input_dim\n",
    "        self.max_length = max_length\n",
    "        self.vocab = encoder.vocab\n",
    "        self.weight = [1]*len(self.vocab.word2token)\n",
    "        self.weight[self.vocab.word2token['<padding>']]=0\n",
    "        #self.weight[self.vocab.word2token['<eos>']]=1.01\n",
    "        #self.weight[self.vocab.word2token['<split>']]=1.01\n",
    "        \n",
    "        self.hidden_size = self.hidden_dim\n",
    "        self.V = len(self.vocab.word2token)\n",
    "        self.SOS = self.vocab.word2token['<sos>']\n",
    "        self.EOS = self.vocab.word2token['<eos>']\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lstmcell = torch.nn.LSTMCell(input_size=self.input_dim, hidden_size=self.hidden_dim*2, bias=True)\n",
    "        \n",
    "        #embedding\n",
    "        self.embed=encoder.embed# reference share\n",
    "        #fcnn: projection for crossentroy loss\n",
    "        self.fcnn = nn.Linear(in_features = self.hidden_dim*2, out_features = len(self.vocab.word2token))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.cost_func = nn.CrossEntropyLoss(weight=torch.Tensor(self.weight), reduce=True)\n",
    "\n",
    "        print('init lookup embedding matrix size: ', self.embed.weight.data.size())\n",
    "        \n",
    "\n",
    "    def forward(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        labels = Variable(labels)\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        all_loss = 0\n",
    "        predicts = []\n",
    "        max_probs=[]\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        final_hidden_states = h0_and_c0[0]\n",
    "\n",
    "        for ii in range(self.max_length):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,0])\n",
    "#                     t=torch.mean(loss)\n",
    "                    all_loss+=loss\n",
    "                \n",
    "                #last_timestep_output = self.log_softmax(last_timestep_output)\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 max_prob = self.cost_func(last_timestep_output, labels[:,0])\n",
    "#                 max_probs.append(torch.unsqueeze(max_prob, dim=0))\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #print('max_prob size: ', max_prob.size())\n",
    "                #print('max_idxs size: ',max_idxs.size(), max_idxs)\n",
    "                \n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "#                     t=torch.mean(loss)\n",
    "                    all_loss+=loss\n",
    "#                 _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 #print('max_idx size: ', max_idxs.size(), max_idxs)\n",
    "#                 predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                #last_timestep_output = self.log_softmax(last_timestep_output)\n",
    "                _, max_idxs = torch.max(last_timestep_output, dim=1)\n",
    "#                 max_prob = self.cost_func(last_timestep_output, labels[:,ii])\n",
    "#                 max_probs.append(torch.unsqueeze(max_prob, dim=0))\n",
    "                predicts.append(torch.unsqueeze(max_idxs, dim=0))\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=0)\n",
    "        predicts = torch.transpose(predicts, 0, 1)\n",
    "    \n",
    "        if is_train:  #training\n",
    "#             all_loss = torch.cat(all_loss, dim=1)\n",
    "#             all_loss = torch.mean(all_loss, dim=1)\n",
    "#             loss = torch.mean(all_loss)\n",
    "            loss = all_loss/self.max_length\n",
    "            #print('loss size: ', loss.size())\n",
    "            #torch.cuda.empty_cache()\n",
    "            if self.use_cuda:\n",
    "                return loss, predicts.data.cpu().tolist()\n",
    "            else:\n",
    "                return loss, predicts.data.tolist()\n",
    "        else:   #testing\n",
    "            if self.use_cuda:\n",
    "                return predicts.data.cpu().tolist()\n",
    "            else:\n",
    "                return predicts.data.tolist()\n",
    "#         if is_train:  #training\n",
    "#             if self.use_cuda:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return all_loss/(self.max_length+1), predicts.data.numpy()\n",
    "#         else:   #testing\n",
    "#             if self.use_cuda:\n",
    "#                 return predicts.data.cpu().numpy()\n",
    "#             else:\n",
    "#                 return predicts.data.numpy()\n",
    "    def loss_func_rl(self, logits):\n",
    "        logprobs = self.log_softmax(logits)\n",
    "        max_logprobs, max_ids = torch.max(logprobs, dim=1)\n",
    "        weights = torch.FloatTensor(self.weight)\n",
    "        index = max_ids.data\n",
    "        if self.use_cuda:\n",
    "            index = index.cpu()\n",
    "        weights = Variable(torch.index_select(weights, 0, index), requires_grad=False)\n",
    "        if self.use_cuda:\n",
    "            weights = weights.cuda()\n",
    "        return -max_logprobs*weights\n",
    "    \n",
    "    def token_sample(self, logits, batch_size, topk=100, e_greedy_rate=0.2, dec_seq_num=10):\n",
    "        topk_logits, topk_idxs = torch.topk(logits, k=topk, dim=1)\n",
    "        if self.use_cuda:\n",
    "            ltopk_probs = self.softmax(topk_logits).data.cpu().tolist()\n",
    "            ltopk_idxs = topk_idxs.data.cpu().tolist()\n",
    "        else:\n",
    "            ltopk_probs = self.softmax(topk_logits).data.tolist()\n",
    "            ltopk_idxs = topk_idxs.data.tolist()\n",
    "        batch_results=[]\n",
    "        for probs, idxs in zip(ltopk_probs, ltopk_idxs):\n",
    "            sum_probs = sum(probs)\n",
    "            probs = [x/sum_probs for x in probs]\n",
    "            results=[]\n",
    "            for ii in range(dec_seq_num):\n",
    "                result = np.random.choice(idxs, p=probs)\n",
    "                if random.random()>e_greedy_rate:\n",
    "                    results.append(int(result))\n",
    "                else:\n",
    "                    results.append(random.randint(0, len(self.vocab.word2token)-1))\n",
    "            batch_results.append(results)\n",
    "        return batch_results\n",
    "    \n",
    "    def train_using_rl_2(self, enc_outputs, sent_lens, h0_and_c0, labels, teaching_rate=0.6, is_train=1):\n",
    "        #nll_loss = nn.NLLLoss(weight=torch.FloatTensor(self.weight).cuda(), reduce=False)\n",
    "        \n",
    "        # parameter labels must be python list type\n",
    "        labels_tokens=labels\n",
    "        labels = Variable(torch.LongTensor(labels))\n",
    "        if self.use_cuda:\n",
    "            labels = labels.cuda()\n",
    "        #print('rl...')\n",
    "        loss=0\n",
    "        all_loss = 0\n",
    "        losss=[]\n",
    "        predicts = []\n",
    "        max_probs=[]\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        topk=20\n",
    "        e_greedy_rate=0.3\n",
    "        dec_seq_num=100\n",
    "        vocab_size = len(self.vocab.word2token)\n",
    "        index_bias = torch.LongTensor([vocab_size*ii for ii in range(batch_size)]).unsqueeze(dim=1)\n",
    "        index_bias = index_bias.expand(batch_size, dec_seq_num).contiguous().view(-1)\n",
    "        if self.use_cuda:\n",
    "            index_bias = index_bias.cuda()\n",
    "        for ii in range(self.max_length):\n",
    "            if ii==0:\n",
    "                zero_timestep_input = Variable(torch.LongTensor([self.vocab.word2token['<sos>']]*batch_size))\n",
    "                if self.use_cuda:\n",
    "                    zero_timestep_input = zero_timestep_input.cuda()\n",
    "                    \n",
    "                zero_timestep_input = self.embed(zero_timestep_input)#size: batch_size * self.input_dim\n",
    "\n",
    "                last_timestep_hidden_state,cx = self.lstmcell(zero_timestep_input, h0_and_c0)\n",
    "                #print('hn and cn sizes: ', last_timestep_hidden_state.size(), cx.size())\n",
    "                \n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "\n",
    "                batch_ltokens = self.token_sample(logits=last_timestep_output, batch_size=batch_size, topk=topk, e_greedy_rate=e_greedy_rate, dec_seq_num=dec_seq_num)\n",
    "                #print('batch_ltokens size: ', torch.LongTensor(batch_ltokens).size())\n",
    "                batch_ltokens=torch.LongTensor(batch_ltokens).view(-1)\n",
    "                if self.use_cuda:\n",
    "                    batch_ltokens = batch_ltokens.cuda()\n",
    "                predicts.append(batch_ltokens.view(-1,1))\n",
    "                if is_train:\n",
    "                    neg_log_prob = -self.log_softmax(last_timestep_output).view(-1)\n",
    "                    #print('batch_ltokens and index_bias size: ', batch_ltokens.size(), index_bias.size())\n",
    "                    indices = (batch_ltokens+index_bias)\n",
    "                    \n",
    "                    loss=neg_log_prob[indices]\n",
    "#                     loss = self.cost_func(last_timestep_output, pseudo_labels)#labels[:,ii])\n",
    "#                     losss.append(torch.unsqueeze(loss, dim=1))\n",
    "#                 if is_train:\n",
    "#                     loss = self.cost_func(last_timestep_output, max_idxs)#labels[:,0])\n",
    "#                     losss.append(torch.unsqueeze(loss, dim=1))\n",
    "            else:\n",
    "                if is_train:\n",
    "                    rand = random.random()\n",
    "                    if rand<teaching_rate:\n",
    "                        this_timestep_input = self.embed(labels[:,ii-1])#label teaching, lookup embedding\n",
    "                    else:\n",
    "                        this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                else:\n",
    "                    this_timestep_input = self.embed(max_idxs)#last_timestep output, and then look up word embedding\n",
    "                    \n",
    "                last_timestep_hidden_state ,cx = self.lstmcell(this_timestep_input, (last_timestep_hidden_state,cx))\n",
    "                last_timestep_output = self.fcnn(last_timestep_hidden_state)\n",
    "                \n",
    "                \n",
    "                batch_ltokens = self.token_sample(logits=last_timestep_output, batch_size=batch_size, topk=topk, e_greedy_rate=e_greedy_rate, dec_seq_num=dec_seq_num)\n",
    "                batch_ltokens=torch.LongTensor(batch_ltokens).view(-1)\n",
    "                if self.use_cuda:\n",
    "                    batch_ltokens = batch_ltokens.cuda()\n",
    "                predicts.append(batch_ltokens.view(-1,1))\n",
    "                if is_train:\n",
    "                    neg_log_prob = -self.log_softmax(last_timestep_output).view(-1)\n",
    "                    indices = (batch_ltokens+index_bias)\n",
    "                    \n",
    "                    loss+=neg_log_prob[indices]\n",
    "                \n",
    "        predicts = torch.cat(predicts, dim=1)\n",
    "        #print('predicts size: ', predicts.size())\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            predicts = predicts.cpu().numpy()\n",
    "        else:\n",
    "            predicts = predicts.numpy()\n",
    "        \n",
    "        predicts_tokens=batch_tokens_remove_eos(predicts.tolist(), self.vocab)\n",
    "        labels_tokens=[x for x in labels_tokens for ii in range(dec_seq_num)]\n",
    "        labels_tokens=batch_tokens_remove_eos(labels_tokens, self.vocab)\n",
    "        bleu_scores = batch_tokens_bleu(references=labels_tokens, candidates=predicts_tokens, smooth_epsilon=0.01)\n",
    "#         predicts_words=batch_tokens2words(predicts_tokens, self.vocab)\n",
    "#         predicts_sents=batch_words2sentence(predicts_words)\n",
    "        labels_words = batch_tokens2words(labels_tokens, self.vocab)\n",
    "        labels_sents = batch_words2sentence(labels_words)\n",
    "#         print(len(labels_sents))\n",
    "#         for ii in labels_sents[:21]:\n",
    "#             print(ii)\n",
    "        \n",
    "        bleu_scores = torch.FloatTensor(bleu_scores).view(batch_size, dec_seq_num)\n",
    "        bleu_mean = torch.mean(bleu_scores, dim=1).unsqueeze(dim=1)\n",
    "        bleu_scores = bleu_scores-bleu_mean\n",
    "        bleu_scores = bleu_scores.view(-1)\n",
    "        \n",
    "        bleu_scores = Variable(bleu_scores, requires_grad = 0)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            bleu_scores = bleu_scores.cuda()\n",
    "            \n",
    "        loss = torch.dot(loss, bleu_scores)/batch_size/dec_seq_num\n",
    "        \n",
    "        return loss, predicts, torch.mean(bleu_mean.squeeze())\n",
    "    \n",
    "    def train_using_rl_3(self, enc_outputs, h0_and_c0, labels, topk=10):\n",
    "        batch_size = enc_outputs.size(dim = 0)\n",
    "        metadata = self.decode_by_beamsearch(encoder_hidden=h0_and_c0, encoder_outputs=enc_outputs, topk = topk)\n",
    "        results = metadata['topk_sequence']\n",
    "        results =torch.cat(results, dim = 2)\n",
    "        results=results.view(batch_size*topk, -1)\n",
    "        if self.use_cuda:\n",
    "            results = results.data.cpu().tolist()\n",
    "        else:\n",
    "            results = results.data.tolist()\n",
    "        results=batch_tokens_remove_eos(results, self.vocab)\n",
    "\n",
    "        labels = [x for x in labels for ii in range(topk)]\n",
    "        labels = batch_tokens_remove_eos(labels, self.vocab)\n",
    "        bleu_scores = batch_tokens_bleu(references=labels, candidates=results, smooth_epsilon=0.01)\n",
    "        \n",
    "        bleu_scores = torch.FloatTensor(bleu_scores).view(batch_size, topk)\n",
    "        bleu_max, _ = torch.max(bleu_scores, dim=1)\n",
    "        \n",
    "        bleu_mean = torch.mean(bleu_scores, dim=1).unsqueeze(dim=1)\n",
    "        bleu_scores = bleu_scores-bleu_mean\n",
    "        bleu_scores = bleu_scores.view(-1)\n",
    "        \n",
    "        bleu_scores = self._tocuda(Variable(bleu_scores, requires_grad = 0))\n",
    "        \n",
    "        log_probs = metadata['score']\n",
    "        log_probs = log_probs.view(batch_size*topk)\n",
    "        \n",
    "        loss = -torch.dot(log_probs, bleu_scores)/batch_size/topk\n",
    "        \n",
    "        return loss, results, torch.mean(bleu_mean.squeeze()), torch.mean(bleu_max)\n",
    "        \n",
    "        \n",
    "    def _tocuda(self, var):\n",
    "        if self.use_cuda:\n",
    "            return var.cuda()\n",
    "        else:\n",
    "            return var\n",
    "    def decode_by_beamsearch(self, encoder_hidden=None, encoder_outputs=None, topk = 10):\n",
    "        self.k = topk\n",
    "        batch_size = encoder_outputs.size(dim=0)\n",
    "        \n",
    "        self.pos_index = self._tocuda(Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1))\n",
    "\n",
    "        hidden = tuple([_inflate(h, self.k, 1).view(batch_size*self.k, -1) for h in encoder_hidden])\n",
    "        #print('hidden0 size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "\n",
    "        # Initialize the scores; for the first step,\n",
    "        # ignore the inflated copies to avoid duplicate entries in the top k\n",
    "        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n",
    "        sequence_scores.fill_(-float('Inf'))\n",
    "        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n",
    "        sequence_scores = self._tocuda(Variable(sequence_scores))\n",
    "\n",
    "        # Initialize the input vector\n",
    "        input_var = self._tocuda(Variable(torch.LongTensor([self.SOS] * batch_size * self.k)))\n",
    "\n",
    "        # Store decisions for backtracking\n",
    "        stored_outputs = list()\n",
    "        stored_scores = list()\n",
    "        stored_predecessors = list()\n",
    "        stored_emitted_symbols = list()\n",
    "        stored_hidden = list()\n",
    "\n",
    "        for ii in range(0, self.max_length):\n",
    "            # Run the RNN one step forward\n",
    "            #print('setp: %s'%ii)\n",
    "            input_vec = self.embed(input_var)\n",
    "            #print('input_var and input_vec size: ', input_var.size(), input_vec.size())\n",
    "            hidden = self.lstmcell(input_vec, hidden)\n",
    "            #print('hidden size: (%s, %s)'%(hidden[0].size(), hidden[1].size()))\n",
    "            \n",
    "            log_softmax_output = self.log_softmax(self.fcnn(hidden[0]))\n",
    "\n",
    "            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n",
    "            sequence_scores = _inflate(sequence_scores, self.V, 1)\n",
    "            sequence_scores += log_softmax_output.squeeze(1)\n",
    "            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n",
    "\n",
    "            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n",
    "            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n",
    "            sequence_scores = scores.view(batch_size * self.k, 1)\n",
    "\n",
    "            # Update fields for next timestep\n",
    "            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n",
    "            if isinstance(hidden, tuple):\n",
    "                hidden = tuple([h.index_select(0, predecessors.squeeze()) for h in hidden])\n",
    "            else:\n",
    "                hidden = hidden.index_select(0, predecessors.squeeze())\n",
    "\n",
    "            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren't expanded\n",
    "            stored_scores.append(sequence_scores.clone())\n",
    "            eos_indices = input_var.data.eq(self.EOS)\n",
    "            if eos_indices.nonzero().dim() > 0:\n",
    "                sequence_scores.data.masked_fill_(eos_indices, -float('inf'))\n",
    "\n",
    "            # Cache results for backtracking\n",
    "            stored_predecessors.append(predecessors)\n",
    "            stored_emitted_symbols.append(input_var)\n",
    "#             stored_hidden.append(hidden)\n",
    "\n",
    "        # Do backtracking to return the optimal values\n",
    "        output, h_t, h_n, s, l, p = self._backtrack(hidden,\n",
    "                                                    stored_predecessors, stored_emitted_symbols,\n",
    "                                                    stored_scores, batch_size, self.hidden_size)\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        metadata['score'] = s\n",
    "        metadata['topk_length'] = l\n",
    "        metadata['topk_sequence'] = p\n",
    "        metadata['length'] = [seq_len[0] for seq_len in l]\n",
    "        metadata['sequence'] = [seq[0] for seq in p]\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def _backtrack(self, hidden, predecessors, symbols, scores, b, hidden_size):\n",
    "        \"\"\"Backtracks over batch to generate optimal k-sequences.\n",
    "\n",
    "        Args:\n",
    "            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n",
    "            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n",
    "            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n",
    "            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n",
    "            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n",
    "            b: Size of the batch\n",
    "            hidden_size: Size of the hidden state\n",
    "\n",
    "        Returns:\n",
    "            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n",
    "            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n",
    "\n",
    "            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n",
    "\n",
    "            score [batch, k]: A list containing the final scores for all top-k sequences\n",
    "\n",
    "            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n",
    "\n",
    "            p (batch, k, sequence_len): A Tensor containing predicted sequence\n",
    "        \"\"\"\n",
    "\n",
    "        lstm = isinstance(hidden, tuple)\n",
    "\n",
    "        # initialize return variables given different types\n",
    "        output = list()\n",
    "        h_t = list()\n",
    "        p = list()\n",
    "        # Placeholder for last hidden state of top-k sequences.\n",
    "        # If a (top-k) sequence ends early in decoding, `h_n` contains\n",
    "        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n",
    "        # the last hidden state of decoding.\n",
    "        if lstm:\n",
    "            state_size = hidden[0].size()\n",
    "            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n",
    "        else:\n",
    "            h_n = torch.zeros(nw_hidden[0].size())\n",
    "        l = [[self.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n",
    "                                                                # Similar to `h_n`\n",
    "\n",
    "        # the last step output of the beams are not sorted\n",
    "        # thus they are sorted here\n",
    "        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n",
    "        # initialize the sequence scores with the sorted last step beam scores\n",
    "        s = sorted_score.clone()\n",
    "\n",
    "        batch_eos_found = [0] * b   # the number of EOS found\n",
    "                                    # in the backward loop below for each batch\n",
    "\n",
    "        t = self.max_length - 1\n",
    "        # initialize the back pointer with the sorted order of the last step beams.\n",
    "        # add self.pos_index for indexing variable with b*k as the first dimension.\n",
    "        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n",
    "        while t >= 0:\n",
    "            # Re-order the variables with the back pointer\n",
    "            current_symbol = symbols[t].index_select(0, t_predecessors)\n",
    "            # Re-order the back pointer of the previous step with the back pointer of\n",
    "            # the current step\n",
    "            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n",
    "\n",
    "            # This tricky block handles dropped sequences that see EOS earlier.\n",
    "            # The basic idea is summarized below:\n",
    "            #\n",
    "            #   Terms:\n",
    "            #       Ended sequences = sequences that see EOS early and dropped\n",
    "            #       Survived sequences = sequences in the last step of the beams\n",
    "            #\n",
    "            #       Although the ended sequences are dropped during decoding,\n",
    "            #   their generated symbols and complete backtracking information are still\n",
    "            #   in the backtracking variables.\n",
    "            #   For each batch, everytime we see an EOS in the backtracking process,\n",
    "            #       1. If there is survived sequences in the return variables, replace\n",
    "            #       the one with the lowest survived sequence score with the new ended\n",
    "            #       sequences\n",
    "            #       2. Otherwise, replace the ended sequence with the lowest sequence\n",
    "            #       score with the new ended sequence\n",
    "            #\n",
    "            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n",
    "            if eos_indices.dim() > 0:\n",
    "                for i in range(eos_indices.size(0)-1, -1, -1):\n",
    "                    # Indices of the EOS symbol for both variables\n",
    "                    # with b*k as the first dimension, and b, k for\n",
    "                    # the first two dimensions\n",
    "                    idx = eos_indices[i]\n",
    "                    b_idx = int(idx[0] / self.k)\n",
    "                    # The indices of the replacing position\n",
    "                    # according to the replacement strategy noted above\n",
    "                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n",
    "                    batch_eos_found[b_idx] += 1\n",
    "                    res_idx = b_idx * self.k + res_k_idx\n",
    "\n",
    "                    # Replace the old information in return variables\n",
    "                    # with the new ended sequence information\n",
    "                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n",
    "\n",
    "                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n",
    "                    s[b_idx, res_k_idx] = scores[t][idx[0]]\n",
    "                    l[b_idx][res_k_idx] = t + 1\n",
    "\n",
    "            # record the back tracked results\n",
    "            p.append(current_symbol)\n",
    "            t -= 1\n",
    "\n",
    "        # Sort and re-order again as the added ended sequences may change\n",
    "        # the order (very unlikely)\n",
    "        s, re_sorted_idx = s.topk(self.k)\n",
    "        for b_idx in range(b):\n",
    "            l[b_idx] = [l[b_idx][k_idx.data[0]] for k_idx in re_sorted_idx[b_idx,:]]\n",
    "\n",
    "        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n",
    "\n",
    "        # Reverse the sequences and re-order at the same time\n",
    "        # It is reversed because the backtracking happens in reverse time order\n",
    "#         output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n",
    "        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n",
    "        #    --- fake output ---\n",
    "        output = None\n",
    "        #    --- fake ---\n",
    "        return output, h_t, h_n, s, l, p\n",
    "\n",
    "    def _mask_symbol_scores(self, score, idx, masking_score=-float('inf')):\n",
    "            score[idx] = masking_score\n",
    "\n",
    "    def _mask(self, tensor, idx, dim=0, masking_score=-float('inf')):\n",
    "        if len(idx.size()) > 0:\n",
    "            indices = idx[:, 0]\n",
    "            tensor.index_fill_(dim, indices, masking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, use_cuda, input_dim, hidden_dim, vocab, max_length = 25):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        self.enc = Encoder(use_cuda=use_cuda, hidden_dim=hidden_dim, input_dim=input_dim, vocab=vocab)\n",
    "        self.dec = Decoder(use_cuda=use_cuda, encoder=self.enc, hidden_dim=hidden_dim, max_length=max_length)\n",
    "        if use_cuda:\n",
    "            self.enc = self.enc.cuda()\n",
    "            self.dec = self.dec.cuda()\n",
    "    def forward(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        if is_train:\n",
    "            loss, predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                    h0_and_c0=(enc_hn, enc_cn), \n",
    "                                    sent_lens=input_lens,\n",
    "                                    labels=torch.LongTensor(labels), \n",
    "                                    is_train=1, \n",
    "                                    teaching_rate = 1\n",
    "                                    )\n",
    "            return loss, predicts\n",
    "        else:\n",
    "            predicts = self.dec(enc_outputs = enc_outputs, \n",
    "                                h0_and_c0=(enc_hn, enc_cn), \n",
    "                                sent_lens=input_lens,\n",
    "                                labels=torch.LongTensor(labels), \n",
    "                                is_train=0, \n",
    "                                teaching_rate = 1\n",
    "                                )\n",
    "            return predicts\n",
    "#     def train_using_rl(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "#         enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "#         loss, predicts, bleu_mean = self.dec.train_using_rl_2(enc_outputs = enc_outputs, \n",
    "#                                                 h0_and_c0=(enc_hn, enc_cn), \n",
    "#                                                 sent_lens=input_lens,\n",
    "#                                                 labels=labels,\n",
    "#                                                 is_train=1, \n",
    "#                                                 teaching_rate = 1\n",
    "#                                                 )\n",
    "#         return loss, predicts, bleu_mean\n",
    "    def train_using_rl(self, inputs, input_lens, labels, is_train=1, teaching_rate=1):\n",
    "        topk=10\n",
    "        enc_outputs, (enc_hn, enc_cn) = self.enc(torch.LongTensor(inputs), torch.LongTensor(input_lens))\n",
    "        loss, results, bleu_mean, bleu_max = self.dec.train_using_rl_3(enc_outputs=enc_outputs, h0_and_c0=(enc_hn, enc_cn), labels=labels, topk=topk)\n",
    "        return loss, results, bleu_mean, bleu_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use_cuda = 1\n",
    "# hidden_dim = 256\n",
    "# input_dim = 100\n",
    "\n",
    "# enc = Encoder(use_cuda=use_cuda, \n",
    "#             hidden_dim=hidden_dim, \n",
    "#             input_dim=input_dim, \n",
    "#             vocab=vocab\n",
    "#            )\n",
    "# if use_cuda:\n",
    "#     enc = enc.cuda()\n",
    "    \n",
    "# sample_num = 11\n",
    "# print('sentences length: ', train_set_input_lens[0:sample_num])\n",
    "\n",
    "# enc_outputs, (enc_hn, enc_cn) = enc(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "#                                     torch.LongTensor(train_set_input_lens[0:sample_num]))\n",
    "# print('enc result size: ', enc_outputs.size(), enc_hn.size(), enc_cn.size())\n",
    "\n",
    "# dec = Decoder(use_cuda=use_cuda, encoder=enc, hidden_dim=hidden_dim, max_length=61)\n",
    "# if use_cuda:\n",
    "#     dec = dec.cuda()\n",
    "    \n",
    "# loss, predicts = dec(enc_outputs = enc_outputs, \n",
    "#                     h0_and_c0=(enc_hn, enc_cn), \n",
    "#                     sent_lens=train_set_input_lens[0:sample_num], \n",
    "#                     labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "#                     is_train=1, teaching_rate = 1\n",
    "#                     )\n",
    "# print('loss is %4.7f'%loss.data[0])\n",
    "\n",
    "# split_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 61)\n",
    "# a=time.time()\n",
    "# loss, predicts = split_model.forward(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "#                                      torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "#                                      labels=torch.LongTensor(train_set_labels[0:sample_num]), \n",
    "#                                      is_train=1, teaching_rate=1)\n",
    "# print('split_model: loss is %4.7f'%loss.data[0])\n",
    "# print(time.time()-a)\n",
    "\n",
    "# split_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, vocab = vocab, max_length = 61)\n",
    "# a=time.time()\n",
    "# loss, predicts, train_bleu_mean, train_bleu_max = split_model.train_using_rl(torch.LongTensor(train_set_inputs[0:sample_num]), \n",
    "#                                                                              torch.LongTensor(train_set_input_lens[0:sample_num]), \n",
    "#                                                                              labels=train_set_labels[0:sample_num], \n",
    "#                                                                              is_train=1, teaching_rate=1\n",
    "#                                                                             )\n",
    "# print('split_model: loss is %4.7f'%loss.data[0], train_bleu_mean)\n",
    "# print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([44380, 100])\n",
      "init lookup embedding matrix size:  torch.Size([44380, 100])\n",
      "--------split model training sampling display--------\n",
      " 1---->  ( <low_freq> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      " 2---->  a right - handed middle - order batsman , lance <low_freq> made his . <split> first - class debut for south australia on his nineteenth birthday in 1941 .\n",
      " 1---->  ( <low_freq> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      " 2---->  a right - handed opening batsman who along with jack <low_freq> formed an opening pair in the 1930s regarded as one of the finest . <split> in test history , brown was a member of don bradman 's '' <low_freq> '' that toured england in 1948 without suffering a defeat .\n",
      " 1---->  ( <low_freq> . . . . .\n",
      " 2---->  a right - handed specialist batsman , he is the captain of chittagong division . <split> , for whom he made his first class debut at the age of 16 .\n",
      " 1---->  ( <low_freq> . . .\n",
      " 2---->  a right - <low_freq> , <low_freq> had also won the french singles in . <split> 1954 and the french doubles in 1950 , 1954 , and 1955 .\n",
      " 1---->  ( <low_freq> .\n",
      " 2---->  a right answer earned 25 points and gave the team a chance to answer three '' spin - off '' questions . <split> , also worth 25 points each ; but a wrong answer gave the opposing team control , but do n't score .\n",
      "--------fusion model training sampling display--------\n",
      " 1---->  ( <low_freq>\n",
      " 2---->  a right - handed middle - order batsman , lance <low_freq> made his first - class debut for south australia on his nineteenth birthday in 1941 .\n",
      " 1---->  ( <low_freq>\n",
      " 2---->  a right - handed opening batsman who along with jack <low_freq> formed an opening pair in the 1930s regarded as one of the finest in test history , brown was a member of don bradman 's '' <low_freq> '' that toured england in 1948 without suffering a defeat .\n",
      " 1---->  ( <low_freq>\n",
      " 2---->  a right - handed specialist batsman , he is the captain of chittagong division , for whom he made his first class debut at the age of 16 .\n",
      " 1---->  ( <low_freq>\n",
      " 2---->  a right - <low_freq> , <low_freq> had also won the french singles in 1954 and the french doubles in 1950 , 1954 , and 1955 .\n",
      " 1---->  ( <low_freq>\n",
      " 2---->  a right answer earned 25 points and gave the team a chance to answer three '' spin - off '' questions , also worth 25 points each ; but a wrong answer gave the opposing team control , but do n't score .\n",
      "split_loss=9.117067337-train_bleu_mean=-1.000000000-train_bleu_max=-1.000000000-batch_size=100-epoch=0-batch_id=(1/9899)\n",
      "info=[split_model]-loss=9.117067337-bleu=0.0000-hidden_dim=512-input_dim=100-epoch=0-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050 0.0\n",
      "info=[fusion_model]-loss=10.496542931-bleu=0.0000-hidden_dim=512-input_dim=100-epoch=0-batch_size=100-batch_id=[1-[of]-9899]-lr=0.0050 0.0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 512\n",
    "input_dim = 100\n",
    "lr=0.005\n",
    "batch_size=100\n",
    "split_train_set_size=int(len(split_train_set_inputs)/1)\n",
    "epochs=10000\n",
    "train_bleu_mean=-1\n",
    "train_bleu_max=-1\n",
    "split_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 61)\n",
    "\n",
    "fusion_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 51)\n",
    "#pre train para\n",
    "# pre_train = torch.load('./models_saved/time-[2019-01-24-17-17-59]-info=[split_model]-loss=1.704468250-bleu=0.0485-hidden_dim=256-input_dim=100-epoch=9-batch_size=200-batch_id=[1-[of]-4949]-lr=0.0050', map_location='cpu')\n",
    "# split_model.load_state_dict(pre_train)\n",
    "# pre_train = torch.load('./models_saved/time-[2019-01-24-17-17-59]-info=[fusion_model]-loss=2.401571035-bleu=0.0334-hidden_dim=256-input_dim=100-epoch=9-batch_size=200-batch_id=[1-[of]-4949]-lr=0.0050', map_location='cpu')\n",
    "# fusion_model.load_state_dict(pre_train)\n",
    "\n",
    "if use_cuda:\n",
    "    split_model = split_model.cuda()\n",
    "    fusion_model = fusion_model.cuda()\n",
    "    \n",
    "split_optimizer = optim.Adam(filter(lambda p: p.requires_grad, split_model.parameters()), lr=lr)\n",
    "fusion_optimizer = optim.Adam(filter(lambda p: p.requires_grad, fusion_model.parameters()), lr=lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def model_train(epoch, batch_size, train_set_size):\n",
    "    batch_id = 0\n",
    "    valid_bleu = 0\n",
    "    for start_idx in range(0, train_set_size-batch_size+1, batch_size):\n",
    "#         print('batch id: ', batch_id)\n",
    "            \n",
    "        batch_id+=1\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        split_optimizer.zero_grad()#clear\n",
    "        fusion_optimizer.zero_grad()#clear\n",
    "        \n",
    "        split_loss, predicts = split_model.forward(torch.LongTensor(split_train_set_inputs[start_idx:end_idx]), \n",
    "                                     torch.LongTensor(split_train_set_input_lens[start_idx:end_idx]), \n",
    "                                     labels=torch.LongTensor(split_pseudo_train_set_labels[start_idx:end_idx]), \n",
    "                                     is_train=1, teaching_rate=1)\n",
    "        \n",
    "#         loss, predicts, train_bleu_mean, train_bleu_max = split_model.train_using_rl(torch.LongTensor(train_set_inputs[start_idx:end_idx]), \n",
    "#                                                                  torch.LongTensor(train_set_input_lens[start_idx:end_idx]), \n",
    "#                                                                  labels=train_set_labels[start_idx:end_idx], \n",
    "#                                                                  is_train=1, teaching_rate=1)\n",
    "#         torch.cuda.empty_cache()\n",
    "        #optimize\n",
    "        split_loss.backward()#retain_graph=True)\n",
    "        split_optimizer.step()\n",
    "        \n",
    "        fusion_loss, predicts = fusion_model.forward(torch.LongTensor(fusion_pseudo_train_set_inputs[start_idx:end_idx]), \n",
    "                                     torch.LongTensor(fusion_pseudo_train_set_input_lens[start_idx:end_idx]), \n",
    "                                     labels=torch.LongTensor(fusion_pseudo_train_set_labels[start_idx:end_idx]), \n",
    "                                     is_train=1, teaching_rate=1)\n",
    "        fusion_loss.backward()#retain_graph=True)\n",
    "        fusion_optimizer.step()\n",
    "\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_id%50==1:\n",
    "            split_model.eval()\n",
    "            fusion_model.eval()\n",
    "            \n",
    "            sample_num = 5\n",
    "            rand_idx = random.randint(0, train_set_size-sample_num-1)\n",
    "            \n",
    "            print('--------split model training sampling display--------')\n",
    "            #teaching forcing\n",
    "            loss_, predicts = split_model.forward(torch.LongTensor(split_train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(split_train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(split_pseudo_train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            \n",
    "            predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "            labels = batch_tokens_remove_eos(split_pseudo_train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "            predicts = batch_tokens2words(predicts, vocab)\n",
    "            labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "            predicts_sents = batch_words2sentence(predicts)\n",
    "            labels_sents = batch_words2sentence(labels)\n",
    "            \n",
    "            for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "                print(' 1----> ', predict_sent)\n",
    "                print(' 2----> ', label_sent)\n",
    "                \n",
    "            print('--------fusion model training sampling display--------')\n",
    "            loss_, predicts = fusion_model.forward(torch.LongTensor(fusion_pseudo_train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "                                             torch.LongTensor(fusion_pseudo_train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "                                             labels=torch.LongTensor(fusion_pseudo_train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "                                             is_train=1, teaching_rate=1)\n",
    "            del loss_\n",
    "            \n",
    "            predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "            labels = batch_tokens_remove_eos(fusion_pseudo_train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "            predicts = batch_tokens2words(predicts, vocab)\n",
    "            labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "            predicts_sents = batch_words2sentence(predicts)\n",
    "            labels_sents = batch_words2sentence(labels)\n",
    "            \n",
    "            for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "                print(' 1----> ', predict_sent)\n",
    "                print(' 2----> ', label_sent)\n",
    "                \n",
    "#             #no teaching forcing\n",
    "#             print('----no teaching forcing----')\n",
    "#             predicts = split_model.forward(torch.LongTensor(train_set_inputs[rand_idx:rand_idx+sample_num]), \n",
    "#                                              torch.LongTensor(train_set_input_lens[rand_idx:rand_idx+sample_num]), \n",
    "#                                              labels=[],#torch.LongTensor(train_set_labels[rand_idx:rand_idx+sample_num]), \n",
    "#                                              is_train=0, teaching_rate=1)\n",
    "#             predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "#             labels = batch_tokens_remove_eos(train_set_labels[rand_idx:rand_idx+sample_num], vocab)\n",
    "            \n",
    "#             predicts = batch_tokens2words(predicts, vocab)\n",
    "#             labels = batch_tokens2words(labels, vocab)\n",
    "            \n",
    "#             predicts_sents = batch_words2sentence(predicts)\n",
    "#             labels_sents = batch_words2sentence(labels)\n",
    "\n",
    "#             for (predict_sent, label_sent) in zip(predicts_sents, labels_sents):\n",
    "#                 print(' 1----> ', predict_sent)\n",
    "#                 print(' 2----> ', label_sent)\n",
    "                \n",
    "            info_stamp = 'split_loss={:2.9f}-train_bleu_mean={:2.9f}-train_bleu_max={:2.9f}-batch_size={:n}-epoch={:n}-batch_id=({:n}/{:n})'.format(\n",
    "                              split_loss.data[0], train_bleu_mean, train_bleu_max, batch_size, epoch, batch_id, int(train_set_size/batch_size))\n",
    "            print(info_stamp)\n",
    "# #             #valid_set testing\n",
    "            if batch_id%1000==1:\n",
    "                rand_idx=random.randint(0, len(split_valid_set_inputs)-batch_size-1-1)\n",
    "                predicts = split_model.forward(torch.LongTensor(split_valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "                                                 torch.LongTensor(split_valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "                                                 labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "                                                 is_train=0, teaching_rate=1)\n",
    "                predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "                labels = batch_tokens_remove_eos(split_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size], vocab)\n",
    "                \n",
    "                bleu_scores = batch_tokens_bleu(references=labels, candidates=predicts, smooth_epsilon=0.001)\n",
    "\n",
    "                valid_bleu = 0\n",
    "                for x in bleu_scores:\n",
    "                    valid_bleu+=x\n",
    "                valid_bleu/=len(bleu_scores)\n",
    "                       \n",
    "                info_stamp = 'info=[{:s}]-loss={:2.9f}-bleu={:1.4f}-hidden_dim={:n}-input_dim={:n}-epoch={:n}-batch_size={:n}-batch_id=[{:n}-[of]-{:n}]-lr={:1.4f}'.format(\n",
    "                              'split_model', split_loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "                print(info_stamp, valid_bleu)\n",
    "                now = int(round(time.time()*1000))\n",
    "                time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "                torch.save(split_model.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "\n",
    "\n",
    "                rand_idx=random.randint(0, len(fusion_pseudo_valid_set_inputs)-batch_size-1-1)\n",
    "                predicts = fusion_model.forward(torch.LongTensor(fusion_pseudo_valid_set_inputs[rand_idx:rand_idx+batch_size]), \n",
    "                                                 torch.LongTensor(fusion_pseudo_valid_set_input_lens[rand_idx:rand_idx+batch_size]), \n",
    "                                                 labels=[],#torch.LongTensor(valid_set_labels[rand_idx:rand_idx+batch_size]), \n",
    "                                                 is_train=0, teaching_rate=1)\n",
    "                predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "                labels = batch_tokens_remove_eos(fusion_pseudo_valid_set_labels[rand_idx:rand_idx+batch_size], vocab)\n",
    "                \n",
    "                bleu_scores = batch_tokens_bleu(references=labels, candidates=predicts, smooth_epsilon=0.001)\n",
    "\n",
    "                valid_bleu = 0\n",
    "                for x in bleu_scores:\n",
    "                    valid_bleu+=x\n",
    "                valid_bleu/=len(bleu_scores)\n",
    "                info_stamp = 'info=[{:s}]-loss={:2.9f}-bleu={:1.4f}-hidden_dim={:n}-input_dim={:n}-epoch={:n}-batch_size={:n}-batch_id=[{:n}-[of]-{:n}]-lr={:1.4f}'.format(\n",
    "                              'fusion_model', fusion_loss.data[0], valid_bleu, hidden_dim, input_dim, epoch, batch_size, batch_id, int(train_set_size/batch_size), lr)\n",
    "                print(info_stamp, valid_bleu)\n",
    "                now = int(round(time.time()*1000))\n",
    "                time_stamp = time.strftime('time-[%Y-%m-%d-%H-%M-%S]-',time.localtime(now/1000))\n",
    "                torch.save(fusion_model.state_dict(), ''.join(['./models_saved/', time_stamp, info_stamp]))\n",
    "                \n",
    "            split_model.train()\n",
    "            fusion_model.train()\n",
    "            \n",
    "for epoch in range(epochs):\n",
    "    model_train(epoch, batch_size, split_train_set_size)\n",
    "    \n",
    "print('running time: %.2f mins'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('running time: %.2f mins'%((time.time()-start_time)/60))\n",
    "\n",
    "\n",
    "\n",
    "a=fusion_pseudo_train_set_labels[0]\n",
    "print(len(a))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
