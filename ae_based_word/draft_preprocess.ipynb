{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_handle = open('./train.zh', 'r')\n",
    "data_rows=[]\n",
    "for row in data_handle:\n",
    "    data_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9903244 我丧失了原有的信心。 \n",
      " 我们约了8:30。不是，是9:00。\n",
      " <class 'str'> 8\n"
     ]
    }
   ],
   "source": [
    "print(len(data_rows), random.choice(data_rows), row, type(row), row[4])\n",
    "with open('all_data.json', 'w') as f:\n",
    "    json.dump(data_rows, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_data.json', 'r') as f:\n",
    "    all_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.242 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9903244 我觉得嘛你对结局有种病态的纠结。\n",
      " ['我', '觉', '得', '嘛', '你', '对', '结', '局', '有', '种', '病', '态', '的', '纠', '结', '。', '\\n'] ['我', '觉得', '嘛', '你', '对', '结局', '有种', '病态', '的', '纠结', '。', '\\n']\n"
     ]
    }
   ],
   "source": [
    "rand_sample = random.choice(all_data)\n",
    "print(len(all_data), rand_sample, list(rand_sample), jieba.lcut(rand_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, stop_words_set=set()):\n",
    "        self.word_counter=Counter()\n",
    "        self.stopwords=stop_words_set\n",
    "        \n",
    "    def insert_sentence(self, sentence):#remove stop words?\n",
    "        self.word_counter.update(sentence)\n",
    "    \n",
    "    def vocab_analyse(self, count_clip=5):\n",
    "        removed_word_cnt=0\n",
    "        removed_token_cnt=0\n",
    "\n",
    "        all_word_cnt=0\n",
    "        all_token_cnt=0\n",
    "        \n",
    "        for x in self.word_counter:\n",
    "            all_word_cnt+=self.word_counter[x]\n",
    "            all_token_cnt+=1\n",
    "            \n",
    "            if self.word_counter[x]<count_clip:\n",
    "                removed_word_cnt+=self.word_counter[x]\n",
    "                removed_token_cnt+=1\n",
    "            \n",
    "        print('in corpus, %2.4f%%(%s/%s) words will be removed if count_clip is %s.'%(100*removed_word_cnt/all_word_cnt, \n",
    "                                                                                      removed_word_cnt, all_word_cnt, count_clip))\n",
    "        print('in vocabulary, %2.4f%%(%s/%s) tokens will be removed if count_clip is %s.'%(100*removed_token_cnt/all_token_cnt, \n",
    "                                                                                           removed_token_cnt, all_token_cnt, count_clip))\n",
    "    \n",
    "    def build_vocab(self, count_clip=0):\n",
    "        self.token2word=['sos', 'padding', 'eos', 'low_freq', 'mask']\n",
    "        self.word2token={'sos':0, 'padding':1, 'eos':2, 'low_freq':3, 'mask':4}\n",
    "        self.token2count={'sos':99, 'padding':99, 'eos':99, 'low_freq':99, 'mask':99}\n",
    "        \n",
    "        for x in self.word_counter:\n",
    "            if self.word_counter[x] >= count_clip:\n",
    "                self.token2word.append(x)\n",
    "                self.word2token[x]=len(self.token2word)-1\n",
    "                self.token2count[x] = self.word_counter[x]\n",
    "                \n",
    "    def vocab_save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "990325\n",
      "1980649\n",
      "2970973\n",
      "3961297\n",
      "4951621\n",
      "5941945\n",
      "6932269\n",
      "7922593\n",
      "8912917\n",
      "9903241\n"
     ]
    }
   ],
   "source": [
    "all_data_lcut = [1]*len(all_data)\n",
    "data_size = len(all_data_lcut)\n",
    "\n",
    "for idx, data in enumerate(all_data):\n",
    "    all_data_lcut[idx]=jieba.lcut(data)\n",
    "    \n",
    "    if idx%int(data_size/10)==1:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "990325\n",
      "1980649\n",
      "2970973\n",
      "3961297\n",
      "4951621\n",
      "5941945\n",
      "6932269\n",
      "7922593\n",
      "8912917\n",
      "9903241\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "data_size = len(all_data_lcut)\n",
    "for idx, data in enumerate(all_data_lcut):\n",
    "    vocab.insert_sentence(data)\n",
    "    \n",
    "    if idx%int(data_size/10)==1:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in corpus, 1.4255%(1766026/123885822) words will be removed if count_clip is 20.\n",
      "in vocabulary, 85.3907%(576499/675131) tokens will be removed if count_clip is 20.\n",
      "98637\n"
     ]
    }
   ],
   "source": [
    "vocab.vocab_analyse(count_clip=20)\n",
    "vocab.build_vocab(count_clip=20)\n",
    "print(len(vocab.word2token))\n",
    "vocab.vocab_save('vocab.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab, stop_words = set()):\n",
    "        self.stop_words = stop_words\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    \n",
    "    def sentences_length(self, sentences):\n",
    "        self.sentences_length=[]\n",
    "        for sent in sentences:\n",
    "            #sent = list(sentence)\n",
    "            word_cnt=0\n",
    "            for word in sent:\n",
    "                if word not in self.stop_words:\n",
    "                    word_cnt+=1\n",
    "            self.sentences_length.append(word_cnt)\n",
    "            \n",
    "            \n",
    "    def sentences_length_analyse(self, sentence_length_clip=30):\n",
    "        sent_cnt=0\n",
    "        for sent_length in self.sentences_length:\n",
    "            if sent_length <= sentence_length_clip:\n",
    "                sent_cnt+=1\n",
    "        print('%2.4f%%(%s/%s) sentences\\'length are shorter than %s.'%(100*sent_cnt/len(self.sentences_length), \n",
    "                                                                     sent_cnt, len(self.sentences_length), sentence_length_clip))\n",
    "        \n",
    "    def sentence_tokenize(self, sentence, max_length):\n",
    "        tokenized_sent=[]\n",
    "        for word in sentence:\n",
    "            if word not in self.stop_words and len(tokenized_sent)<max_length:\n",
    "                if word in self.vocab.word2token:\n",
    "                    tokenized_sent.append(self.vocab.word2token[word])\n",
    "                else:\n",
    "                    tokenized_sent.append(self.vocab.word2token['low_freq'])\n",
    "        real_len = len(tokenized_sent)\n",
    "        \n",
    "        while(len(tokenized_sent)<max_length):\n",
    "            tokenized_sent.append(self.vocab.word2token['padding'])\n",
    "            \n",
    "        return tokenized_sent, real_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('vocab.pk', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "stop_words = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'z', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "              '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '*', '&', '^', '#', '$', '@', '￥', '~', '+', '-', '=', '\\n', ' '}\n",
    "tokenizer = Tokenizer(vocab, stop_words= stop_words)\n",
    "tokenizer.sentences_length(all_data_lcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.7130%(9379655/9903244) sentences'length are shorter than 25.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.sentences_length_analyse(sentence_length_clip=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "990325\n",
      "1980649\n",
      "2970973\n",
      "3961297\n",
      "4951621\n",
      "5941945\n",
      "6932269\n",
      "7922593\n",
      "8912917\n",
      "9903241\n"
     ]
    }
   ],
   "source": [
    "tokenized_sents=[]\n",
    "tokenized_sents_real_len=[]\n",
    "data_size = len(all_data_lcut)\n",
    "\n",
    "for idx, sent in enumerate(all_data_lcut):\n",
    "    tokenized_sent, real_len = tokenizer.sentence_tokenize(sent, max_length=25)\n",
    "    tokenized_sents.append(tokenized_sent)\n",
    "    tokenized_sents_real_len.append(real_len)\n",
    "    \n",
    "    if idx%int(data_size/10)==1:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9903244 7091984 [14058, 59835, 86185, 80168, 97088, 32932, 83094, 88055, 16218, 2352, 43062, 80168, 42385, 7489, 11659, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 25 15\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0,len(tokenized_sents))\n",
    "print(len(tokenized_sents), idx, tokenized_sents[idx], len(tokenized_sents[idx]), tokenized_sents_real_len[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tokenized_sentences.pk', 'wb') as f:\n",
    "    pickle.dump(tokenized_sents, f)\n",
    "with open('tokenized_sentences_real_length.pk', 'wb') as f:\n",
    "    pickle.dump(tokenized_sents_real_len, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
