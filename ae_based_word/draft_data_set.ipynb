{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from Vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenized_sent2real_sent(tokenized_sent, vocab):\n",
    "    real_sent=[]\n",
    "    for token in tokenized_sent:\n",
    "        if token == vocab.word2token['eos']:\n",
    "            break\n",
    "        else:\n",
    "            real_sent.append(vocab.token2word[token])\n",
    "    return ''.join(real_sent)\n",
    "\n",
    "def reverse_tokenized_sent2real_sent(tokenized_sent, vocab):\n",
    "    real_sent=[]\n",
    "    for token in tokenized_sent:\n",
    "        if token == vocab.word2token['eos']:\n",
    "            break\n",
    "        else:\n",
    "            real_sent.append(vocab.token2word[token])\n",
    "    real_sent.reverse()\n",
    "    return ''.join(real_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./tokenized_sentences.pk', 'rb') as f:\n",
    "    tokenized_sentences = pickle.load(f)\n",
    "    \n",
    "with open('./tokenized_sentences_real_length.pk', 'rb') as f:\n",
    "    tokenized_sentences_real_length = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./vocab.pk', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "label_lengths=[]\n",
    "max_length = len(random.choice(tokenized_sentences))\n",
    "\n",
    "for idx, sent in enumerate(tokenized_sentences):\n",
    "    label = sent[:tokenized_sentences_real_length[idx]]\n",
    "    #label.reverse()\n",
    "    label.append(vocab.word2token['eos'])\n",
    "    label_lengths.append(len(label))\n",
    "    \n",
    "    while(len(label)<max_length+1):    #so the length of label is max_length+1\n",
    "        label.append(vocab.word2token['padding'])\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824124 [60912, 69943, 21788, 63265, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 4 [60912, 69943, 21788, 63265, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 5 26\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0,len(labels))\n",
    "print(idx, tokenized_sentences[idx], tokenized_sentences_real_length[idx], labels[idx], label_lengths[idx], len(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9903244 9903244 9903244\n",
      "9882955 9882955 9882955\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_sentences_real_length), len(tokenized_sentences), len(labels))\n",
    "\n",
    "tokenized_sentences_real_length_=[]\n",
    "tokenized_sentences_=[]\n",
    "labels_=[]\n",
    "\n",
    "for idx, sent_len in enumerate(tokenized_sentences_real_length):\n",
    "    if sent_len>=3:\n",
    "        tokenized_sentences_real_length_.append(sent_len)\n",
    "        tokenized_sentences_.append(tokenized_sentences[idx])\n",
    "        labels_.append(labels[idx])\n",
    "        \n",
    "tokenized_sentences = tokenized_sentences_\n",
    "tokenized_sentences_real_length = tokenized_sentences_real_length_\n",
    "labels = labels_\n",
    "\n",
    "    \n",
    "print(len(tokenized_sentences_real_length), len(tokenized_sentences), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_packed = list(zip(tokenized_sentences, tokenized_sentences_real_length, labels))\n",
    "random.shuffle(dataset_packed)\n",
    "(tokenized_sents, tokenized_sents_real_length, labels_)=zip(*dataset_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6715715 [20202, 65947, 71114, 24656, 10626, 80168, 71636, 67893, 69886, 66138, 77837, 80168, 31612, 11659, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 14 [20202, 65947, 71114, 24656, 10626, 80168, 71636, 67893, 69886, 66138, 77837, 80168, 31612, 11659, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 7 26\n",
      "6715715 [14962, 64466, 86413, 86185, 90050, 11619, 25973, 62860, 25852, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 9 [14962, 64466, 86413, 86185, 90050, 11619, 25973, 62860, 25852, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0,len(labels_))\n",
    "print(idx, tokenized_sentences[idx], tokenized_sentences_real_length[idx], labels[idx], label_lengths[idx], len(labels[idx]))\n",
    "print(idx, tokenized_sents[idx], tokenized_sents_real_length[idx], labels_[idx] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8697000 1185955\n"
     ]
    }
   ],
   "source": [
    "train_set_rate=0.88\n",
    "train_set_num = int(train_set_rate*len(tokenized_sents))\n",
    "\n",
    "train_set_inputs = tokenized_sents[:train_set_num]\n",
    "train_set_input_lens = tokenized_sents_real_length[:train_set_num]\n",
    "train_set_labels = labels_[:train_set_num]\n",
    "\n",
    "valid_set_inputs = tokenized_sents[train_set_num:]\n",
    "valid_set_input_lens = tokenized_sents_real_length[train_set_num:]\n",
    "valid_set_labels = labels_[train_set_num:]\n",
    "\n",
    "print(len(train_set_inputs) ,len(valid_set_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data_set/train_set_inputs.pk', 'wb') as f:\n",
    "    pickle.dump(train_set_inputs,f)\n",
    "with open('./data_set/train_set_input_lens.pk', 'wb') as f:\n",
    "    pickle.dump(train_set_input_lens,f)\n",
    "with open('./data_set/train_set_labels.pk', 'wb') as f:\n",
    "    pickle.dump(train_set_labels,f)\n",
    "\n",
    "with open('./data_set/valid_set_inputs.pk', 'wb') as f:\n",
    "    pickle.dump(valid_set_inputs,f)\n",
    "with open('./data_set/valid_set_input_lens.pk', 'wb') as f:\n",
    "    pickle.dump(valid_set_input_lens,f)\n",
    "with open('./data_set/valid_set_labels.pk', 'wb') as f:\n",
    "    pickle.dump(valid_set_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "伙计，你可是个健康白种人paddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpaddingpadding\n"
     ]
    }
   ],
   "source": [
    "idx = random.choice(range(len(tokenized_sentences)))\n",
    "t=tokenized_sentences[idx]\n",
    "print(tokenized_sent2real_sent(t, vocab))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
