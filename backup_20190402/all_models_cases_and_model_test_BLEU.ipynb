{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import over\n",
      "import over\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from Vocab import Vocab\n",
    "from LanguageModel import LanguageModel\n",
    "from Seq2Seq_att import Seq2Seq_att\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "print('import over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_words2sentence(words_list):\n",
    "    return [' '.join(words) for words in words_list]\n",
    "def batch_tokens2words(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return: words_list corresponding to tokens\n",
    "    return [[vocab.token2word[token] for token in tokens] for tokens in tokens_list]\n",
    "\n",
    "def batch_tokens_remove_eos(tokens_list, vocab):\n",
    "    ##    para: tokens_list is list[list] type\n",
    "    ##    return pure tokens_list removed eos symbol\n",
    "    result=[]\n",
    "    for tokens in tokens_list:\n",
    "        tokens_filtered=[]\n",
    "        for token in tokens:\n",
    "            if token == vocab.word2token['<eos>']:\n",
    "#                 tokens_filtered.append(token)\n",
    "                break\n",
    "            else:\n",
    "                tokens_filtered.append(token)\n",
    "        result.append(tokens_filtered)\n",
    "    return result\n",
    "\n",
    "def batch_tokens_bleu(references, candidates, smooth_epsilon=0.001):\n",
    "    ##    para: references and candidates are list[list] type\n",
    "    ##    return: list of BLEU for every sample\n",
    "    ##\n",
    "    bleu_scores=[]\n",
    "    for ref, candidate in zip(references, candidates):\n",
    "        if min(len(ref), len(candidate))<4:\n",
    "            bleu_scores.append(0)\n",
    "        else:\n",
    "            bleu_scores.append(sentence_bleu([ref], candidate, smoothing_function = SmoothingFunction(epsilon=smooth_epsilon).method1))\n",
    "    return bleu_scores\n",
    "\n",
    "with open('data_set/vocab.pk', 'rb') as f:\n",
    "    vocab=pickle.load(f)\n",
    "\n",
    "    \n",
    "def seqs_split(seqs, vocab):\n",
    "    seqs = batch_tokens_remove_eos(seqs, vocab)\n",
    "    simple_sent1s=[]\n",
    "    simple_sent2s=[]\n",
    "    for seq in seqs:\n",
    "        simple_sent1=[]\n",
    "        simple_sent2=[]\n",
    "        sent=simple_sent1\n",
    "        for token in seq:\n",
    "            if token==vocab.word2token['<split>']:\n",
    "                sent=simple_sent2\n",
    "            else:\n",
    "                sent.append(token)\n",
    "        simple_sent1s.append(simple_sent1)\n",
    "        simple_sent2s.append(simple_sent2)\n",
    "        \n",
    "    return simple_sent1s, simple_sent2s\n",
    "\n",
    "def simple_sents_concat(simple_sent1s, simple_sent2s, vocab, max_length):\n",
    "    simple_sent_lens=[]\n",
    "    simple_sents=simple_sent1s\n",
    "    for i, sent in enumerate(simple_sent2s):\n",
    "        simple_sents[i].append(vocab.word2token['<split>'])\n",
    "        for token in sent:\n",
    "            simple_sents[i].append(token)\n",
    "\n",
    "        #if there is no <split> in simple_sent1s and simple_sent2s, then the length of sents_concat will be longer than max_length\n",
    "        if len(simple_sents[i])>max_length:\n",
    "            simple_sents[i] = simple_sents[i][:max_length]\n",
    "            \n",
    "        simple_sent_lens.append(len(simple_sents[i]))\n",
    "            \n",
    "        while(len(simple_sents[i])<max_length):\n",
    "            simple_sents[i].append(vocab.word2token['<padding>'])\n",
    "            \n",
    "    return simple_sents, simple_sent_lens\n",
    "\n",
    "\n",
    "def get_lm_inputs_and_labels(sents, vocab, max_length):\n",
    "    lm_inputs=copy.deepcopy(sents)\n",
    "    lm_labels=copy.deepcopy(sents)\n",
    "    lm_input_lens=[]\n",
    "    \n",
    "    for sent in lm_inputs:\n",
    "        if len(sent)>=max_length:\n",
    "            sent=sent[:max_length-1]\n",
    "        sent.insert(0, vocab.word2token['<sos>'])\n",
    "        lm_input_lens.append(len(sent))\n",
    "        while(len(sent)<max_length):\n",
    "            sent.append(vocab.word2token['<padding>'])\n",
    "\n",
    "    for sent in lm_labels:\n",
    "        if len(sent)>=max_length:\n",
    "            sent = sent[:max_length-1]\n",
    "        sent.append(vocab.word2token['<eos>'])\n",
    "        while(len(sent)<max_length):\n",
    "            sent.append(vocab.word2token['<padding>'])\n",
    "        \n",
    "    return lm_inputs, lm_input_lens, lm_labels\n",
    "\n",
    "\n",
    "def duplicate_reconstruct_labels(sents, topk):\n",
    "    return [x for x in sents for ii in range(topk)]\n",
    "\n",
    "\n",
    "def batch_tokens_bleu_split_version(references, candidates, vocab, smooth_epsilon=0.001):\n",
    "    # needn't remove '<sos>' token before calling this function, which is different from the 'batch_token_bleu()' version\n",
    "    #\n",
    "    ref1, ref2 = seqs_split(references, vocab)\n",
    "    cand1, cand2 = seqs_split(candidates, vocab)\n",
    "    bleu_simple_sent1s = batch_tokens_bleu(ref1, cand1)\n",
    "    bleu_simple_sent2s = batch_tokens_bleu(ref2, cand2)\n",
    "#     print(bleu_simple_sent1s)\n",
    "#     print(bleu_simple_sent2s)\n",
    "    bleu=[]\n",
    "    for idx in range(len(bleu_simple_sent1s)):\n",
    "        bleu.append((bleu_simple_sent1s[idx]+bleu_simple_sent2s[idx])/2)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def set_model_grad(model, is_grad):\n",
    "    for param in model.parameters():\n",
    "         param.requires_grad = is_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data_set2/split_data_set/validation_complex_sents.pk', 'rb') as f:\n",
    "    split_valid_set_inputs = pickle.load(f)\n",
    "with open('./data_set2/split_data_set/validation_complex_sent_lens.pk', 'rb') as f:\n",
    "    split_valid_set_input_lens = pickle.load(f)\n",
    "with open('./data_set2/split_data_set/validation_labels.pk', 'rb') as f:\n",
    "    split_pseudo_valid_set_labels = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_model_eval(model, inputs, input_lens, labels):\n",
    "    dataset_size = len(inputs)\n",
    "    print(dataset_size)\n",
    "    scores_ground_truth=0\n",
    "    scores_no_ground_truth=0\n",
    "    for idx in range(0, dataset_size, batch_size):\n",
    "        \n",
    "        #no teacher forcing\n",
    "        predicts = model.forward(torch.LongTensor(inputs[idx:idx+batch_size]),\n",
    "                                 torch.LongTensor(input_lens[idx:idx+batch_size]),\n",
    "                                 labels=[],\n",
    "                                 is_train=0, teaching_rate=1)\n",
    "        bleu_scores = batch_tokens_bleu_split_version(references = labels[idx:idx+batch_size],\n",
    "                                                     candidates = predicts,\n",
    "                                                     smooth_epsilon=0.001,\n",
    "                                                     vocab=vocab)\n",
    "        for x in bleu_scores:\n",
    "            scores_no_ground_truth+=x\n",
    "    return scores_no_ground_truth/dataset_size\n",
    "\n",
    "\n",
    "def split_model_eval_topk(model, inputs, input_lens, labels, topk):\n",
    "    \n",
    "    dataset_size = len(inputs)\n",
    "    print(dataset_size)\n",
    "    scores_no_ground_truth=0\n",
    "    for idx in range(0, dataset_size, batch_size):\n",
    "        dec_seqs, log_probs = model.dec.decode_topk_seqs(model.enc, inputs=torch.LongTensor(inputs[idx:idx+batch_size]), \n",
    "                                                         input_lens=torch.LongTensor(input_lens[idx:idx+batch_size]),\n",
    "                                                         topk=topk)\n",
    "        predicts = []\n",
    "        for ii in range(len(dec_seqs)):\n",
    "            if ii%topk==0:\n",
    "                predicts.append(dec_seqs[ii])\n",
    "        \n",
    "        bleu_scores = batch_tokens_bleu_split_version(references = labels[idx:idx+batch_size],\n",
    "                                                     candidates = predicts,\n",
    "                                                     smooth_epsilon=0.001,\n",
    "                                                     vocab=vocab)\n",
    "        for x in bleu_scores:\n",
    "            scores_no_ground_truth+=x\n",
    "        \n",
    "    return scores_no_ground_truth/dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([44380, 100])\n"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "hidden_dim = 256\n",
    "input_dim = 100\n",
    "lr=0.005\n",
    "batch_size=35\n",
    "\n",
    "epochs=10000\n",
    "train_bleu_mean=-1\n",
    "train_bleu_max=-1\n",
    "split_model = Seq2Seq_att(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 61)\n",
    "\n",
    "# fusion_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "#                           vocab = vocab, max_length = 51)\n",
    "#pre train para\n",
    "split_model_path = './models_saved/time-[2019-03-24-21-45-10]-info=[pretrain_split-att-20per]-loss=0.359141707-bleu=0.6181-hidden_dim=256-input_dim=100-epoch=5-batch_size=180-batch_id=[501-[of]-1099]-lr=0.0050'\n",
    "# fusion_model_path = './models_saved/time-[2019-03-10-13-23-11]-info=[pre-trained_fusion_model-20per]-loss=0.346116364-bleu=0.7466-hidden_dim=256-input_dim=100-epoch=4-batch_size=100-batch_id=[1-[of]-1979]-lr=0.0050'\n",
    "\n",
    "pre_train = torch.load(split_model_path, map_location='cpu')\n",
    "split_model.load_state_dict(pre_train)\n",
    "# pre_train = torch.load(fusion_model_path, map_location='cpu')\n",
    "# fusion_model.load_state_dict(pre_train)\n",
    "\n",
    "if use_cuda:\n",
    "    split_model = split_model.cuda()\n",
    "#     fusion_model = fusion_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3571125471626948]\n",
      "the school has two campuses . <split> around 3,000 students at the dover campus and 2,400 at the east campus .\n"
     ]
    }
   ],
   "source": [
    "#case\n",
    "sent = 'greene married denny miller in 1941 who died in 1991 .'\n",
    "label ='greene married denny miller in 1941 . <split> they were married until her death in 1991 .'\n",
    "\n",
    "sent = 'the school has two campuses , with around 3,000 students at the dover campus and 2,400 at the east campus .'\n",
    "label = 'the school has two campuses , dover and east . <split> there are currently around 3,000 students on dover campus and 2,400 on east campus .'\n",
    "\n",
    "\n",
    "tokenized_sent = []\n",
    "sent=sent.split(' ')\n",
    "for word in sent:\n",
    "    if word in vocab.word2token:\n",
    "        tokenized_sent.append(vocab.word2token[word])\n",
    "    else:\n",
    "        tokenized_sent.append(vocab.word2token['<low_freq>'])\n",
    "# print(tokenized_sent)\n",
    "sent_len = len(sent)\n",
    "\n",
    "tokenized_label=[]\n",
    "label = label.split(' ')\n",
    "for word in label:\n",
    "    if word in vocab.word2token:\n",
    "        tokenized_label.append(vocab.word2token[word])\n",
    "    else:\n",
    "        tokenized_label.append(vocab.word2token['<low_freq>'])\n",
    "\n",
    "        \n",
    "# model with att\n",
    "predicts = split_model.forward(torch.LongTensor([tokenized_sent]),\n",
    "                                 torch.LongTensor([sent_len]),\n",
    "                                 labels=[],\n",
    "                                 is_train=0, teaching_rate=1)\n",
    "\n",
    "bleu_scores = batch_tokens_bleu_split_version(references = [tokenized_label],\n",
    "                                             candidates = predicts,\n",
    "                                             smooth_epsilon=0.001,\n",
    "                                             vocab=vocab)\n",
    "print(bleu_scores)\n",
    "\n",
    "predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "predicts = batch_tokens2words(predicts, vocab)\n",
    "predicts_sents = batch_words2sentence(predicts)\n",
    "print(predicts_sents[0])\n",
    "\n",
    "topk=2\n",
    "dec_seqs, log_probs = split_model.dec.decode_topk_seqs(split_model.enc, inputs=torch.LongTensor([tokenized_sent]),\n",
    "                                                         input_lens=torch.LongTensor([sent_len]),\n",
    "                                                         topk=topk)\n",
    "predicts = []\n",
    "for ii in range(len(dec_seqs)):\n",
    "    if ii%topk==0:\n",
    "        predicts.append(dec_seqs[ii])\n",
    "\n",
    "bleu_scores = batch_tokens_bleu_split_version(references = [tokenized_label],\n",
    "                                             candidates = predicts,\n",
    "                                             smooth_epsilon=0.001,\n",
    "                                             vocab=vocab)\n",
    "print(bleu_scores)\n",
    "\n",
    "predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "predicts = batch_tokens2words(predicts, vocab)\n",
    "predicts_sents = batch_words2sentence(predicts)\n",
    "print(predicts_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "0.6862993168709564\n"
     ]
    }
   ],
   "source": [
    "batch_size=35\n",
    "score = split_model_eval_topk(model=split_model, \n",
    "                             inputs=split_valid_set_inputs, \n",
    "                             input_lens=split_valid_set_input_lens, \n",
    "                             labels=split_pseudo_valid_set_labels,\n",
    "                             topk=2)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "0.6449701481493929\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "score = split_model_eval(model=split_model, \n",
    "                         inputs=split_valid_set_inputs, \n",
    "                         input_lens=split_valid_set_input_lens, \n",
    "                         labels=split_pseudo_valid_set_labels)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init lookup embedding matrix size:  torch.Size([44380, 100])\n"
     ]
    }
   ],
   "source": [
    "from Seq2Seq import Seq2Seq\n",
    "\n",
    "#copy\n",
    "split_model2 = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "                          vocab = vocab, max_length = 61)\n",
    "\n",
    "# fusion_model = Seq2Seq(use_cuda = use_cuda, input_dim = input_dim, hidden_dim = hidden_dim, \n",
    "#                           vocab = vocab, max_length = 51)\n",
    "#pre train para\n",
    "split_model_path = './models_saved/time-[2019-03-24-21-30-26]-info=[pre-trained_split_model-20per]-loss=0.543618917-bleu=0.6642-hidden_dim=256-input_dim=100-epoch=2-batch_size=100-batch_id=[1-[of]-1979]-lr=0.0050'\n",
    "split_model_path = './models_saved/time-[2019-03-26-09-50-00]-info=[pre-trained_split_model-5per]-loss=0.243756816-bleu=0.6464-hidden_dim=256-input_dim=100-epoch=12-batch_size=100-batch_id=[1-[of]-494]-lr=0.0050'\n",
    "split_model_path = './models_saved/time-[2019-03-25-13-32-25]-info=[pre-trained_split_model-5per]-loss=0.368953973-bleu=0.6889-hidden_dim=256-input_dim=100-epoch=5-batch_size=100-batch_id=[1-[of]-494]-lr=0.0050'\n",
    "split_model_path = './models_saved/time-[2019-03-30-13-37-54]-info=[pre-trained_split_model-unsuper]-loss=0.000472637-bleu=0.6470-hidden_dim=256-input_dim=100-epoch=0-batch_size=100-batch_id=[6501-[of]-7919]-lr=0.0050'\n",
    "\n",
    "\n",
    "pre_train = torch.load(split_model_path, map_location='cpu')\n",
    "split_model2.load_state_dict(pre_train)\n",
    "\n",
    "if use_cuda:\n",
    "    split_model2 = split_model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35787900668872624]\n",
      "the school has two campuses , with around 3,000 students . <split> at the dover campus and 2,400 at the east campus .\n",
      "[0.35787900668872624]\n",
      "the school has two campuses , with around 3,000 students . <split> at the dover campus and 2,400 at the east campus .\n"
     ]
    }
   ],
   "source": [
    "#model with att and copy\n",
    "predicts = split_model2.forward(torch.LongTensor([tokenized_sent]),\n",
    "                                 torch.LongTensor([sent_len]),\n",
    "                                 labels=[],\n",
    "                                 is_train=0, teaching_rate=1)\n",
    "\n",
    "bleu_scores = batch_tokens_bleu_split_version(references = [tokenized_label],\n",
    "                                             candidates = predicts,\n",
    "                                             smooth_epsilon=0.001,\n",
    "                                             vocab=vocab)\n",
    "print(bleu_scores)\n",
    "\n",
    "predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "predicts = batch_tokens2words(predicts, vocab)\n",
    "predicts_sents = batch_words2sentence(predicts)\n",
    "print(predicts_sents[0])\n",
    "\n",
    "\n",
    "\n",
    "topk=2\n",
    "dec_seqs, log_probs = split_model2.dec.decode_topk_seqs(split_model2.enc, inputs=torch.LongTensor([tokenized_sent]),\n",
    "                                                         input_lens=torch.LongTensor([sent_len]),\n",
    "                                                         topk=topk)\n",
    "predicts = []\n",
    "for ii in range(len(dec_seqs)):\n",
    "    if ii%topk==0:\n",
    "        predicts.append(dec_seqs[ii])\n",
    "\n",
    "bleu_scores = batch_tokens_bleu_split_version(references = [tokenized_label],\n",
    "                                             candidates = predicts,\n",
    "                                             smooth_epsilon=0.001,\n",
    "                                             vocab=vocab)\n",
    "print(bleu_scores)\n",
    "\n",
    "predicts = batch_tokens_remove_eos(predicts, vocab)\n",
    "predicts = batch_tokens2words(predicts, vocab)\n",
    "predicts_sents = batch_words2sentence(predicts)\n",
    "print(predicts_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "0.6578814041853408\n"
     ]
    }
   ],
   "source": [
    "batch_size=35\n",
    "score = split_model_eval_topk(model=split_model2, \n",
    "                             inputs=split_valid_set_inputs, \n",
    "                             input_lens=split_valid_set_input_lens, \n",
    "                             labels=split_pseudo_valid_set_labels,\n",
    "                             topk=2)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "0.6579575758892041\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "score = split_model_eval(model=split_model2, \n",
    "                         inputs=split_valid_set_inputs, \n",
    "                         input_lens=split_valid_set_input_lens, \n",
    "                         labels=split_pseudo_valid_set_labels)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44380\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.word2token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
